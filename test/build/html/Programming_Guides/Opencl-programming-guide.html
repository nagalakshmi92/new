

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>OpenCL Programming Guide &mdash; ReadTheDocs-Breathe 1.0.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> ReadTheDocs-Breathe
          

          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../ROCm.html">Quick Start Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Current_Release_Notes/Current-Release-Notes.html">Current Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Installation_Guide/Installation-Guide.html">ROCm Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="Programming-Guides.html">Programming Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ROCm_GPU_Tunning_Guides/ROCm-GPU-Tunning-Guides.html">ROCm GPU Tuning Guides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GCN_ISA_Manuals/GCN-ISA-Manuals.html">GCN ISA Manuals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ROCm_API_References/ROCm-API-References.html">ROCm API References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ROCm_Tools/ROCm-Tools.html">ROCm Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ROCm_Libraries/ROCm_Libraries.html">ROCm Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ROCm_Compiler_SDK/ROCm-Compiler-SDK.html">ROCm Compiler SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ROCm_System_Managment/ROCm-System-Managment.html">ROCm System Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ROCm_Virtualization_Containers/ROCm-Virtualization-&amp;-Containers.html">ROCm Virtualization &amp; Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Remote_Device_Programming/Remote-Device-Programming.html">Remote Device Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Deep_learning/Deep-learning.html">Deep Learning on ROCm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Other_Solutions/Other-Solutions.html">System Level Debug</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Tutorial/Tutorial.html">Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ROCm_Glossary/ROCm-Glossary.html">ROCm Glossary</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">ReadTheDocs-Breathe</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>OpenCL Programming Guide</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/Programming_Guides/Opencl-programming-guide.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="opencl-programming-guide">
<span id="id1"></span><h1>OpenCL Programming Guide<a class="headerlink" href="#opencl-programming-guide" title="Permalink to this headline">¶</a></h1>
<blockquote>
<div><blockquote>
<div><ul class="simple">
<li><dl class="first docutils">
<dt><span class="xref std std-ref">OpenCL Architecture</span></dt>
<dd><ul class="first last">
<li><a class="reference internal" href="#terminology"><span class="std std-ref">Terminology</span></a></li>
<li><a class="reference internal" href="#opencl-overview"><span class="std std-ref">OpenCL Overview</span></a></li>
<li><a class="reference internal" href="#programming-model"><span class="std std-ref">Programming Model</span></a></li>
<li><a class="reference internal" href="#synchronization"><span class="std std-ref">Synchronization</span></a></li>
<li><a class="reference internal" href="#memory-arch"><span class="std std-ref">Memory Architecture and Access</span></a></li>
<li><a class="reference internal" href="#example"><span class="std std-ref">Example Programs</span></a></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference internal" href="#amd-implementation"><span class="std std-ref">AMD Implementation</span></a></dt>
<dd><ul class="first last">
<li><a class="reference internal" href="#amd-rocm-implementation"><span class="std std-ref">The AMD ROCm Implementation of OpenCL</span></a></li>
<li><a class="reference internal" href="#hardware-overview-gcndevices"><span class="std std-ref">Hardware Overview for GCN Devices</span></a></li>
<li><a class="reference internal" href="#communication-host-gpu"><span class="std std-ref">Communication Between Host and the GPU Compute Device</span></a></li>
<li><a class="reference internal" href="#wavefront-scheduling"><span class="std std-ref">Wavefront Scheduling</span></a></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference internal" href="#build-run-opencl"><span class="std std-ref">Building and Running OpenCL Programs</span></a></dt>
<dd><ul class="first last">
<li><a class="reference internal" href="#compilin-host-program"><span class="std std-ref">Compiling the Host Program</span></a></li>
<li><a class="reference internal" href="#compiling-device-programs"><span class="std std-ref">Compiling the device programs</span></a></li>
<li><a class="reference internal" href="#supported-standard-opencl-compiler"><span class="std std-ref">Supported Standard OpenCL Compiler Options</span></a></li>
<li><a class="reference internal" href="#amd-developed-supplemental-compiler"><span class="std std-ref">AMD-Developed Supplemental Compiler Options</span></a></li>
<li><a class="reference internal" href="#creating-device-specific-binaries"><span class="std std-ref">Creating device-specific binaries</span></a></li>
<li><a class="reference internal" href="#command-execution-flow"><span class="std std-ref">Command execution flow</span></a></li>
<li><a class="reference internal" href="#running-program"><span class="std std-ref">Running the Program</span></a></li>
<li><a class="reference internal" href="#note-on-thread-safety"><span class="std std-ref">A note on thread safety</span></a></li>
<li><a class="reference internal" href="#toolchain-considerations"><span class="std std-ref">Toolchain considerations</span></a></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference internal" href="#profiling-opencl"><span class="std std-ref">Profiling OpenCL</span></a></dt>
<dd><ul class="first last">
<li><a class="reference internal" href="#amd-codexl-gpu"><span class="std std-ref">Downloading and installing CodeXL and Radeon Compute Profiler</span></a></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference internal" href="#opencl-static"><span class="std std-ref">OpenCL Static C++ Programming Language</span></a></dt>
<dd><ul class="first last">
<li><a class="reference internal" href="#overview"><span class="std std-ref">Overview</span></a></li>
<li><a class="reference internal" href="#opencl-c-runtime"><span class="std std-ref">Additions and Changes to Section 5 - The OpenCL C Runtime</span></a></li>
<li><a class="reference internal" href="#c-programming-language"><span class="std std-ref">Additions and Changes to Section 6 - The OpenCL 1.2 C Programming Language</span></a></li>
<li><a class="reference internal" href="../ROCm_Tools/hcFFT.html#examples"><span class="std std-ref">Examples</span></a></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference internal" href="#opencl-2-0"><span class="std std-ref">OpenCL 2.0</span></a></dt>
<dd><ul class="first last">
<li><a class="reference internal" href="../ROCm_Tools/hcFFT.html#introduction"><span class="std std-ref">Introduction</span></a></li>
<li><a class="reference internal" href="#shared-virtual-memory"><span class="std std-ref">Shared Virtual Memory (SVM)</span></a></li>
<li><a class="reference internal" href="#generi"><span class="std std-ref">Generic Address Space</span></a></li>
<li><a class="reference internal" href="#device-side-enqueue"><span class="std std-ref">Device-side enqueue and workgroup/sub-group level functions</span></a></li>
<li><a class="reference internal" href="#atomics"><span class="std std-ref">Atomics and synchronization</span></a></li>
<li><a class="reference internal" href="#pipes"><span class="std std-ref">Pipes</span></a></li>
<li><a class="reference internal" href="#program-scope-global-variables"><span class="std std-ref">Program-scope global Variables</span></a></li>
<li><a class="reference internal" href="#image-enhancements"><span class="std std-ref">Image Enhancements</span></a></li>
<li><a class="reference internal" href="#non-uniform-work-group-size"><span class="std std-ref">Non-uniform work group size</span></a></li>
<li><a class="reference internal" href="#portability-considerations"><span class="std std-ref">Portability considerations</span></a></li>
</ul>
</dd>
</dl>
</li>
<li><a class="reference internal" href="#opencl-extentions"><span class="std std-ref">OpenCL Optional Extensions</span></a></li>
<li><a class="reference internal" href="#icd"><span class="std std-ref">The OpenCL Installable Client Driver (ICD)</span></a></li>
<li><a class="reference internal" href="#bif"><span class="std std-ref">OpenCL Binary Image Format (BIF) v2.0</span></a></li>
<li><a class="reference internal" href="#pre-gcn-devices"><span class="std std-ref">Hardware overview of pre-GCN devices</span></a></li>
<li><a class="reference internal" href="#opencl-opengl"><span class="std std-ref">OpenCL-OpenGL Interoperability</span></a></li>
<li><a class="reference internal" href="#functions-opencl"><span class="std std-ref">New built-in functions in OpenCL 2.0</span></a></li>
</ul>
</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="opencl-architecture-and-amd-accelerated-parallel-processing-technology">
<h1>OpenCL Architecture and AMD Accelerated Parallel Processing Technology<a class="headerlink" href="#opencl-architecture-and-amd-accelerated-parallel-processing-technology" title="Permalink to this headline">¶</a></h1>
<div class="section" id="terminology">
<span id="id2"></span><h2>Terminology<a class="headerlink" href="#terminology" title="Permalink to this headline">¶</a></h2>
<p><strong>compute kernel :</strong></p>
<p>To define a compute kernel, it is first necessary to define a kernel. A kernel is a small unit of execution that performs a clearly defined function and that can be executed in parallel. Such a kernel can be executed on each element of an input stream (called an NDRange), or simply at each point in an arbitrary index space. A kernel is analogous and, on some devices identical, to what graphics programmers call a shader program. This kernel is not to be confused with an OS kernel, which controls hardware. The most basic form of an NDRange is simply mapped over input data and produces one output item for each input tuple. Subsequent extensions of the basic model provide random-access functionality, variable output counts, and reduction/accumulation operations. Kernels are specified using the kernel keyword.</p>
<p>A compute kernel is a specific type of kernel that is not part of the traditional graphics pipeline. The compute kernel type can be used for graphics, but its strength lies in using it for non-graphics fields such as physics, AI, modeling, HPC, and various other computationally intensive applications.</p>
<p>In a compute kernel, the work-item spawn order is sequential. This means that on a chip with N work-items per wavefront, the first N work- items go to wavefront 1, the second N work-items go to wavefront 2, etc. Thus, the work-item IDs for wavefront K are in the range (K•N) to ((K+1)•N)-1.</p>
<p><strong>wavefronts and work-groups :</strong></p>
<p>Wavefronts and work-groups are two concepts relating to compute kernels that provide data-parallel granularity. On most AMD GPUs, a wavefront has 64 work-items. A wavefront is the lowest level that flow control can affect. This means that if two work-items inside of a wavefront go divergent paths of flow control, all work-items in the wavefront go to both paths of flow control.</p>
<p>Grouping is a higher-level granularity of data parallelism that is enforced in software, not hardware. Synchronization points in a kernel guarantee that all work-items in a work-group reach that point (barrier) in the code before the next statement is executed.</p>
<p>Work-groups are composed of wavefronts. Best performance is attained when the group size is an integer multiple of the wavefront size.</p>
<p><strong>local data store(LDS) :</strong></p>
<p>The LDS is a high-speed, low-latency memory private to each compute unit. It is a full gather/scatter model: a work-group can write anywhere in its allocated space. This model is unchanged for the AMD Radeon™ HD 7XXX series. The constraints of the current LDS model are:</p>
<blockquote>
<div><ul class="simple">
<li>The LDS size is allocated per work-group. Each work-group specifies how much of the LDS it requires. The hardware scheduler uses     this information to determine which work groups can share a compute unit.</li>
<li>Data can only be shared within work-items in a work-group.</li>
<li>Memory accesses outside of the work-group result in undefined behavior.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="opencl-overview">
<span id="id3"></span><h2>OpenCL Overview<a class="headerlink" href="#opencl-overview" title="Permalink to this headline">¶</a></h2>
<p>The OpenCL programming model consists of producing complicated task graphs from data-parallel execution nodes.</p>
<p>In a given data-parallel execution, commonly known as a kernel launch, a computation is defined in terms of a sequence of instructions that executes at each point in an N-dimensional index space. It is a common, though by not required, formulation of an algorithm that each computation index maps to an element in an input data set.</p>
<p>The OpenCL data-parallel programming model is hierarchical. The hierarchical subdivision can be specified in two ways:</p>
<blockquote>
<div><ul class="simple">
<li>Explicitly - the developer defines the total number of work-items to execute in parallel, as well as the division of work-items     into specific work-groups.</li>
<li>Implicitly - the developer specifies the total number of work-items to execute in parallel, and OpenCL manages the division into     work-groups.</li>
</ul>
</div></blockquote>
<p>OpenCL’s API also supports the concept of a task dispatch. This is equivalent to executing a kernel on a compute device with a work-group and NDRange containing a single work-item. Parallelism is expressed using vector data types implemented by the device, enqueuing multiple tasks, and/or enqueuing native kernels developed using a programming model orthogonal to OpenCL.</p>
</div>
<div class="section" id="programming-model">
<span id="id4"></span><h2>Programming Model<a class="headerlink" href="#programming-model" title="Permalink to this headline">¶</a></h2>
<p>The OpenCL programming model is based on the notion of a host device, supported by an application API, and a number of devices connected through a bus. These are programmed using OpenCL C. The host API is divided into platform and runtime layers. OpenCL C is a C-like language with extensions for parallel programming such as memory fence operations and barriers. Figure illustrates this model with queues of commands, reading/writing data, and
executing kernels for specific devices.</p>
<img alt="../_images/img1.png" class="align-center" src="../_images/img1.png" />
<p>The devices are capable of running data- and task-parallel work. A kernel can be executed as a function of multi-dimensional domains of indices. Each element is called a work-item; the total number of indices is defined as the global work-size. The global work-size can be divided into sub-domains, called work-groups, and individual work-items within a group can communicate through global or locally shared memory. Work-items are synchronized through barrier or fence operations. Figure 1.1 is a representation of the host/device architecture with a single platform, consisting of a GPU and a CPU.</p>
<p>An OpenCL application is built by first querying the runtime to determine which platforms are present. There can be any number of different OpenCL implementations installed on a single system. The desired OpenCL platform can be selected by matching the platform vendor string to the desired vendor name, such as “Advanced Micro Devices, Inc.” The next step is to create a context. As shown in Figure 1.1, an OpenCL context has associated with it a number of compute devices (for example, CPU or GPU devices),. Within a context, OpenCL guarantees a relaxed consistency between these devices. This means that memory objects, such as buffers or images, are allocated per context; but changes made by one device are only guaranteed to be visible by another device at well-defined synchronization points. For this, OpenCL provides events, with the ability to synchronize on a given event to enforce the correct order of execution.</p>
<p>Many operations are performed with respect to a given context; there also are many operations that are specific to a device. For example, program compilation and kernel execution are done on a per-device basis. Performing work with a device, such as executing kernels or moving data to and from the device’s local memory, is done using a corresponding command queue. A command queue is associated with a single device and a given context; all work for a specific device is done through this interface. Note that while a single command queue can be associated with only a single device, there is no limit to the number of command queues that can point to the same device. For example, it is possible to have one command queue for executing kernels and a command queue for managing data transfers between the host and the device.</p>
<p>Most OpenCL programs follow the same pattern. Given a specific platform, select a device or devices to create a context, allocate memory, create device-specific command queues, and perform data transfers and computations. Generally, the platform is the gateway to accessing specific devices, given these devices and a corresponding context, the application is independent of the platform. Given a context, the application can:</p>
<blockquote>
<div><ul class="simple">
<li>Create one or more command queues.</li>
<li>Create programs to run on one or more associated devices.</li>
<li>Create kernels within those programs.</li>
<li>Allocate memory buffers or images, either on the host or on the device(s). (Memory can be copied between the host and device.)</li>
<li>Write data to the device.</li>
<li>Submit the kernel (with appropriate arguments) to the command queue for execution.</li>
<li>Read data back to the host from the device.</li>
</ul>
</div></blockquote>
<p>The relationship between context(s), device(s), buffer(s), program(s), kernel(s), and command queue(s) is best seen by looking at sample code. For an example, see the HelloWorld sample in the AMD Compute SDK.</p>
</div>
<div class="section" id="synchronization">
<span id="id5"></span><h2>Synchronization<a class="headerlink" href="#synchronization" title="Permalink to this headline">¶</a></h2>
<p>The two domains of synchronization in OpenCL are work-items in a single work- group and command-queue(s) in a single context. Work-group barriers enable synchronization of work-items in a work-group. Each work-item in a work-group must first execute the barrier before executing any instruction beyond this barrier. Either all of, or none of, the work-items in a work-group must encounter the barrier. A barrier or mem_fence operation does not have global scope, but is relevant only to the local workgroup on which they operate.</p>
<p>There are two types of synchronization between commands in a command- queue:</p>
<blockquote>
<div><ul class="simple">
<li>command-queue barrier - enforces ordering within a single queue. Any resulting changes to memory are available to the following      commands in the queue.</li>
<li>events - enforces ordering between, or within, queues. Enqueued commands in OpenCL return an event identifying the command as well    as the memory object updated by it. This ensures that following commands waiting on that event see the updated memory objects          before they execute.</li>
</ul>
</div></blockquote>
<p>OpenCL 2.0 provides additional synchronization options. For an overview, see “Atomics and synchronization.”.</p>
</div>
<div class="section" id="memory-architecture-and-access">
<span id="memory-arch"></span><h2>Memory Architecture and Access<a class="headerlink" href="#memory-architecture-and-access" title="Permalink to this headline">¶</a></h2>
<p>OpenCL has four memory domains: private, local, global, and constant; the AMD Compute Technology system also recognizes host (CPU) and PCI Express®  (PCIe® ) memory.</p>
<table border="1" class="docutils">
<colgroup>
<col width="6%" />
<col width="94%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Memory Type</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>private</td>
<td>Specific to a work-item; it is not visible to other work-items.</td>
</tr>
<tr class="row-odd"><td>local</td>
<td>Specific to a work-group; accessible only by work-items belonging to that work-group.</td>
</tr>
<tr class="row-even"><td>global</td>
<td>Accessible to all work-items executing in a context, as well as to the host
(read, write, and map commands).</td>
</tr>
<tr class="row-odd"><td>constant</td>
<td>Read-only region for host-allocated and -initialized objects that are not changed during kernel execution.</td>
</tr>
<tr class="row-even"><td>host (CPU)</td>
<td>Host-accessible region for an application’s data structures and program data.</td>
</tr>
<tr class="row-odd"><td>PCIe</td>
<td>Part of host (CPU) memory accessible from, and modifiable by, the host program and the GPU compute device. Modifying                    this memory requires synchronization between the GPU compute device and the CPU.</td>
</tr>
</tbody>
</table>
<blockquote>
<div><strong>Table: illustrates the interrelationship of the memories.</strong></div></blockquote>
<img alt="../_images/img2.png" class="align-center" src="../_images/img2.png" />
<img alt="../_images/img3.png" class="align-center" src="../_images/img3.png" />
<p>There are two ways to copy data from the host to the GPU compute device memory:</p>
<blockquote>
<div><ul class="simple">
<li>Implicitly by using <code class="docutils literal notranslate"><span class="pre">clEnqueueMapBuffer</span></code> and <code class="docutils literal notranslate"><span class="pre">clEnqueueUnMapMemObject</span></code>.</li>
<li>Explicitly through  <code class="docutils literal notranslate"><span class="pre">clEnqueueReadBuffer</span></code>, <code class="docutils literal notranslate"><span class="pre">clEnqueueWriteBuffer</span></code> and <code class="docutils literal notranslate"><span class="pre">(clEnqueueReadImage,</span> <span class="pre">clEnqueueWriteImage).</span></code></li>
</ul>
</div></blockquote>
<p>When using these interfaces, it is important to consider the amount of copying involved. There is a two-copy processes: between host and PCIe, and between PCIe and GPU compute device.</p>
<p>With proper memory transfer management and the use of system pinned memory (host/CPU memory remapped to the PCIe memory space), copying between host (CPU) memory and PCIe memory can be skipped.</p>
<p>Double copying lowers the overall system memory bandwidth. In GPU compute device programming, pipelining and other techniques help reduce these bottlenecks. See the AMD OpenCL Optimization Reference Guide for more specifics about optimization techniques.</p>
<div class="section" id="data-share-operations">
<h3>Data Share Operations<a class="headerlink" href="#data-share-operations" title="Permalink to this headline">¶</a></h3>
<p>Local data share (LDS) is a very low-latency, RAM scratchpad for temporary data located within each compute unit. The programmer explicitly controls all accesses to the LDS. The LDS can thus provide efficient memory access when used as a software cache for predictable re-use of data (such as holding parameters for pixel shader parameter interpolation), as a data exchange machine for the work-items of a work-group, or as a cooperative way to enable more efficient access to off-chip memory.</p>
<p>The high-speed write-to-read re-use of the memory space (full gather/read/load and scatter/write/store operations) is especially useful in pre-GCN devices with read-only caches. LDS offers at least one order of magnitude higher effective bandwidth than direct, uncached global memory.</p>
<p>Figure 1.4 shows the conceptual framework of the LDS is integration into the memory of AMD GPUs using OpenCL.</p>
<img alt="../_images/img4.png" class="align-center" src="../_images/img4.png" />
<p>Physically located on-chip, directly next to the ALUs, the LDS is approximately one order of magnitude faster than global memory (assuming no bank conflicts).</p>
<p>GCN devices contain 64 kB memory per compute unit and allow up to a maximum of 32 kB per workgroup.</p>
<p>The high bandwidth of the LDS memory is achieved not only through its proximity to the ALUs, but also through simultaneous access to its memory banks. Thus, it is possible to concurrently execute 32 write or read instructions, each nominally
32-bits; extended instructions, read2/write2, can be 64-bits each. If, however, more than one access attempt is made to the same bank at the same time, a bank conflict occurs. In this case, for indexed and atomic operations, hardware prevents the attempted concurrent accesses to the same bank by turning them into serial accesses. This decreases the effective bandwidth of the LDS. For maximum throughput (optimal efficiency), therefore, it is important to avoid bank conflicts. A knowledge of request scheduling and address mapping is key to achieving this.</p>
</div>
<div class="section" id="dataflow-in-memory-hierarchy">
<h3>Dataflow in Memory Hierarchy<a class="headerlink" href="#dataflow-in-memory-hierarchy" title="Permalink to this headline">¶</a></h3>
<img alt="../_images/img5.png" class="align-center" src="../_images/img5.png" />
<p>To load data into LDS from global memory, it is read from global memory and placed into the work-item’s registers; then, a store is performed to LDS. Similarly, to store data into global memory, data is read from LDS and placed into the work- item’s registers, then placed into global memory. To make effective use of the LDS, an algorithm must perform many operations on what is transferred between global memory and LDS. It also is possible to load data from a memory buffer directly into LDS, bypassing VGPRs.</p>
<p>LDS atomics are performed in the LDS hardware. (Thus, although ALUs are not directly used for these operations, latency is incurred by the LDS executing this function.) If the algorithm does not require write-to-read reuse (the data is read only), it usually is better to use the image dataflow (see right side of Figure 1.5) because of the cache hierarchy.</p>
<p>Actually, buffer reads may use L1 and L2. When caching is not used for a buffer, reads from that buffer bypass L2. After a buffer read, the line is invalidated; then, on the next read, it is read again (from the same wavefront or from a different clause). After a buffer write, the changed parts of the cache line are written to memory.</p>
<p>Buffers and images are written through the texture L2 cache, but this is flushed immediately after an image write.</p>
<p>In GCN devices, both reads and writes happen through L1 and L2.</p>
<p>The data in private memory is first placed in registers. If more private memory is used than can be placed in registers, or dynamic indexing is used on private arrays, the overflow data is placed (spilled) into scratch memory. Scratch memory is a private subset of global memory, so performance can be dramatically degraded if spilling occurs.</p>
<p>Global memory can be in the high-speed GPU memory (VRAM) or in the host memory, which is accessed by the PCIe bus. A work-item can access global memory either as a buffer or a memory object. Buffer objects are generally read and written directly by the work-items. Data is accessed through the L2 and L1 data caches on the GPU. This limited form of caching provides read coalescing among work-items in a wavefront. Similarly, writes are executed through the texture L2 cache.</p>
<p>Global atomic operations are executed through the texture L2 cache. Atomic instructions that return a value to the kernel are handled similarly to fetch instructions: the kernel must use S_WAITCNT to ensure the results have been written to the destination GPR before using the data.</p>
</div>
<div class="section" id="memory-access">
<h3>Memory Access<a class="headerlink" href="#memory-access" title="Permalink to this headline">¶</a></h3>
<p>Using local memory (known as local data store, or LDS, as shown in Figure 1.2) typically is an order of magnitude faster than accessing host memory through global memory (VRAM), which is one order of magnitude faster again than PCIe. However, stream cores do not directly access memory; instead, they issue memory requests through dedicated hardware units. When a work-item tries to access memory, the work-item is transferred to the appropriate fetch unit. The work-item then is deactivated until the access unit finishes accessing memory. Meanwhile, other work-items can be active within the compute unit, contributing to better performance. The data fetch units handle three basic types of memory operations: loads, stores, and streaming stores. GPU compute devices can store writes to random memory locations using global buffers.</p>
</div>
<div class="section" id="global-memory">
<h3>Global Memory<a class="headerlink" href="#global-memory" title="Permalink to this headline">¶</a></h3>
<p>The global memory lets applications read from, and write to, arbitrary locations in memory. When using global memory, such read and write operations from the stream kernel are done using regular GPU compute device instructions with the global memory used as the source or destination for the instruction. The programming interface is similar to load/store operations used with CPU programs, where the relative address in the read/write buffer is specified.</p>
<p>When using a global memory, each work-item can write to an arbitrary location within it. Global memory use a linear layout. If consecutive addresses are written, the compute unit issues a burst write for more efficient memory access. Only read-only buffers, such as constants, are cached.</p>
</div>
<div class="section" id="image-read-write">
<h3>Image Read/Write<a class="headerlink" href="#image-read-write" title="Permalink to this headline">¶</a></h3>
<p>Image reads are done by addressing the desired location in the input memory using the fetch unit. The fetch units can process either 1D or 2 D addresses. These addresses can be normalized or un-normalized. Normalized coordinates are between 0.0 and 1.0 (inclusive). For the fetch units to handle 2D addresses and normalized coordinates, pre-allocated memory segments must be bound to the fetch unit so that the correct memory address can be computed. For a single kernel invocation, up to 128 images can be bound at once for reading, and eight for writing. The maximum number of addresses is 8192x8192 for Evergreen and Northern Islands-based devices, 16384x16384 for SI-based products.</p>
<p>Image reads are cached through the texture system (corresponding to the L2 and
L1 caches).</p>
</div>
</div>
<div class="section" id="example-programs">
<span id="example"></span><h2>Example Programs<a class="headerlink" href="#example-programs" title="Permalink to this headline">¶</a></h2>
<p>The following subsections provide simple programming examples with explanatory comments.</p>
<div class="section" id="first-example-simple-buffer-write">
<h3>First Example: Simple Buffer Write<a class="headerlink" href="#first-example-simple-buffer-write" title="Permalink to this headline">¶</a></h3>
<p>This sample shows a minimalist OpenCL C program that sets a given buffer to some value. It illustrates the basic programming steps with a minimum amount of code. This sample contains no error checks and the code is not generalized. Yet, many simple test programs might look very similar. The entire code for this sample is provided at the end of this section.</p>
<ol class="arabic simple">
<li>The host program must select a platform, which is an abstraction for a given OpenCL implementation. Implementations by multiple vendors can coexist on a host, and the sample uses the first one available.</li>
<li>A device id for a GPU device is requested. A CPU device could be requested by using CL_DEVICE_TYPE_CPU instead. The device can be a physical device, such as a given GPU, or an abstracted device, such as the collection of all CPU cores on the host.</li>
<li>On the selected device, an OpenCL context is created. A context ties together a device, memory buffers related to that device, OpenCL programs, and command queues. Note that buffers related to a device can reside on either the host or the device. Many OpenCL programs have only a single context, program, and command queue.</li>
<li>Before an OpenCL kernel can be launched, its program source is compiled, and a handle to the kernel is created.</li>
<li>A memory buffer is allocated in the context.</li>
<li>The kernel is launched. While it is necessary to specify the global work size, OpenCL determines a good local work size for this device. Since the kernel was launch asynchronously, <code class="docutils literal notranslate"><span class="pre">clFinish()</span></code> is used to wait for completion.</li>
<li>The data is mapped to the host for examination. Calling clEnqueueMapBuffer ensures the visibility of the buffer on the host, which in this case probably includes a physical transfer. Alternatively, we could use <code class="docutils literal notranslate"><span class="pre">clEnqueueWriteBuffer()</span></code>, which requires a pre-allocated host-side buffer.</li>
</ol>
<p><strong>Example Code 1</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span>
<span class="o">//</span> <span class="n">Copyright</span> <span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="mi">2010</span> <span class="n">Advanced</span> <span class="n">Micro</span> <span class="n">Devices</span><span class="p">,</span> <span class="n">Inc</span><span class="o">.</span> <span class="n">All</span> <span class="n">rights</span> <span class="n">reserved</span><span class="o">.</span>
<span class="o">//</span>

<span class="o">//</span> <span class="n">A</span> <span class="n">minimalist</span> <span class="n">OpenCL</span> <span class="n">program</span><span class="o">.</span>

<span class="c1">#include &lt;CL/cl.h&gt;</span>
<span class="c1">#include &lt;stdio.h&gt;</span>

<span class="c1">#define NWITEMS 512</span>
<span class="o">//</span> <span class="n">A</span> <span class="n">simple</span> <span class="n">memset</span> <span class="n">kernel</span>
<span class="n">const</span> <span class="n">char</span> <span class="o">*</span><span class="n">source</span> <span class="o">=</span>
<span class="s2">&quot;kernel void memset(   global uint *dst )             </span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="s2">&quot;{                                                    </span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="s2">&quot;    dst[get_global_id(0)] = get_global_id(0);        </span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="s2">&quot;}                                                    </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">;</span>

<span class="nb">int</span> <span class="n">main</span><span class="p">(</span><span class="nb">int</span> <span class="n">argc</span><span class="p">,</span> <span class="n">char</span> <span class="o">**</span> <span class="n">argv</span><span class="p">)</span>
<span class="p">{</span>
  <span class="o">//</span> <span class="mf">1.</span> <span class="n">Get</span> <span class="n">a</span> <span class="n">platform</span><span class="o">.</span>
  <span class="n">cl_platform_id</span> <span class="n">platform</span><span class="p">;</span>
  <span class="n">clGetPlatformIDs</span><span class="p">(</span> <span class="mi">1</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">platform</span><span class="p">,</span> <span class="n">NULL</span> <span class="p">);</span>

  <span class="o">//</span> <span class="mf">2.</span> <span class="n">Find</span> <span class="n">a</span> <span class="n">gpu</span> <span class="n">device</span><span class="o">.</span>
  <span class="n">cl_device_id</span> <span class="n">device</span><span class="p">;</span>
  <span class="n">clGetDeviceIDs</span><span class="p">(</span> <span class="n">platform</span><span class="p">,</span>
                  <span class="n">CL_DEVICE_TYPE_GPU</span><span class="p">,</span>
                  <span class="mi">1</span><span class="p">,</span>
                  <span class="o">&amp;</span><span class="n">device</span><span class="p">,</span> <span class="n">NULL</span><span class="p">);</span>

  <span class="o">//</span> <span class="mf">3.</span> <span class="n">Create</span> <span class="n">a</span> <span class="n">context</span> <span class="ow">and</span> <span class="n">command</span> <span class="n">queue</span> <span class="n">on</span> <span class="n">that</span> <span class="n">device</span><span class="o">.</span>
  <span class="n">cl_context</span> <span class="n">context</span> <span class="o">=</span> <span class="n">clCreateContext</span><span class="p">(</span> <span class="n">NULL</span><span class="p">,</span>
                                        <span class="mi">1</span><span class="p">,</span>
                                        <span class="o">&amp;</span><span class="n">device</span><span class="p">,</span>
                                        <span class="n">NULL</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="n">NULL</span><span class="p">);</span>

  <span class="n">cl_command_queue</span> <span class="n">queue</span> <span class="o">=</span> <span class="n">clCreateCommandQueue</span><span class="p">(</span> <span class="n">context</span><span class="p">,</span>
                                                 <span class="n">device</span><span class="p">,</span>
                                                 <span class="mi">0</span><span class="p">,</span> <span class="n">NULL</span> <span class="p">);</span>

  <span class="o">//</span> <span class="mf">4.</span> <span class="n">Perform</span> <span class="n">runtime</span> <span class="n">source</span> <span class="n">compilation</span><span class="p">,</span> <span class="ow">and</span> <span class="n">obtain</span> <span class="n">kernel</span> <span class="n">entry</span> <span class="n">point</span><span class="o">.</span>
  <span class="n">cl_program</span> <span class="n">program</span> <span class="o">=</span> <span class="n">clCreateProgramWithSource</span><span class="p">(</span> <span class="n">context</span><span class="p">,</span>
                                                  <span class="mi">1</span><span class="p">,</span>
                                                  <span class="o">&amp;</span><span class="n">source</span><span class="p">,</span>
                                                  <span class="n">NULL</span><span class="p">,</span> <span class="n">NULL</span> <span class="p">);</span>

  <span class="n">clBuildProgram</span><span class="p">(</span> <span class="n">program</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">device</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="n">NULL</span> <span class="p">);</span>

  <span class="n">cl_kernel</span> <span class="n">kernel</span> <span class="o">=</span> <span class="n">clCreateKernel</span><span class="p">(</span> <span class="n">program</span><span class="p">,</span> <span class="s2">&quot;memset&quot;</span><span class="p">,</span> <span class="n">NULL</span> <span class="p">);</span>

  <span class="o">//</span> <span class="mf">5.</span> <span class="n">Create</span> <span class="n">a</span> <span class="n">data</span> <span class="n">buffer</span><span class="o">.</span>
  <span class="n">cl_mem</span> <span class="n">buffer</span> <span class="o">=</span> <span class="n">clCreateBuffer</span><span class="p">(</span> <span class="n">context</span><span class="p">,</span>
                                  <span class="n">CL_MEM_WRITE_ONLY</span><span class="p">,</span>
                                  <span class="n">NWITEMS</span> <span class="o">*</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">cl_uint</span><span class="p">),</span>
                                  <span class="n">NULL</span><span class="p">,</span> <span class="n">NULL</span> <span class="p">);</span>

  <span class="o">//</span> <span class="mf">6.</span> <span class="n">Launch</span> <span class="n">the</span> <span class="n">kernel</span><span class="o">.</span> <span class="n">Let</span> <span class="n">OpenCL</span> <span class="n">pick</span> <span class="n">the</span> <span class="n">local</span> <span class="n">work</span> <span class="n">size</span><span class="o">.</span>
  <span class="n">size_t</span> <span class="n">global_work_size</span> <span class="o">=</span> <span class="n">NWITEMS</span><span class="p">;</span>
  <span class="n">clSetKernelArg</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">buffer</span><span class="p">),</span> <span class="p">(</span><span class="n">void</span><span class="o">*</span><span class="p">)</span> <span class="o">&amp;</span><span class="n">buffer</span><span class="p">);</span>

  <span class="n">clEnqueueNDRangeKernel</span><span class="p">(</span> <span class="n">queue</span><span class="p">,</span>
                          <span class="n">kernel</span><span class="p">,</span>
                          <span class="mi">1</span><span class="p">,</span>
                          <span class="n">NULL</span><span class="p">,</span>
                          <span class="o">&amp;</span><span class="n">global_work_size</span><span class="p">,</span>
                          <span class="n">NULL</span><span class="p">,</span>
                          <span class="mi">0</span><span class="p">,</span>
                          <span class="n">NULL</span><span class="p">,</span> <span class="n">NULL</span><span class="p">);</span>

  <span class="n">clFinish</span><span class="p">(</span> <span class="n">queue</span> <span class="p">);</span>

  <span class="o">//</span> <span class="mf">7.</span> <span class="n">Look</span> <span class="n">at</span> <span class="n">the</span> <span class="n">results</span> <span class="n">via</span> <span class="n">synchronous</span> <span class="n">buffer</span> <span class="nb">map</span><span class="o">.</span>
  <span class="n">cl_uint</span> <span class="o">*</span><span class="n">ptr</span><span class="p">;</span>
  <span class="n">ptr</span> <span class="o">=</span> <span class="p">(</span><span class="n">cl_uint</span> <span class="o">*</span><span class="p">)</span> <span class="n">clEnqueueMapBuffer</span><span class="p">(</span> <span class="n">queue</span><span class="p">,</span>
                                        <span class="n">buffer</span><span class="p">,</span>
                                        <span class="n">CL_TRUE</span><span class="p">,</span>
                                        <span class="n">CL_MAP_READ</span><span class="p">,</span>
                                        <span class="mi">0</span><span class="p">,</span>
                                        <span class="n">NWITEMS</span> <span class="o">*</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">cl_uint</span><span class="p">),</span>
                                        <span class="mi">0</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="n">NULL</span> <span class="p">);</span>

  <span class="nb">int</span> <span class="n">i</span><span class="p">;</span>

  <span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NWITEMS</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
      <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%d</span><span class="s2"> </span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>

  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="example-saxpy-function">
<h3>Example: SAXPY Function<a class="headerlink" href="#example-saxpy-function" title="Permalink to this headline">¶</a></h3>
<p>This section provides an introductory sample for beginner-level OpenCL
programmers using C++ bindings.</p>
<p>The sample implements the SAXPY function (Y = aX + Y, where X and Y are vectors, and a is a scalar). The full code is reproduced at the end of this section. It uses C++ bindings for OpenCL. These bindings are available in the CL/cl.hpp file in the AMD Compute SDK; they also are downloadable from the Khronos website: <a class="reference external" href="http://www.khronos.org/registry/cl">http://www.khronos.org/registry/cl</a></p>
<p>The following steps guide you through this example.</p>
<ol class="arabic">
<li><p class="first">Enable error checking through the exception handling mechanism in the C++
bindings by using the following define.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#define  CL ENABLE_EXCEPTIONS</span>
</pre></div>
</div>
<p>This removes the need to error check after each OpenCL call. If there is an error, the C++ bindings code throw an exception that is caught at the end of the try block, where we can clean up the host memory allocations. In this example, the C++ object representing OpenCL resources (cl::Context, cl::CommandQueue, etc.) are declared as automatic variables, so they do not need to be released. If an OpenCL call returns an error, the error code is defined in the CL/cl.h file.</p>
</li>
<li><p class="first">The kernel is very simple: each work-item, i, does the SAXPY calculation for its corresponding elements <code class="docutils literal notranslate"><span class="pre">Y[i]</span> <span class="pre">=</span> <span class="pre">aX[i]</span> <span class="pre">+</span> <span class="pre">Y[i]</span></code>. Both X and Y vectors are stored in global memory; X is read-only, Y is read-write.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kernel</span> <span class="n">void</span> <span class="n">saxpy</span><span class="p">(</span><span class="n">const</span> <span class="n">__global</span> <span class="nb">float</span> <span class="o">*</span> <span class="n">X</span><span class="p">,</span>
                        <span class="n">__global</span> <span class="nb">float</span> <span class="o">*</span> <span class="n">Y</span><span class="p">,</span>
                  <span class="n">const</span> <span class="nb">float</span> <span class="n">a</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">uint</span> <span class="n">gid</span> <span class="o">=</span> <span class="n">get_global_id</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
  <span class="n">Y</span><span class="p">[</span><span class="n">gid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">gid</span><span class="p">]</span> <span class="o">+</span> <span class="n">Y</span><span class="p">[</span><span class="n">gid</span><span class="p">];</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p class="first">List all platforms on the machine, then select one.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cl</span><span class="p">::</span><span class="n">Platform</span><span class="p">::</span><span class="n">get</span><span class="p">(</span><span class="o">&amp;</span><span class="n">platforms</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p class="first">Create an OpenCL context on that platform.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cl_context_properties</span> <span class="n">cps</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span> <span class="n">CL_CONTEXT_PLATFORM</span><span class="p">,</span> <span class="p">(</span><span class="n">cl_context_properties</span><span class="p">)(</span><span class="o">*</span><span class="nb">iter</span><span class="p">)(),</span> <span class="mi">0</span> <span class="p">};</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">cl</span><span class="p">::</span><span class="n">Context</span><span class="p">(</span><span class="n">CL_DEVICE_TYPE_GPU</span><span class="p">,</span> <span class="n">cps</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p class="first">Get OpenCL devices from the context.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">devices</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">getInfo</span><span class="o">&lt;</span><span class="n">CL_CONTEXT_DEVICES</span><span class="o">&gt;</span><span class="p">();</span>
</pre></div>
</div>
</li>
<li><p class="first">Create an OpenCL command queue.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">queue</span> <span class="o">=</span> <span class="n">cl</span><span class="p">::</span><span class="n">CommandQueue</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
</pre></div>
</div>
</li>
<li><p class="first">Create two buffers, corresponding to the X and Y vectors. Ensure the host- side buffers, pX and pY, are allocated and initialized.    The CL_MEM_COPY_HOST_PTR flag instructs the runtime to copy over the contents of the host pointer pX in order to initialize the        buffer bufX. The bufX buffer uses the CL_MEM_READ_ONLY flag, while bufY requires the CL_MEM_READ_WRITE flag.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bufX</span> <span class="o">=</span> <span class="n">cl</span><span class="p">::</span><span class="n">Buffer</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">CL_MEM_READ_ONLY</span> <span class="o">|</span> <span class="n">CL_MEM_COPY_HOST_PTR</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">cl_float</span><span class="p">)</span> <span class="o">*</span> <span class="n">length</span><span class="p">,</span> <span class="n">pX</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p class="first">Create a program object from the kernel source string, build the program for our devices, and create a kernel object corresponding to the SAXPY kernel. (At this point, it is possible to create multiple kernel objects if there are more than one.)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cl</span><span class="p">::</span><span class="n">Program</span><span class="p">::</span><span class="n">Sources</span> <span class="n">sources</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">std</span><span class="p">::</span><span class="n">make_pair</span><span class="p">(</span><span class="n">kernelStr</span><span class="o">.</span><span class="n">c_str</span><span class="p">(),</span> <span class="n">kernelStr</span><span class="o">.</span><span class="n">length</span><span class="p">()));</span>
<span class="n">program</span> <span class="o">=</span> <span class="n">cl</span><span class="p">::</span><span class="n">Program</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">sources</span><span class="p">);</span>
<span class="n">program</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">devices</span><span class="p">);</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="n">cl</span><span class="p">::</span><span class="n">Kernel</span><span class="p">(</span><span class="n">program</span><span class="p">,</span> <span class="s2">&quot;saxpy&quot;</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p class="first">Enqueue the kernel for execution on the device (GPU in our example).</p>
<p>Set each argument individually in separate kernel.setArg() calls. The arguments, do not need to be set again for subsequent kernelenqueue calls. Reset only those arguments that are to pass a new value to the kernel. Then, enqueue the kernel to the command queue with the appropriate global and local work sizes.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kernel</span><span class="o">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">bufX</span><span class="p">);</span> <span class="n">kernel</span><span class="o">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">bufY</span><span class="p">);</span> <span class="n">kernel</span><span class="o">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">a</span><span class="p">);</span>
<span class="n">queue</span><span class="o">.</span><span class="n">enqueueNDRangeKernel</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">cl</span><span class="p">::</span><span class="n">NDRange</span><span class="p">(),</span> <span class="n">cl</span><span class="p">::</span><span class="n">NDRange</span><span class="p">(</span><span class="n">length</span><span class="p">),</span> <span class="n">cl</span><span class="p">::</span><span class="n">NDRange</span><span class="p">(</span><span class="mi">64</span><span class="p">));</span>
</pre></div>
</div>
</li>
<li><p class="first">Read back the results from bufY to the host pointer pY. We will make this a blocking call (using the CL_TRUE argument) since we     do not want to proceed before the kernel has finished execution and we have our results back.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">queue</span><span class="o">.</span><span class="n">enqueueReadBuffer</span><span class="p">(</span><span class="n">bufY</span><span class="p">,</span> <span class="n">CL_TRUE</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">length</span> <span class="o">*</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">cl_float</span><span class="p">),</span> <span class="n">pY</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p class="first">Clean up the host resources (pX and pY). OpenCL resources is cleaned up by the C++ bindings support code.</p>
<p>The catch(cl::Error err) block handles exceptions thrown by the C++ bindings code. If there is an OpenCL call error, it prints      out the name of the call and the error code (codes are defined in CL/cl.h). If there is a kernel compilation error, the error           code is CL_BUILD_PROGRAM_FAILURE, in which case it is necessary to print out the build log.</p>
</li>
</ol>
<p><strong>Example Code 2</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>#define __CL_ENABLE_EXCEPTIONS

#include &lt;CL/cl.hpp&gt;
#include &lt;string&gt;
#include &lt;iostream&gt;
#include &lt;string&gt;

using std::cout;
using std::cerr;
using std::endl;
using std::string;

/////////////////////////////////////////////////////////////////
// Helper function to print vector elements
/////////////////////////////////////////////////////////////////
void printVector(const std::string arrayName,
                 const cl_float * arrayData,
                 const unsigned int length)
{
  int numElementsToPrint = (256 &lt; length) ? 256 : length;
  cout &lt;&lt; endl &lt;&lt; arrayName &lt;&lt; &quot;:&quot; &lt;&lt; endl;
  for(int i = 0; i &lt; numElementsToPrint; ++i)
    cout &lt;&lt; arrayData[i] &lt;&lt; &quot; &quot;;
  cout &lt;&lt; endl;
}

/////////////////////////////////////////////////////////////////
// Globals
/////////////////////////////////////////////////////////////////
int length      = 256;
cl_float * pX   = NULL;
cl_float * pY   = NULL;
cl_float a      = 2.f;

std::vector&lt;cl::Platform&gt; platforms;
cl::Context context;
std::vector&lt;cl::Device&gt; devices;
cl::CommandQueue queue;
cl::Program program;

cl::Kernel kernel;
cl::Buffer bufX;
cl::Buffer bufY;

/////////////////////////////////////////////////////////////////
// The saxpy kernel
/////////////////////////////////////////////////////////////////
string kernelStr      =
    &quot;__kernel void saxpy(const global float * x,\n&quot;
    &quot;                    __global float * y,\n&quot;
    &quot;                    const float a)\n&quot;
    &quot;{\n&quot;
    &quot;   uint gid = get_global_id(0);\n&quot;
    &quot;   y[gid] = a* x[gid] + y[gid];\n&quot;
    &quot;}\n&quot;;

/////////////////////////////////////////////////////////////////
// Allocate and initialize memory on the host
/////////////////////////////////////////////////////////////////
void initHost()
{
  size_t sizeInBytes = length * sizeof(cl_float);
  pX = (cl_float *) malloc(sizeInBytes);
  if(pX == NULL)
    throw(string(&quot;Error: Failed to allocate input memory on host\n&quot;));

  pY = (cl_float *) malloc(sizeInBytes);
  if(pY == NULL)
    throw(string(&quot;Error: Failed to allocate input memory on host\n&quot;));

  for(int i = 0; i &lt; length; i++)
  {
    pX[i] = cl_float(i);
    pY[i] = cl_float(length-1-i);
  }

  printVector(&quot;X&quot;, pX, length);
  printVector(&quot;Y&quot;, pY, length);
}

/////////////////////////////////////////////////////////////////
// Release host memory
/////////////////////////////////////////////////////////////////
void cleanupHost()
{
  if(pX)
  {
    free(pX);
    pX = NULL;
  }
  if(pY != NULL)
  {
    free(pY);
    pY = NULL;
  }
}

int main(int argc, char * argv[])
{
  try
  {
    /////////////////////////////////////////////////////////////////
    // Allocate and initialize memory on the host
    /////////////////////////////////////////////////////////////////
    initHost();

    /////////////////////////////////////////////////////////////////
    // Find the platform
    /////////////////////////////////////////////////////////////////
    cl::Platform::get(&amp;platforms);
    std::vector&lt;cl::Platform&gt;::iterator iter;
    for(iter = platforms.begin(); iter != platforms.end(); ++iter)
    {
      if( !strcmp((*iter).getInfo&lt;CL_PLATFORM_VENDOR&gt;().c_str(), &quot;Advanced Micro Devices, Inc.&quot;) )
      {
        break;
      }
    }

    /////////////////////////////////////////////////////////////////
    // Create an OpenCL context
    /////////////////////////////////////////////////////////////////
    cl_context_properties cps[3] = { CL_CONTEXT_PLATFORM,
                                     (cl_context_properties)(*iter)(), 0 };
    context = cl::Context(CL_DEVICE_TYPE_GPU, cps);

    /////////////////////////////////////////////////////////////////
    // Detect OpenCL devices
    /////////////////////////////////////////////////////////////////
    devices = context.getInfo&lt;CL_CONTEXT_DEVICES&gt;();

    /////////////////////////////////////////////////////////////////
    // Create an OpenCL command queue
    /////////////////////////////////////////////////////////////////
    queue = cl::CommandQueue(context, devices[0]);

    /////////////////////////////////////////////////////////////////
    // Create OpenCL memory buffers
    /////////////////////////////////////////////////////////////////
    bufX = cl::Buffer(context,
                      CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR,
                      sizeof(cl_float) * length,
                      pX);
    bufY = cl::Buffer(context,
                      CL_MEM_READ_WRITE | CL_MEM_COPY_HOST_PTR,
                      sizeof(cl_float) * length,
                      pY);

    /////////////////////////////////////////////////////////////////
    // Load CL file, build CL program object, create CL kernel object
    /////////////////////////////////////////////////////////////////
    cl::Program::Sources sources(1, std::make_pair(kernelStr.c_str(),
                                 kernelStr.length()));
    program = cl::Program(context, sources);
    program.build(devices);
    kernel = cl::Kernel(program, &quot;saxpy&quot;);

    /////////////////////////////////////////////////////////////////
    // Set the arguments that will be used for kernel execution
    /////////////////////////////////////////////////////////////////
    kernel.setArg(0, bufX);
    kernel.setArg(1, bufY);
    kernel.setArg(2, a);

    /////////////////////////////////////////////////////////////////
    // Enqueue the kernel to the queue
    // with appropriate global and local work sizes
    /////////////////////////////////////////////////////////////////
    queue.enqueueNDRangeKernel(kernel, cl::NDRange(),
    cl::NDRange(length), cl::NDRange(64));

    /////////////////////////////////////////////////////////////////
    // Enqueue blocking call to read back buffer Y
    /////////////////////////////////////////////////////////////////
    queue.enqueueReadBuffer(bufY, CL_TRUE, 0, length *
    sizeof(cl_float), pY);

    printVector(&quot;Y&quot;, pY, length);

    /////////////////////////////////////////////////////////////////
    // Release host resources
    /////////////////////////////////////////////////////////////////
    cleanupHost();
  }
  catch (cl::Error err)
  {
    /////////////////////////////////////////////////////////////////
    // Catch OpenCL errors and print log if it is a build error
    /////////////////////////////////////////////////////////////////
    cerr &lt;&lt; &quot;ERROR: &quot; &lt;&lt; err.what() &lt;&lt; &quot;(&quot; &lt;&lt; err.err() &lt;&lt; &quot;)&quot; &lt;&lt; endl;
    if (err.err() == CL_BUILD_PROGRAM_FAILURE)
    {
      string str = program.getBuildInfo&lt;CL_PROGRAM_BUILD_LOG&gt;(devices[0]);
      cout &lt;&lt; &quot;Program Info: &quot; &lt;&lt; str &lt;&lt; endl;
    }
    cleanupHost();
  }
  catch(string msg)
  {
    cerr &lt;&lt; &quot;Exception caught in main(): &quot; &lt;&lt; msg &lt;&lt; endl;
    cleanupHost();
  }
}
</pre></div>
</div>
</div>
<div class="section" id="example-parallel-min-function">
<h3>Example: Parallel Min( ) Function<a class="headerlink" href="#example-parallel-min-function" title="Permalink to this headline">¶</a></h3>
<p>This medium-complexity sample shows how to implement an efficient parallel min() function.</p>
<p>The code is written so that it performs very well on either CPU or GPU. The number of threads launched depends on how many hardware processors are available. Each thread walks the source buffer, using a device-optimal access pattern selected at runtime. A multi-stage reduction using    local and    global atomics produces the single result value.</p>
<p>The sample includes a number of programming techniques useful for simple tests. Only minimal error checking and resource tear-down is used.</p>
<p>Runtime Code –</p>
<ol class="arabic">
<li><p class="first">The source memory buffer is allocated, and initialized with a random pattern.
Also, the actual min() value for this data set is serially computed, in order to later verify the parallel result.</p>
</li>
<li><p class="first">The compiler is instructed to dump the intermediate IL and ISA files for further analysis.</p>
</li>
<li><p class="first">The main section of the code, including device setup, CL data buffer creation, and code compilation, is executed for each device,    in this case for CPU and GPU. Since the source memory buffer exists on the host, it is shared. All other resources are device     specific.</p>
</li>
<li><p class="first">The global work size is computed for each device. A simple heuristic is used to ensure an optimal number of threads on each          device. For the CPU, a given CL implementation can translate one work-item per CL compute unit into one thread per CPU core.</p>
<p>On the GPU, an initial multiple of the wavefront size is used, which is adjusted to ensure even divisibility of the input data       over all threads. The value of 7 is a minimum value to keep all independent hardware units of the compute units busy, and to            provide a minimum amount of memory latency hiding for a kernel with little ALU activity.</p>
</li>
<li><p class="first">After the kernels are built, the code prints errors that occurred during kernel compilation and linking.</p>
</li>
<li><p class="first">The main loop is set up so that the measured timing reflects the actual kernel performance. If a sufficiently large NLOOPS is     chosen, effects from kernel launch time and delayed buffer copies to the device by the CL runtime are minimized. Note that while   only a single clFinish() is executed at the end of the timing run, the two kernels are always linked using an event to ensure           serial execution.</p>
<p>The bandwidth is expressed as “number of input bytes processed.” For high- end graphics cards, the bandwidth of this algorithm is    about an order of magnitude higher than that of the CPU, due to the parallelized memory subsystem of the graphics card.</p>
</li>
<li><p class="first">The results then are checked against the comparison value. This also establishes that the result is the same on both CPU and GPU,    which can serve as the first verification test for newly written kernel code.</p>
</li>
<li><p class="first">Note the use of the debug buffer to obtain some runtime variables. Debug buffers also can be used to create short execution traces for each thread, assuming the device has enough memory.</p>
</li>
<li><p class="first">You can use the Timer.cpp and Timer.h files from the TransferOverlap sample, which is in the SDK samples.</p>
</li>
</ol>
<p>Kernel Code –</p>
<ol class="arabic" start="10">
<li><p class="first">The code uses four-component vectors (uint4) so the compiler can identify concurrent execution paths as often as possible. On the GPU, this can be used to further optimize memory accesses and distribution across ALUs. On the CPU, it can be used to enable SSE  like execution.</p>
</li>
<li><p class="first">The kernel sets up a memory access pattern based on the device. For the CPU, the source buffer is chopped into continuous           buffers: one per thread. Each CPU thread serially walks through its buffer portion, which results in good cache and prefetch            behavior for each core.</p>
<p>On the GPU, each thread walks the source buffer using a stride of the total number of threads. As many threads are executed in      parallel, the result is a maximally coalesced memory pattern requested from the memory back-end. For example, if each compute           unit has 16 physical processors, 16 uint4 requests are produced in parallel, per clock, for a total of 256 bytes per clock.</p>
</li>
<li><p class="first">The kernel code uses a reduction consisting of three stages:     global to private,private to local, which is flushed to  global, and finally global to global. In the first loop, each thread walks     global memory, and reduces all values into a min value   in private memory (typically, a register). This is the bulk of the work, and is mainly bound by global memory bandwidth. The      subsequent reduction stages are brief in comparison.</p>
</li>
<li><p class="first">Next, all per-thread minimum values inside the work-group are reduced to a
local value, using an atomic operation. Access to the     local value is serialized; however, the number of these operations is     very small compared to the work of the previous reduction stage. The threads within a work-group are synchronized through a local barrier(). The reduced min value is stored in     global memory.</p>
</li>
<li><p class="first">After all work-groups are finished, a second kernel reduces all work-group values into a single value in     global memory, using an atomic operation. This is a minor contributor to the overall runtime.</p>
</li>
</ol>
<p><strong>Example Code 3</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>//
// Copyright (c) 2010 Advanced Micro Devices, Inc. All rights reserved.
//

#include &lt;CL/cl.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;time.h&gt;
#include &quot;Timer.h&quot;

#define NDEVS      1

// A parallel min() kernel that works well on CPU and GPU

const char *kernel_source =
&quot;                                                                                 \n&quot;
&quot;#pragma OPENCL EXTENSION cl_khr_local_int32_extended_atomics : enable            \n&quot;
&quot;#pragma OPENCL EXTENSION cl_khr_global_int32_extended_atomics : enable           \n&quot;
&quot;                                                                                 \n&quot;
&quot;// 9. The source buffer is accessed as 4-vectors.                                \n&quot;
&quot;__kernel void minp(__global uint4 *src,                                          \n&quot;
&quot;                   __global uint *gmin,                                          \n&quot;
&quot;                   __local uint *lmin,                                           \n&quot;
&quot;                   __global uint *dbg,                                           \n&quot;
&quot;                   int nitems,                                                   \n&quot;
&quot;                   uint dev )                                                    \n&quot;
&quot;{                                                                                \n&quot;
&quot;                                                                                 \n&quot;
&quot;  // 10. Set up   global memory access pattern.                                  \n&quot;
&quot;                                                                                 \n&quot;
&quot;  uint count = ( nitems / 4 ) / get_global_size(0);                              \n&quot;
&quot;  uint idx   = (dev == 0) ? get_global_id(0) * count                             \n&quot;
&quot;                          :  get_global_id(0);                                   \n&quot;
&quot;  uint stride = (dev == 0) ? 1 : get_global_size(0);                             \n&quot;
&quot;  uint pmin  = (uint) -1;                                                        \n&quot;
&quot;  // 11. First, compute private min, for this work-item.                         \n&quot;
&quot;  for( int n=0; n &lt; count; n++, idx += stride )                                  \n&quot;
&quot;  {                                                                              \n&quot;
&quot;    pmin = min( pmin, src[idx].x );                                              \n&quot;
&quot;    pmin = min( pmin, src[idx].y );                                              \n&quot;
&quot;    pmin = min( pmin, src[idx].z );                                              \n&quot;
&quot;    pmin = min( pmin, src[idx].w );                                              \n&quot;
&quot;  }                                                                              \n&quot;
&quot;                                                                                 \n&quot;
&quot;  // 12. Reduce min values inside work-group.                                    \n&quot;
&quot;  if( get_local_id(0) == 0 )                                                     \n&quot;
&quot;    lmin[0] = (uint) -1;                                                         \n&quot;
&quot;  barrier( CLK_LOCAL_MEM_FENCE );                                                \n&quot;
&quot;  (void) atom_min( lmin, pmin );                                                 \n&quot;
&quot;  barrier( CLK_LOCAL_MEM_FENCE );                                                \n&quot;
&quot;  // Write out to __global.                                                      \n&quot;
&quot;  if( get_local_id(0) == 0 )                                                     \n&quot;
&quot;    gmin[ get_group_id(0) ] = lmin[0];                                           \n&quot;
&quot;  // Dump some debug information.                                                \n&quot;
&quot;  if( get_global_id(0) == 0 )                                                    \n&quot;
&quot;  {                                                                              \n&quot;
&quot;    dbg[0] = get_num_groups(0);                                                  \n&quot;
&quot;    dbg[1] = get_global_size(0);                                                 \n&quot;
&quot;    dbg[2] = count;                                                              \n&quot;
&quot;    dbg[3] = stride;                                                             \n&quot;
&quot;  }                                                                              \n&quot;
&quot;}                                                                                \n&quot;
&quot;                                                                                 \n&quot;
&quot;// 13. Reduce work-group min values from __global to __global.                   \n&quot;
&quot;kernel void reduce(__global uint4 *src,                                          \n&quot;
&quot;                   __global uint *gmin )                                         \n&quot;
&quot;{                                                                                \n&quot;
&quot;  (void) atom_min( gmin, gmin[get_global_id(0)] );                               \n&quot;
&quot;};                                                                               \n&quot;;

int main(int argc, char ** argv)
{
  cl_platform_id      platform;

  int dev, nw;
  cl_device_type      devs[NDEVS] = { CL_DEVICE_TYPE_GPU };

  cl_uint     *src_ptr;
  unsigned int        num_src_items = 4096*4096;

  // 1. quick &amp; dirty MWC random init of source buffer.
  // Random seed (portable).
  time_t ltime;
  time(&amp;ltime);

  src_ptr = (cl_uint *) malloc( num_src_items * sizeof(cl_uint) );

  cl_uint a = (cl_uint) ltime, b =    (cl_uint) ltime;
  cl_uint min = (cl_uint) -1;
  // Do serial computation of min() for result verification.
  for( int i=0; i &lt; num_src_items; i++ )
  {
    src_ptr[i] = (cl_uint) (b = ( a * ( b &amp; 65535 )) + ( b &gt;&gt; 16 ));
    min = src_ptr[i] &lt; min ? src_ptr[i] : min;
  }

  // Get a platform.
  clGetPlatformIDs( 1, &amp;platform, NULL );

  // 3. Iterate over devices.
  for(dev=0; dev &lt; NDEVS; dev++)
  {
    cl_device_id      device;
    cl_context        context;
    cl_command_queue  queue;
    cl_program        program;
    cl_kernel         minp;
    cl_kernel         reduce;

    cl_mem            src_buf;
    cl_mem            dst_buf;
    cl_mem            dbg_buf;

    cl_uint           *dst_ptr,
                      *dbg_ptr;

    printf(&quot;\n%s: &quot;, dev == 0 ? &quot;CPU&quot; : &quot;GPU&quot;);
    // Find the device.
    clGetDeviceIDs( platform,
                    devs[dev],
                    1,
                    &amp;device,
                    NULL);

    // 4. Compute work sizes.
    cl_uint compute_units;
    size_t global_work_size;
    size_t local_work_size;
    size_t num_groups;

    clGetDeviceInfo( device,
                     CL_DEVICE_MAX_COMPUTE_UNITS,
                     sizeof(cl_uint),
                     &amp;compute_units,
                     NULL);

    if( devs[dev] == CL_DEVICE_TYPE_CPU )
    {
      global_work_size = compute_units * 1;   // 1 thread per core
      local_work_size = 1;
    }
    else
    {
      cl_uint ws = 64;
      global_work_size = compute_units * 7 * ws; // 7 wavefronts per SIMD
      while( (num_src_items / 4) % global_work_size != 0 )
        global_work_size += ws;
      local_work_size = ws;
    }
    num_groups = global_work_size / local_work_size;
    // Create a context and command queue on that device.
    context = clCreateContext( NULL,
                               1,
                               &amp;device,
                               NULL, NULL, NULL);

    queue = clCreateCommandQueue( context,
                                  device,
                                  0,
                                  NULL);
    // Minimal error check.
    if( queue == NULL )
    {
      printf(&quot;Compute device setup failed\n&quot;);
      return(-1);
    }

    // Perform runtime source compilation, and obtain kernel entry point.
    program = clCreateProgramWithSource( context,
                                         1,
                                         &amp;kernel_source,
                                         NULL, NULL );

    //Tell compiler to dump intermediate .il and .isa GPU files.
    cl_int ret = clBuildProgram( program,
                          1,
                          &amp;device,
                          &quot;-save-temps&quot;,
                          NULL, NULL );

    // 5. Print compiler error messages
    if(ret != CL_SUCCESS)
    {
      printf(&quot;clBuildProgram failed: %d\n&quot;, ret);

      char buf[0x10000];

      clGetProgramBuildInfo( program,
                             device,
                             CL_PROGRAM_BUILD_LOG,
                             0x10000,
                             buf,
                             NULL);
      printf(&quot;\n%s\n&quot;, buf);
      return(-1);
    }

    minp      = clCreateKernel( program, &quot;minp&quot;, NULL );
    reduce = clCreateKernel( program, &quot;reduce&quot;, NULL );
    // Create input, output and debug buffers.
    src_buf = clCreateBuffer( context,
                              CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR,
                              num_src_items * sizeof(cl_uint),
                              src_ptr,
                              NULL );

    dst_buf = clCreateBuffer( context,
                              CL_MEM_READ_WRITE,
                              num_groups * sizeof(cl_uint),
                              NULL, NULL );

    dbg_buf = clCreateBuffer( context,
                              CL_MEM_WRITE_ONLY,
                              global_work_size * sizeof(cl_uint),
                              NULL, NULL );

    clSetKernelArg(minp, 0, sizeof(void *),        (void*) &amp;src_buf);
    clSetKernelArg(minp, 1, sizeof(void *),        (void*) &amp;dst_buf);
    clSetKernelArg(minp, 2, 1*sizeof(cl_uint),     (void*) NULL);
    clSetKernelArg(minp, 3, sizeof(void *),        (void*) &amp;dbg_buf);
    clSetKernelArg(minp, 4, sizeof(num_src_items), (void*) &amp;num_src_items);
    clSetKernelArg(minp, 5, sizeof(dev),           (void*) &amp;dev);

    clSetKernelArg(reduce, 0, sizeof(void *),      (void*) &amp;src_buf);
    clSetKernelArg(reduce, 1, sizeof(void *),      (void*) &amp;dst_buf);

    CPerfCounter t;
    t.Reset();
    t.Start();

    // 6. Main timing loop.
    #define NLOOPS 500

    cl_event ev;
    int nloops = NLOOPS;

    while(nloops--)
    {
      clEnqueueNDRangeKernel( queue,
                              minp,
                              1,
                              NULL,
                              &amp;global_work_size,
                              &amp;local_work_size,
                              0,
                              NULL,
                              &amp;ev);

      clEnqueueNDRangeKernel( queue,
                              reduce,
                              1,
                              NULL,
                              &amp;num_groups,
                              NULL,
                              1,
                              &amp;ev,
                              NULL);
    }

    clFinish( queue );
    t.Stop();

    printf(&quot;B/W %.2f GB/sec, &quot;, ((float) num_src_items * sizeof(cl_uint) * NLOOPS) / t.GetElapsedTime() / 1e9 );

    // 7. Look at the results via synchronous buffer map.
    dst_ptr = (cl_uint *) clEnqueueMapBuffer( queue,
                                              dst_buf,
                                              CL_TRUE,
                                              CL_MAP_READ,
                                              0,
                                              num_groups * sizeof(cl_uint),
                                              0,
                                              NULL, NULL, NULL );

    dbg_ptr = (cl_uint *) clEnqueueMapBuffer( queue,
                                              dbg_buf,
                                              CL_TRUE,
                                              CL_MAP_READ,
                                              0,
                                              global_work_size * sizeof(cl_uint),
                                              0,
                                              NULL, NULL, NULL );

    // 8. Print some debug info.
    printf(&quot;%d groups, %d threads, count %d, stride %d\n&quot;, dbg_ptr[0], dbg_ptr[1], dbg_ptr[2], dbg_ptr[3] );

    if( dst_ptr[0] == min )
      printf(&quot;result correct\n&quot;);
    else
      printf(&quot;result INcorrect\n&quot;);
  }

  printf(&quot;\n&quot;);
  return 0;
}
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="amd-implementation">
<span id="id6"></span><h1>AMD Implementation<a class="headerlink" href="#amd-implementation" title="Permalink to this headline">¶</a></h1>
<div class="section" id="the-amd-rocm-implementation-of-opencl">
<span id="amd-rocm-implementation"></span><h2>The AMD ROCm Implementation of OpenCL<a class="headerlink" href="#the-amd-rocm-implementation-of-opencl" title="Permalink to this headline">¶</a></h2>
<p>ROCm OpenCL runtime harnesses the tremendous processing power of GPUs for high- performance, data-parallel computing in a wide range of applications. The AMD system includes a software stack, AMD GPUs, and AMD multicore CPUs.</p>
<p>Figure 2.1 illustrates the relationship of the ROCm OpenCL components.</p>
<img alt="../_images/2.11.png" class="align-center" src="../_images/2.11.png" />
<p>The AMD ROCm software stack provides end-users and developers with a complete, flexible suite of tools to leverage the processing power in AMD GPUs. AMD ROCm software embraces open-systems, open-platform standards. The AMD Accelerated Parallel Processing Technology open platform strategy enables AMD technology partners to develop and provide third-party development tools.</p>
<p>The software includes the following components:</p>
<blockquote>
<div><ul class="simple">
<li>OpenCL compiler and runtime</li>
<li>Debugging and Performance Profiling Tools – AMD CodeXL.</li>
<li>Performance Libraries – clMath and other OpenCL accelerated libraries for optimized NDRange-specific algorithms.</li>
</ul>
</div></blockquote>
<p>The latest generations of AMD GPUs use unified shader architectures capable of running different kernel types interleaved on the same hardware.Programmable GPU compute devices execute various user-developed programs,known to graphics programmers as shaders and to compute programmers as kernels. These GPU compute devices can execute non-graphics functions using a data-parallel programming model that maps executions onto compute units. Each compute unit contains one (pre-GCN devices) or more (GCN devices) vector (SIMD) units. In this programming model, known as AMD Accelerated Parallel Processing Technology, arrays of input data elements stored in memory are accessed by a number of compute units.</p>
<p>Each instance of a kernel running on a compute unit is called a work-item. Work- items are mapped to an n-dimensional index space, called an NDRange.</p>
<p>The GPU schedules the range of work-items onto a group of processing elements, until all work-items have been processed. Subsequent kernels then can be executed, until the application completes. A simplified view of the AMD Accelerated Parallel Processing Technology programming model and the mapping of work-items to processing elements is shown in Figure 2.2.</p>
<img alt="../_images/2.21.png" class="align-center" src="../_images/2.21.png" />
<p>Work-groups are assigned to CUs. All work-items of a work-group can be processed only by the processing elements of a single CU. A processing element can process only one work-item at a time; however, a CU can process multiple work-groups.</p>
<p>Note that in OpenCL 2.0, the work-groups are not required to divide evenly into the NDRange.</p>
<p>OpenCL maps the total number of work-items to be launched onto an n- dimensional grid (ND-Range). The developer can specify how to divide these items into work-groups. AMD GPUs execute on wavefronts (groups of work-items executed in lock-step in a compute unit); there is an integer number of wavefronts in each work-group. Thus, as shown in Figure 2.3, hardware that schedules work-items for execution in the AMD Accelerated Parallel Processing Technology environment includes the intermediate step of specifying wavefronts within a work-group. This permits achieving maximum performance on AMD GPUs. For a more detailed discussion of wavefronts.</p>
<img alt="../_images/2.31.png" class="align-center" src="../_images/2.31.png" />
<div class="section" id="work-item-processing">
<h3>Work-Item Processing<a class="headerlink" href="#work-item-processing" title="Permalink to this headline">¶</a></h3>
<p>All processing elements within a vector unit execute the same instruction in each cycle. For a typical instruction, 16 processing elements execute one instruction for 64 work items over 4 cycles. The block of work-items that are executed together is called a wavefront. For example, on the AMD Radeon™ HD 290X</p>
<p>compute device, the 16 processing elements within each vector unit execute the same instruction for four cycles, which effectively appears as a 64-wide compute unit in execution width.</p>
<p>The size of wavefronts can differ on different GPU compute devices. For example, some of the low-end and older GPUs, such as the AMD Radeon™ HD 54XX series graphics cards, have a wavefront size of 32 work-items. Higher-end and newer AMD GPUs have a wavefront size of 64 work-items.</p>
<p>Compute units operate independently of each other, so it is possible for different compute units to execute different instructions. It is also possible for different vector units within a compute unit to execute different instructions.</p>
<p>Before discussing flow control, it is necessary to clarify the relationship of a wavefront to a work-group. If a user defines a work-group, it consists of one or more wavefronts. A wavefront is a hardware thread with its own program counter; it is capable of following control flow independently of other wavefronts. A wavefront consists of 64 or fewer work-items. The mapping is based on a linear work-item order. On a device with a wavefront size of 64, work-items 0-63 map to wavefront 0, work items 64-127 map to wavefront 1, etc. For optimum hardware usage, an integer multiple of 64 work-items is recommended.</p>
</div>
<div class="section" id="work-item-creation">
<h3>work-Item Creation<a class="headerlink" href="#work-item-creation" title="Permalink to this headline">¶</a></h3>
<p>For each work-group, the GPU compute device spawns the required number of wavefronts on a single compute unit. If there are non-active work-items within a wavefront, the processing elements that would have been mapped to those work- items are idle. An example is a work-group that is a non-multiple of a wavefront size.</p>
</div>
<div class="section" id="flow-control">
<h3>Flow Control<a class="headerlink" href="#flow-control" title="Permalink to this headline">¶</a></h3>
<p>Flow control, such as branching, is achieved by combining all necessary paths as a wavefront. If work-items within a wavefront diverge, all paths are executed serially. For example, if a work-item contains a branch with two paths, the wavefront first executes one path, then the second path. The total time to execute the branch is the sum of each path time. An important point is that even if only one work-item in a wavefront diverges, the rest of the work-items in the wavefront execute the branch. The number of work-items that must be executed during a branch is called the branch granularity. On AMD hardware, the branch granularity is the same as the number of work-items in a wavefront.</p>
<p>Masking of wavefronts is effected by constructs such as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="p">{</span>
<span class="o">.</span>      <span class="o">//</span><span class="n">items</span> <span class="n">within</span> <span class="n">these</span> <span class="n">braces</span> <span class="o">=</span> <span class="n">A</span>
<span class="o">.</span>
<span class="o">.</span>
<span class="p">}</span>
<span class="k">else</span>
<span class="p">{</span>
<span class="o">.</span>      <span class="o">//</span><span class="n">items</span> <span class="n">within</span> <span class="n">these</span> <span class="n">braces</span> <span class="o">=</span> <span class="n">B</span>
<span class="o">.</span>


<span class="o">.</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The wavefront mask is set true for lanes (elements/items) in which x is true, then execute A. The mask then is inverted, and B is executed.</p>
<p><strong>Example 1:</strong> If two branches, A and B, take the same amount of time t to execute over a wavefront, the total time of execution, if any work-item diverges, is 2t.</p>
<p>Loops execute in a similar fashion, where the wavefront occupies a compute unit as long as there is at least one work-item in the wavefront still being processed. Thus, the total execution time for the wavefront is determined by the work-item with the longest execution time.</p>
<p><strong>Example 2:</strong> If t is the time it takes to execute a single iteration of a loop; and within a wavefront all work-items execute the loop one time, except for a single work-item that executes the loop 100 times, the time it takes to execute that entire wavefront is 100t.</p>
</div>
</div>
<div class="section" id="hardware-overview-for-gcn-devices">
<span id="hardware-overview-gcndevices"></span><h2>Hardware Overview for GCN Devices<a class="headerlink" href="#hardware-overview-for-gcn-devices" title="Permalink to this headline">¶</a></h2>
<p>A general OpenCL device comprises compute units (CUs), each of which has sub-modules that ultimately have ALUs. A work-item (or SPMD kernel instance)
executes on an ALU, as shown in Figure 2.4).</p>
<img alt="../_images/2.41.png" class="align-center" src="../_images/2.41.png" />
<p>In GCN devices, each CU includes one Scalar Unit and four Vector (SIMD) units, each of which contains an array of 16 processing elements (PEs). Each PE contains one ALU. Each SIMD unit simultaneously executes a single operation across 16 work items, but each can be working on a separate wavefront.</p>
<p>For example, for the AMD Radeon™ HD 79XX devices each of the 32 CUs has one Scalar Unit and four Vector Units. Figure 2.5 shows only two compute engines/command processors of the array that comprises the compute device of
the AMD Radeon™ HD 79XX family.</p>
<img alt="../_images/2.51.png" class="align-center" src="../_images/2.51.png" />
<p>In Figure 2.5, there are two command processors, which can process two command queues concurrently. The Scalar Unit, Vector Unit, Level 1 data cache (L1), and Local Data Share (LDS) are the components of one compute unit, of which there are 32. The scalar (SC) cache is the scalar unit data cache, and the Level 2 cache consists of instructions and data.</p>
<p>On GCN devices, the instruction stream contains both scalar and vector instructions. On each cycle, it selects a scalar instruction and a vector instruction (as well as a memory operation and a branch operation, if available); it issues one to the scalar unit, the other to the vector unit; this takes four cycles to issue over the four vector cores (the same four cycles over which the 16 units execute 64 work-items).</p>
<p>The Asynchronous Compute Engines (ACEs) manage the CUs; a graphics command processor handles graphics shaders and fixed-function hardware.</p>
<div class="section" id="key-differences-between-pre-gcn-and-gcn-devices">
<h3>Key differences between pre-GCN and GCN devices<a class="headerlink" href="#key-differences-between-pre-gcn-and-gcn-devices" title="Permalink to this headline">¶</a></h3>
<p>In pre-GCN devices (for a hardware overview, see Appendix D, “Hardware overview of pre-GCN devices.”), each compute unit consists of a single vector unit, each containing up to 16 processing elements. Each processing element, which contains 4 or 5 ALUs, could execute bundles of 4 or 5 independent instructions co-issued in a VLIW (Very Long Instruction Word) format. All the processing elements within a vector unit execute a single wavefront (a group of
64 work items). If operations within a wavefront contain dependencies, they cannot be scheduled in the same clock cycle, leaving some ALUs un-utilized. In such cases, some processing elements (and hence, vector units) remain under- utilized.</p>
<p>In GCN devices, the CUs are arranged in four vector unit arrays consisting of 16 processing elements each. Each of these arrays executes a single instruction across each lane for each block of 16 work-items. That instruction is repeated over four cycles to make the 64-element vector called a wavefront.</p>
<p>Thus, in GCN devices, the four vector units within a CU can operate on four different wavefronts. If operations within a wavefront include dependencies, independent operations from different wavefronts can be selected to be assigned to a single vector unit to be executed in parallel every cycle.</p>
<p>GCN-based GPUs have 32KB of dedicated L1 instruction cache. A single instruction cache instance serves up to 4 CUs (depending upon the architecture family and device), with each CU holding up to 40 wavefronts. As each wavefront includes its own program counter, a single instruction cache unit may serve up to 160 wavefronts with each executing a different instruction in the program.</p>
<p><strong>Note:</strong> If the program is larger than 32KB, the L1-L2 cache trashing can inhibit performance. The size of the ISA can be determined by using the CodeXL analysis mode, under the Statistics tab. For information about how to use CodeXL, see Chapter 4.</p>
</div>
<div class="section" id="key-differences-between-southern-islands-sea-islands-and-volcanic-islands-families">
<h3>Key differences between Southern Islands, Sea Islands, and Volcanic Islands families<a class="headerlink" href="#key-differences-between-southern-islands-sea-islands-and-volcanic-islands-families" title="Permalink to this headline">¶</a></h3>
<p>The number of Asynchronous Compute Engines (ACEs) and CUs in an AMD GCN family GPU, and the way they are structured, vary with the GCN device family, as well as with the device designations within the family.</p>
<p>The ACEs are responsible for managing the CUs and for scheduling and resource allocation of the compute tasks (but not of the graphics shader tasks). The ACEs operate independently; the greater the number of ACEs, the greater is the performance. Each ACE fetches commands from cache or memory, and</p>
<p>creates task queues to be scheduled for execution on the CUs depending on their priority and on the availability of resources.</p>
<p>Each ACE contains up to eight hardware queues and, together with the graphics command processor, allows up to nine independent vector instructions to be executed per clock cycle. Some of these queues are not available for use by OpenCL.</p>
<p>Devices in the Southern Islands families typically have two ACEs. The ACE engines on the Southern Islands families are single-threaded, which means that they contain two hardware queues.</p>
<p>Devices in the Sea Islands and Volcanic Islands families contain between four and eight ACEs, and are multi-threaded (thereby supporting more hardware queues) so they offer more performance. For example, the AMD Radeon™ R9
290X devices, in the VI family contain 8 ACEs and 44 CUs.</p>
</div>
<div class="section" id="a-note-on-hardware-queues">
<h3>A note on hardware queues<a class="headerlink" href="#a-note-on-hardware-queues" title="Permalink to this headline">¶</a></h3>
<p>A hardware queue can be thought of as a GPU entry point. The GPU can process kernels from several compute queues concurrently. All hardware queues ultimately share the same compute cores. The use of multiple hardware queues is beneficial when launching small kernels that do not fully saturate the GPU. For example, the AMD Radeon™ HD 290X compute device can execute up to
112,640 threads concurrently. The GPU can execute two kernels each spawning
56320 threads (assuming fully occupancy) twice as fast if launched concurrently through two hardware queues than serially through a single hardware queue.</p>
<p>An OpenCL queue is assigned to a hardware queue on creation time. The hardware queue is selected according to the creation order of the OpenCL queue within an OpenCL context. If the GPU supports K hardware queues, the Nth created OpenCL queue will be assigned to the (N mod K) hardware queue. The number of compute queues can be limited by specifying the GPU_NUM_COMPUTE_RINGS environment variable.</p>
</div>
</div>
<div class="section" id="communication-between-host-and-the-gpu-compute-device">
<span id="communication-host-gpu"></span><h2>Communication Between Host and the GPU Compute Device<a class="headerlink" href="#communication-between-host-and-the-gpu-compute-device" title="Permalink to this headline">¶</a></h2>
<p>The following subsections discuss the communication between the host (CPU) and the GPU in a compute device. This includes an overview of the PCIe bus, processing API calls, and DMA transfers.</p>
<p>Communication and data transfers between the system and the GPU compute device occur on the PCIe channel. AMD graphics cards use PCIe 2.0 x16 (second generation, 16 lanes).  Generation 1 x16 has a theoretical maximum
throughput of 4 GBps in each direction. Generation 2 x16 doubles the throughput to 8 GBps in each direction. Southern Islands AMD GPUs support PCIe 3.0 with a theoretical peak performance of 16 GBps. Actual transfer performance is CPU and chipset dependent.</p>
<p>Transfers from the system to the GPU compute device are done either by the
command processor or by the DMA engine. The GPU compute device also can</p>
<p>read and write system memory directly from the compute unit through kernel instructions over the PCIe bus.</p>
<div class="section" id="processing-api-calls-the-command-processor">
<h3>Processing API Calls: The Command Processor<a class="headerlink" href="#processing-api-calls-the-command-processor" title="Permalink to this headline">¶</a></h3>
<p>The host application does not interact with the GPU compute device directly. A driver layer translates and issues commands to the hardware on behalf of the application.</p>
<p>Most commands to the GPU compute device are buffered in a command queue on the host side. The queue of commands is sent to, and processed by, the GPU compute device. There is no guarantee as to when commands from the command queue are executed, only that they are executed in order.</p>
<p>Command queue elements include:</p>
<blockquote>
<div><ul class="simple">
<li>Kernel execution calls</li>
<li>Kernels</li>
<li>Constants</li>
<li>Transfers between device and host</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="dma-transfers">
<h3>DMA Transfers<a class="headerlink" href="#dma-transfers" title="Permalink to this headline">¶</a></h3>
<p>Certain memory transfer calls use the DMA engine. To properly leverage the DMA engine, make the associated OpenCL data transfer calls. See the AMD OpenCL Optimization Reference Guide for more information.</p>
<p>Direct Memory Access (DMA) memory transfers can be executed separately from the command queue using the DMA engine on the GPU compute device. DMA calls are executed immediately; and the order of DMA calls and command queue flushes is guaranteed.</p>
<p>DMA transfers can occur asynchronously. This means that a DMA transfer is executed concurrently with other system or GPU compute operations when there are no dependencies. However, data is not guaranteed to be ready until the DMA engine signals that the event or transfer is completed. The application can use OpenCL to query the hardware for DMA event completion. If used carefully, DMA transfers are another source of parallelization.</p>
<p>All GCN devices have two DMA engines that can perform bidirectional transfers over the PCIe bus with multiple queues created in consecutive order, since each DMA engine is assigned to an odd or an even queue correspondingly.</p>
</div>
<div class="section" id="masking-visible-devices">
<h3>Masking Visible Devices<a class="headerlink" href="#masking-visible-devices" title="Permalink to this headline">¶</a></h3>
<p>By default, OpenCL applications are exposed to all GPUs installed in the system;
this allows applications to use multiple GPUs to run the compute task.</p>
<p>In some cases, the user might want to mask the visibility of the GPUs seen by the OpenCL application. One example is to dedicate one GPU for regular</p>
<p>graphics operations and the other three (in a four-GPU system) for Compute. To do that, set the GPU_DEVICE_ORDINAL environment parameter, which is a comma- separated list variable:</p>
<blockquote>
<div><ul class="simple">
<li>Under Windows: set GPU_DEVICE_ORDINAL=1,2,3</li>
<li>Under Linux: export GPU_DEVICE_ORDINAL=1,2,3</li>
</ul>
</div></blockquote>
<p>Another example is a system with eight GPUs, where two distinct OpenCL applications are running at the same time. The administrator might want to set GPU_DEVICE_ORDINAL to 0,1,2,3 for the first application, and 4,5,6,7 for the second application; thus, partitioning the available GPUs so that both applications can run at the same time.</p>
</div>
</div>
<div class="section" id="wavefront-scheduling">
<span id="id7"></span><h2>Wavefront Scheduling<a class="headerlink" href="#wavefront-scheduling" title="Permalink to this headline">¶</a></h2>
<p>GPU compute devices are very efficient at parallelizing large numbers of work- items in a manner transparent to the application. Each GPU compute device uses the large number of wavefronts to hide memory access latencies by having the resource scheduler switch the active wavefront in a given compute unit whenever the current wavefront is waiting for a memory access to complete. Hiding memory access latencies requires that each work-item contain a large number of ALU operations per memory load/store.</p>
<p>Figure 2.6 shows the timing of a simplified execution of wavefronts in a single compute unit. At time 0, the wavefronts are queued and waiting for execution. In this example, only four wavefronts (T0…T3) are scheduled for the compute unit. The hardware limit for the number of active wavefront is dependent on the resource usage (such as the number of active registers used) of the program being executed. An optimally programmed GPU compute device typically has many of active wavefronts.</p>
<img alt="../_images/2.6.png" class="align-center" src="../_images/2.6.png" />
<p>At runtime, wavefront T0 executes until cycle 20; at this time, a stall occurs due to a memory fetch request. The scheduler then begins execution of the next wavefront, T1. Wavefront T1 executes until it stalls or completes. New wavefronts execute, and the process continues until the available number of active wavefronts is reached. The scheduler then returns to the first wavefront, T0.</p>
<p>If the data wavefront T0 is waiting for has returned from memory, T0 continues execution. In the example in Figure 2.6, the data is ready, so T0 continues. Since there were enough wavefronts and processing element operations to cover the long memory latencies, the compute unit does not idle. This method of memory latency hiding helps the GPU compute device achieve maximum performance.</p>
<p>If none of T0 – T3 are runnable, the compute unit waits (stalls) until one of T0 – T3 is ready to execute. In the example shown in Figure 2.7, T0 is the first to continue execution.</p>
<img alt="../_images/2.7.png" class="align-center" src="../_images/2.7.png" />
</div>
</div>
<div class="section" id="building-and-running-opencl-programs">
<span id="build-run-opencl"></span><h1>Building and Running OpenCL Programs<a class="headerlink" href="#building-and-running-opencl-programs" title="Permalink to this headline">¶</a></h1>
<p>An OpenCL application consists of a host program (C/C++) and an optional kernel program (.cl). To compile an OpenCL application, the host program must be compiled; this can be done using an off-the-shelf compiler such as g++ or MSVC++. The application kernels are compiled into device-specific binaries using the OpenCL compiler.</p>
<div class="section" id="compiling-the-host-program">
<span id="compilin-host-program"></span><h2>Compiling the Host Program<a class="headerlink" href="#compiling-the-host-program" title="Permalink to this headline">¶</a></h2>
<p>In order to compile the host program, users must install the OpenCL Compiler and language runtime on the ROCm, On Ubuntu is rocm-opencl-dev which provides all the necessary OpenCL runtime headers and libraries required by the host compiler. If wish to support application build with the historical  APPS SDK sets an environmental variable named AMDAPPSDKROOT to the path of the directory in which the ROCm OpenCL is installed. It should be /opt/rocm/opencl.  The runtime headers and libraries are placed in the install directory under the “include” and “lib” sub-folders, respectively.</p>
<p>While building the host program, these headers and libraries must be included in the project by choosing the appropriate options for the targeted operating system, IDE, and compiler.</p>
<div class="section" id="compiling-on-linux">
<h3>Compiling on Linux<a class="headerlink" href="#compiling-on-linux" title="Permalink to this headline">¶</a></h3>
<p>To compile OpenCL applications on Linux, gcc or the Intel C compiler must be installed. There are two major steps: compiling and linking.</p>
<ol class="arabic simple">
<li>Compile all the C++ files (Template.cpp), and get the object files.</li>
</ol>
<blockquote>
<div><p>64-bit object files on 64-bit system:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>g++ -o Template.o -c Template.cpp -I$ROCMOPENCL/include
</pre></div>
</div>
</div></blockquote>
<ol class="arabic" start="2">
<li><p class="first">Link all the object files generated in the previous step to the OpenCL library and create an executable.</p>
<p>For linking to a 64-bit library:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>g++ -o Template Template.o -lOpenCL -L$ROCMOPENCL/lib/x86_64
</pre></div>
</div>
</li>
</ol>
</div>
</div>
<div class="section" id="compiling-the-device-programs">
<span id="compiling-device-programs"></span><h2>Compiling the device programs<a class="headerlink" href="#compiling-the-device-programs" title="Permalink to this headline">¶</a></h2>
<p>OpenCL device programs that will be executed in parallel by each work-item are expressed in terms of kernel functions. The device programs may also include other helper functions (which cannot be invoked by the host) in addition to the kernels.</p>
<p>The device programs are written in the OpenCL C language. The device programs must be built for each target device before they can be executed on the OpenCL device. As a result, the same source program may have multiple device-specific binaries. To manage this conveniently, the OpenCL runtime provides a container-like object, called a program object, that contains the source code as well as the device-specific binaries of all the kernels and helper functions that are defined in a program scope. Compiling the application kernels requires first creating program objects.</p>
<div class="section" id="creating-opencl-program-objects">
<h3>Creating OpenCL program objects<a class="headerlink" href="#creating-opencl-program-objects" title="Permalink to this headline">¶</a></h3>
<p>In general, OpenCL program objects are created in two ways:</p>
<blockquote>
<div><ul class="simple">
<li>From the OpenCL C source</li>
<li>From a pre-built binary (either device-specific or device-agnostic)</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="creating-program-objects-from-the-opencl-c-source">
<h3>Creating program objects from the OpenCL C source<a class="headerlink" href="#creating-program-objects-from-the-opencl-c-source" title="Permalink to this headline">¶</a></h3>
<p>In this method, the OpenCL C source is passed to the
clCreateProgramWithSource runtime API (for more details, see the OpenCL</p>
<p>specification) as a text buffer to create the program object. If the source code is in an external file, then it must be read and placed in a text buffer before passing the buffer to the API.</p>
<p>Note: Most of the examples in this chapter are shown using runtime C APIs. In order to use the C++ wrapper APIs, one must map (a trivial step) the C APIs to corresponding C++ wrapper APIs. For cleanness, error checking is not shown.</p>
<p><strong>Example creation of program objects from an inline text string :</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">const</span> <span class="n">char</span> <span class="o">*</span><span class="n">source</span> <span class="o">=</span>
<span class="s2">&quot; kernel void myKernel(  global uint *src,  global uint *dst)</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="s2">&quot;{ </span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="s2">&quot; uint gid = get_global_id(0); </span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="s2">&quot; dst[gid] = src[gid] * 10; </span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="s2">&quot;} </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">;</span>

<span class="n">cl_program</span> <span class="n">program</span> <span class="o">=</span> <span class="n">clCreateProgramWithSource</span><span class="p">(</span> <span class="n">context</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">source</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="n">NULL</span> <span class="p">);</span>
</pre></div>
</div>
<p><strong>Example creation of program objects from an external file :</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="p">::</span><span class="n">ifstream</span> <span class="n">f</span><span class="p">(</span><span class="s2">&quot;my_kernel.cl&quot;</span><span class="p">);</span>
<span class="n">std</span><span class="p">::</span><span class="n">stringstream</span> <span class="n">st</span><span class="p">;</span>
<span class="n">st</span> <span class="o">&lt;&lt;</span> <span class="n">f</span><span class="o">.</span><span class="n">rdbuf</span><span class="p">();</span>
<span class="n">std</span><span class="p">::</span><span class="n">string</span> <span class="n">ss</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">str</span><span class="p">();</span>
<span class="n">const</span> <span class="n">char</span><span class="o">*</span> <span class="n">source</span> <span class="o">=</span> <span class="n">ss</span><span class="o">.</span><span class="n">c_str</span><span class="p">();</span>
<span class="n">const</span> <span class="n">size_t</span> <span class="n">length</span> <span class="o">=</span> <span class="n">ss</span><span class="o">.</span><span class="n">length</span><span class="p">();</span>

<span class="n">cl_program</span> <span class="n">program</span> <span class="o">=</span> <span class="n">clCreateProgramWithSource</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">source</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">length</span><span class="p">,</span> <span class="n">NULL</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="creating-program-objects-from-a-pre-built-binary">
<h3>Creating program objects from a pre-built binary<a class="headerlink" href="#creating-program-objects-from-a-pre-built-binary" title="Permalink to this headline">¶</a></h3>
<p>OpenCL allows the creation of program object from binaries previously built for one or more specific device(s) or from intermediate device-agnostic binaries (using, for example, the Standard Portable Intermediate Representation (SPIR) format). Such binaries serve two useful purposes:</p>
<blockquote>
<div><ul class="simple">
<li>Software vendors can protect their IP by supplying the OpenCL library as a collection of pre-built binary programs instead of as  raw source code.</li>
<li>The consumer of the OpenCL library can create new program objects using those binaries for use with their own applications.</li>
</ul>
</div></blockquote>
<p>In this method, the OpenCL binary is passed to the binaries argument of the clCreateProgramWithBinary runtime API (for more details, see the OpenCL specification). If the binary program code is in a file, the binary must be loaded from the file, the content of the file must be placed in a character buffer, and the resulting buffer must be passed to the clCreateProgramWithBinary API.</p>
</div>
<div class="section" id="building-the-program-executable-from-the-program-objects">
<h3>Building the program executable from the program objects<a class="headerlink" href="#building-the-program-executable-from-the-program-objects" title="Permalink to this headline">¶</a></h3>
<p>After the program object is created (from either sources or binaries), the program must be built for the targeted devices and the device executables must be generated. The executables are generated mainly in two ways:</p>
<blockquote>
<div><ul class="simple">
<li>Building (compile and link) the program in a single step (using clBuildProgram)</li>
<li>Compiling and linking the program separately (using clCompileProgram and clLinkProgram)</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="building-the-program-in-a-single-step">
<h3>Building the program in a single step<a class="headerlink" href="#building-the-program-in-a-single-step" title="Permalink to this headline">¶</a></h3>
<p>The most common way of building program objects, this method uses a single API, clBuildProgram, for both compiling and linking the program. For additional details about this API, see the OpenCL specification.</p>
<p><strong>Example(s):</strong></p>
<dl class="docutils">
<dt>Suppose a program object has been created as follows:</dt>
<dd><div class="first last line-block">
<div class="line">cl_program program = clCreateProgramWithSource(context, 1, &amp;source,&amp;length, NULL);</div>
</div>
</dd>
</dl>
<p>Next, the program object can be built for all the devices in the context or for a list of selected devices.</p>
<ul class="simple">
<li>To build the program for all the devices, “NULL” must be passed against the target device list argument, as shown below:</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">clBuildProgram</span><span class="p">(</span><span class="n">program</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="n">NULL</span><span class="p">);</span>
</pre></div>
</div>
<ul class="simple">
<li>To build for any particular GPU device or a list of devices :</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">int</span> <span class="n">nDevices</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="n">clGetDeviceIDs</span><span class="p">(</span><span class="n">platform</span><span class="p">,</span> <span class="n">CL_DEVICE_TYPE_GPU</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">nDevices</span><span class="p">);</span>
<span class="n">cl_device_id</span> <span class="o">*</span> <span class="n">devices</span> <span class="o">=</span> <span class="n">malloc</span><span class="p">(</span><span class="n">nDevices</span> <span class="o">*</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">cl_device_id</span><span class="p">));</span>
<span class="n">clGetDeviceIDs</span><span class="p">(</span><span class="n">platform</span><span class="p">,</span> <span class="n">CL_DEVICE_TYPE_GPU</span><span class="p">,</span> <span class="n">nDevices</span> <span class="o">*</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">cl_device_id</span><span class="p">),</span> <span class="n">devices</span><span class="p">,</span> <span class="n">NULL</span><span class="p">);</span>
</pre></div>
</div>
<ul class="simple">
<li>To build for the nth GPU device in a list of devices:</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">clBuildProgram</span><span class="p">(</span><span class="n">program</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">devices</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">NULL</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="n">NULL</span><span class="p">);</span>
</pre></div>
</div>
<ul class="simple">
<li>To build for the first n number of GPU devices</li>
</ul>
<dl class="docutils">
<dt>::</dt>
<dd>clBuildProgram(program, n, devices, NULL, NULL, NULL);</dd>
</dl>
<p><strong>Build Options:</strong></p>
<p>A list of options can be passed during program build to control each stage of the building process. The full list includes various categories of options, such as preprocessor, compiler, optimization, linker, and debugger. Some of them are standard (specified by Khronos); others are vendor-specific. For details about the standard options, see the clBuildProgram API’s description in the OpenCL specification.</p>
<p>For information about the frequently used standard build options, see  “Supported Standard OpenCL Compiler Options”.</p>
<p>For information about AMD-developed supplemental options and environment variables, see  “AMD-Developed Supplemental Compiler Options”.</p>
<p><strong>Special note for building OpenCL 2.0 programs:</strong></p>
<p>In order to build the program with OpenCL 2.0 support, the <code class="docutils literal notranslate"><span class="pre">-cl-std=CL2.0</span></code> option must be specified; otherwise, the highest OpenCL C 1.x language version supported by each device is used when compiling the program for each device.</p>
<p>OpenCL 2.0 is backwards-compatible with OpenCL 1.X. Applications written on OpenCL 1.x should run on OpenCL 2.0 without requiring any changes to the application.</p>
<p><strong>Special note for debugging:</strong></p>
<p>OpenCL provides a way to check and query the compilation/linking errors that occur during program build. Various build parameters for each device in the program object can be queried by using the clGetProgramBuildInfo API. Retrieving the build, compile or link log by using the <code class="docutils literal notranslate"><span class="pre">CL_PROGRAM_BUILD_LOG</span></code> input parameter is a useful and frequently-used technique. For details, see the OpenCL specification.</p>
<p><strong>Example:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cl_int</span> <span class="n">err</span> <span class="o">=</span> <span class="n">clBuildProgram</span><span class="p">(</span><span class="n">program</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">device</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="n">NULL</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">err</span> <span class="o">!=</span> <span class="n">CL_SUCCESS</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;clBuildProgram failed: </span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">err</span><span class="p">);</span>
  <span class="n">char</span> <span class="n">log</span><span class="p">[</span><span class="mh">0x10000</span><span class="p">];</span>
  <span class="n">clGetProgramBuildInfo</span><span class="p">(</span> <span class="n">program</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">CL_PROGRAM_BUILD_LOG</span><span class="p">,</span> <span class="mh">0x10000</span><span class="p">,</span> <span class="n">log</span><span class="p">,</span> <span class="n">NULL</span><span class="p">);</span>
  <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">log</span><span class="p">);</span>
  <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="compiling-and-linking-the-program-separately">
<h3>Compiling and linking the program separately<a class="headerlink" href="#compiling-and-linking-the-program-separately" title="Permalink to this headline">¶</a></h3>
<p>In this method, two separate steps are performed to generate the device executable. First, program objects are compiled by using the clCompileProgram API (for details, see the OpenCL specification); then the compiled programs are linked together to generate the final executable by using the clLinkProgram API (for details, see the OpenCL specification). This method is particularly useful– and is the only way–to link a previously-compiled program. By using this method, users can link their program objects with external program objects to build the final program object.</p>
<p>Both the APIs support similar options (depends on whether one is compiling or linking) as the options in clBuildProgram, to control the compiler and linker. For details about the options supported by each API, see the respective API description section in the OpenCL specification.</p>
<p><strong>Compiling the program</strong></p>
<p>The user must compile each program object separately. This step may be a little tedious if a source program depends on other header files. In that case, separate program objects corresponding each header file must be created first. Then, during compilation, those header programs must be passed as embedded headers along with the intended program object.</p>
<p><strong>Example (derived from the OpenCL specification):</strong></p>
<p>Consider the following program source:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#include &lt;foo.h&gt;</span>
<span class="c1">#include &lt;mydir/myinc.h&gt;</span>
<span class="n">__kernel</span> <span class="n">void</span> <span class="n">image_filter</span> <span class="p">(</span><span class="nb">int</span> <span class="n">n</span><span class="p">,</span> <span class="nb">int</span> <span class="n">m</span><span class="p">,</span> <span class="n">constant</span> <span class="nb">float</span> <span class="o">*</span><span class="n">filter_weights</span><span class="p">,</span>
                            <span class="n">read_only</span> <span class="n">image2d_t</span> <span class="n">src_image</span><span class="p">,</span> <span class="n">write_only</span> <span class="n">image2d_t</span> <span class="n">dst_image</span><span class="p">)</span>
<span class="p">{</span>
<span class="o">...</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This kernel includes two headers, foo.h and mydir/myinc.h. So first create the program objects corresponding to each header as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cl_program</span> <span class="n">foo_pg</span> <span class="o">=</span> <span class="n">clCreateProgramWithSource</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">foo_header_src</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">err</span><span class="p">);</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cl_program</span> <span class="n">myinc_pg</span> <span class="o">=</span> <span class="n">clCreateProgramWithSource</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">myinc_header_src</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">err</span><span class="p">);</span>
</pre></div>
</div>
<p>Suppose the program source described above is given by program_A and is loaded via clCreateProgramWithSource.</p>
<p>Now, these headers can be passed as embedded headers along with the program object</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>cl_program input_headers[2] = { foo_pg, myinc_pg };
char * input_header_names[2] = { “foo.h”, “mydir/myinc.h” };

clCompileProgram(program_A, 0, NULL, // num_devices &amp; device_list
   NULL, // compile_options
   2, // num_input_headers
   input_headers,
   input_header_names,
   NULL, NULL); // pfn_notify &amp; user_data
</pre></div>
</div>
<p><strong>Linking the program</strong></p>
<p>In this phase, multiple pre-compiled program objects are linked together to create a new program object that contains the final executable. The executable binary can be queried by using <code class="docutils literal notranslate"><span class="pre">clGetProgramInfo</span></code> and can be specified to <code class="docutils literal notranslate"><span class="pre">clCreateProgramWithBinary</span></code>, as shown earlier.</p>
<p><strong>Example :</strong></p>
<p>Assume there are two pre-compiled program objects, program_A and
program_B. These two can be linked together as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cl_program</span> <span class="n">program_list</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span> <span class="n">program_A</span><span class="p">,</span> <span class="n">program_B</span><span class="p">};</span>
<span class="n">cl_program</span> <span class="n">program_final</span> <span class="o">=</span> <span class="n">clLinkProgram</span><span class="p">(</span><span class="n">context</span><span class="p">,</span>
    <span class="mi">0</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="o">//</span> <span class="n">num_devices</span> <span class="o">&amp;</span> <span class="n">device_list</span>
    <span class="n">NULL</span><span class="p">,</span> <span class="o">//</span> <span class="n">compile_options</span>
    <span class="mi">2</span><span class="p">,</span> <span class="o">//</span> <span class="n">num_input_programs</span><span class="p">,</span>
    <span class="n">program_list</span><span class="p">,</span> <span class="o">//</span> <span class="n">const</span> <span class="n">cl_program</span>
    <span class="o">*</span><span class="n">input_programs</span><span class="p">,</span>

    <span class="n">user_data</span>

    <span class="n">NULL</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="o">//</span> <span class="n">pfn_notify</span> <span class="o">&amp;</span>
    <span class="n">NULL</span><span class="p">);</span> <span class="o">//</span> <span class="n">errcode_ret</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="supported-standard-opencl-compiler-options">
<span id="supported-standard-opencl-compiler"></span><h2>Supported Standard OpenCL Compiler Options<a class="headerlink" href="#supported-standard-opencl-compiler-options" title="Permalink to this headline">¶</a></h2>
<p>The frequently-used build options are:</p>
<blockquote>
<div><ul class="simple">
<li>-I dir — Add the directory dir to the list of directories to be searched for header files. When parsing #include directives, the     OpenCL compiler resolves relative paths using the current working directory of the application.</li>
<li>-D name — Predefine name as a macro, with definition = 1. For -D name=definition, the contents of definition are tokenized and processed as if they appeared during the translation phase three  in a #define directive. In particular, the definition is truncated by embedded newline characters.
-D options are processed in the order they are given in the options argument to <code class="docutils literal notranslate"><span class="pre">clBuildProgram</span></code>.</li>
</ul>
</div></blockquote>
<p>For additional build options, see the :ref:OpenCL specification.</p>
</div>
<div class="section" id="amd-developed-supplemental-compiler-options">
<span id="amd-developed-supplemental-compiler"></span><h2>AMD-Developed Supplemental Compiler Options<a class="headerlink" href="#amd-developed-supplemental-compiler-options" title="Permalink to this headline">¶</a></h2>
<p>The following supported options are not part of the OpenCL specification:</p>
<blockquote>
<div><ul class="simple">
<li>-g — This is an experimental feature that lets you use the GNU project debugger, GDB, to debug kernels on x86 CPUs running Linux or
cygwin/minGW under Windows. For more details, see Chapter 4, “Debugging and Profiling OpenCL.” This option does not affect the       default optimization of the OpenCL code.</li>
<li>-O0 — Specifies to the compiler not to optimize. This is equivalent to the OpenCL standard option -cl-opt-disable.</li>
<li>-f[no-]bin-source — Does [not] generate OpenCL source in the .source section. For more information, see Appendix C, “OpenCL BinaryImage Format (BIF) v2.0.” by default, this option does NOT generate the source.</li>
<li>-f[no-]bin-llvmir — Does [not] generate LLVM IR in the .llvmir section.
For more information, see Appendix C, “OpenCL Binary Image Format (BIF) v2.0.” By default, this option GENERATES the LLVM IR.</li>
<li>-f[no-]bin-amdil — Does [not] generate AMD IL in the .amdil section. For more information, see Appendix C, “OpenCL Binary Image      Format (BIF) v2.0.” By default, this option does NOT generate the AMD IL.</li>
<li>-f[no-]bin-exe — Does [not] generate the executable (ISA) in the .text section. For more information, see Appendix C, “OpenCL        Binary Image Format (BIF) v2.0.” By default, this option GENERATES the ISA.</li>
<li>-f[no-]bin-hsail — Does [not] generate HSAIL/BRIG in the binary. By default, this option does NOT generate HSA IL/BRIG in the        binary.</li>
<li>-save-temps[=&lt;prefix&gt;] — This option dumps intermediate temporary files, such as IL and ISA code, for each OpenCL kernel. If         &lt;prefix&gt; is not given, temporary files are saved in the default temporary directory (the current directory for Linux, C:Users          &lt;user&gt;AppDataLocal for Windows). If &lt;prefix&gt; is given, those temporary files are saved with the given
&lt;prefix&gt;. If &lt;prefix&gt; is an absolute path prefix, such as
C:yourworkdirmydumpprefix, those temporaries are saved under C:yourworkdir, with mydumpprefix as prefix to all temporary      names. For example,</li>
</ul>
<blockquote>
<div><div class="line-block">
<div class="line">-save-temps</div>
<div class="line">under the default directory</div>
<div class="line">_temp_nn_xxx_yyy.il,  _temp_nn_xxx_yyy.isa</div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="line-block">
<div class="line">-save-temps=aaa</div>
<div class="line">under the default directory</div>
<div class="line">aaa_nn_xxx_yyy.il,  aaa_nn_xxx_yyy.isa</div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="line-block">
<div class="line">-save-temps=C:youdirbbb</div>
<div class="line">under C:youdir</div>
<div class="line">bbb_nn_xxx_yyy.il,  bbb_nn_xxx_yyy.isa</div>
</div>
</div></blockquote>
</div></blockquote>
<p>where xxx and yyy are the device name and kernel name for this build, respectively, and nn is an internal number to identify a build to avoid overriding temporary files. Note that this naming convention is subject to change.</p>
<p>To avoid source changes, there are two environment variables that can be used to change CL options during the runtime.</p>
<ul class="simple">
<li>AMD_OCL_BUILD_OPTIONS — Overrides the CL options specified in clBuildProgram().</li>
<li>AMD_OCL_BUILD_OPTIONS_APPEND — Appends options to those specified in clBuildProgram().</li>
</ul>
</div>
<div class="section" id="creating-device-specific-binaries">
<span id="id8"></span><h2>Creating device-specific binaries<a class="headerlink" href="#creating-device-specific-binaries" title="Permalink to this headline">¶</a></h2>
<p>To generate pre-built device-specific binaries from the OpenCL C source or from other binaries (such as the SPIR binaries), certain add-on steps must be performed on the host side. The following is a typical sequence of steps if device- specific binaries are to be generated from the OpenCL C sources:</p>
<ol class="arabic simple">
<li>Create the program object from OpenCL C source using clCreateProgramWithSource().</li>
<li>Build (i.e. compile and link) the program object (for details, see the “Generating program executable” section).</li>
<li>Read the device-specific binaries from the program object using clGetProgramInfo() as shown below:</li>
</ol>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span><span class="n">Get</span> <span class="n">the</span> <span class="n">number</span> <span class="n">of</span> <span class="n">devices</span> <span class="n">attached</span> <span class="k">with</span> <span class="n">program</span> <span class="nb">object</span>
<span class="n">cl_uint</span> <span class="n">nDevices</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="n">clGetProgramInfo</span><span class="p">(</span><span class="n">program</span><span class="p">,</span> <span class="n">CL_PROGRAM_NUM_DEVICES</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">cl_uint</span><span class="p">),</span> <span class="o">&amp;</span><span class="n">nDevices</span><span class="p">,</span> <span class="n">NULL</span><span class="p">);</span>

<span class="o">//</span><span class="n">Get</span> <span class="n">the</span> <span class="n">Id</span> <span class="n">of</span> <span class="nb">all</span> <span class="n">the</span> <span class="n">attached</span> <span class="n">devices</span>
<span class="n">cl_device_id</span> <span class="o">*</span><span class="n">devices</span> <span class="o">=</span> <span class="n">new</span> <span class="n">cl_device_id</span><span class="p">[</span><span class="n">nDevices</span><span class="p">];</span> <span class="n">clGetProgramInfo</span><span class="p">(</span><span class="n">program</span><span class="p">,</span> <span class="n">CL_PROGRAM_DEVICES</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">cl_device_id</span><span class="p">)</span> <span class="o">*</span> <span class="n">nDevices</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">NULL</span><span class="p">);</span>

<span class="o">//</span> <span class="n">Get</span> <span class="n">the</span> <span class="n">sizes</span> <span class="n">of</span> <span class="nb">all</span> <span class="n">the</span> <span class="n">binary</span> <span class="n">objects</span>
<span class="n">size_t</span> <span class="o">*</span><span class="n">pgBinarySizes</span> <span class="o">=</span> <span class="n">new</span> <span class="n">size_t</span><span class="p">[</span><span class="n">nDevices</span><span class="p">];</span> <span class="n">lGetProgramInfo</span><span class="p">(</span><span class="n">program</span><span class="p">,</span> <span class="n">CL_PROGRAM_BINARY_SIZES</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">size_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">nDevices</span><span class="p">,</span> <span class="n">pgBinarySizes</span><span class="p">,</span> <span class="n">NULL</span><span class="p">);</span>

<span class="o">//</span> <span class="n">Allocate</span> <span class="n">storage</span> <span class="k">for</span> <span class="n">each</span> <span class="n">binary</span> <span class="n">objects</span>
<span class="n">unsigned</span> <span class="n">char</span> <span class="o">**</span><span class="n">pgBinaries</span> <span class="o">=</span> <span class="n">new</span> <span class="n">unsigned</span> <span class="n">char</span><span class="o">*</span><span class="p">[</span><span class="n">nDevices</span><span class="p">];</span>
<span class="k">for</span> <span class="p">(</span><span class="n">cl_uint</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">nDevices</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">pgBinaries</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">new</span> <span class="n">unsigned</span> <span class="n">char</span><span class="p">[</span><span class="n">pgBinarySizes</span><span class="p">[</span><span class="n">i</span><span class="p">]];</span>
<span class="p">}</span>

<span class="o">//</span> <span class="n">Get</span> <span class="nb">all</span> <span class="n">the</span> <span class="n">binary</span> <span class="n">objects</span>
<span class="n">clGetProgramInfo</span><span class="p">(</span><span class="n">program</span><span class="p">,</span> <span class="n">CL_PROGRAM_BINARIES</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">unsigned</span> <span class="n">char</span><span class="o">*</span><span class="p">)</span> <span class="o">*</span> <span class="n">nDevices</span><span class="p">,</span> <span class="n">pgBinaries</span><span class="p">,</span> <span class="n">NULL</span><span class="p">);</span>
</pre></div>
</div>
</div></blockquote>
<p>Now, save these device specific binaries into the files for future use.</p>
</div>
<div class="section" id="command-execution-flow">
<span id="id9"></span><h2>Command execution flow<a class="headerlink" href="#command-execution-flow" title="Permalink to this headline">¶</a></h2>
<p>The runtime system assigns the work in the command queues to the underlying devices. Commands are placed into the queue using the <code class="docutils literal notranslate"><span class="pre">clEnqueue</span></code> commands shown in the listing below.</p>
<table border="1" class="docutils">
<colgroup>
<col width="35%" />
<col width="65%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">OpenCL API Function</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>clCreateCommandQueueWith</td>
<td>Create a command queue for a specific device</td>
</tr>
<tr class="row-odd"><td>Properties (in OpenCL 2.0)</td>
<td>(CPU,GPU.)</td>
</tr>
<tr class="row-even"><td>clCreateCommandQueue()</td>
<td>&#160;</td>
</tr>
<tr class="row-odd"><td>(in OpenCL 1.x; deprecated</td>
<td>&#160;</td>
</tr>
<tr class="row-even"><td>in OpenCL 2.0)</td>
<td>&#160;</td>
</tr>
<tr class="row-odd"><td>clCreateKernel()</td>
<td>Creates a kernel object from the program object.</td>
</tr>
<tr class="row-even"><td>clCreateBuffer()</td>
<td>Creates a buffer object for use via OpenCL kernels.</td>
</tr>
<tr class="row-odd"><td>clSetKernelArg()</td>
<td>Set the kernel arguments, and enqueue the kernel in a</td>
</tr>
<tr class="row-even"><td>clEnqueueNDRangeKernel()</td>
<td>command queue.</td>
</tr>
<tr class="row-odd"><td>clEnqueueReadBuffer(),</td>
<td>Enqueue a command in a command queue to read from a</td>
</tr>
<tr class="row-even"><td>clEnqueueWriteBuffer()</td>
<td>buffer object to host memory, or write to the buffer
object from host memory</td>
</tr>
<tr class="row-odd"><td>clEnqueueWaitForEvents()</td>
<td>Wait for the specified events to complete.</td>
</tr>
</tbody>
</table>
<p>The commands can be broadly classified into three categories.</p>
<blockquote>
<div><ul class="simple">
<li>Kernel commands (for example, clEnqueueNDRangeKernel(), etc.),</li>
<li>Memory commands (for example, clEnqueueReadBuffer(), etc.), and</li>
<li>Event commands (for example, clEnqueueWaitForEvents(), etc.</li>
</ul>
</div></blockquote>
<p>As illustrated in Figure 3.1, the application can create multiple command queues (some in libraries, for different components of the application, etc.). These queues are mixed into one queue per device type. The figure shows command queues 1 and 3 merged into one CPU device queue (blue arrows); command queue 2 (and possibly others) are merged into the GPU device queue (red arrow). The device queue then schedules work onto the multiple compute resources present in the device. Here, K = kernel commands, M = memory commands, and E = event commands.</p>
<img alt="../_images/3.12.png" class="align-center" src="../_images/3.12.png" />
</div>
<div class="section" id="running-the-program">
<span id="running-program"></span><h2>Running the Program<a class="headerlink" href="#running-the-program" title="Permalink to this headline">¶</a></h2>
<div class="section" id="creating-kernel-objects">
<h3>Creating Kernel Objects<a class="headerlink" href="#creating-kernel-objects" title="Permalink to this headline">¶</a></h3>
<p>After a program is created and built, the next step is to run the kernel code on the devices. Running the kernel code requires the creation of one or more kernel objects for each kernel function (declared as “   kernel” or “kernel”). Kernel objects are run-time objects that bind the specific kernel function with the argument values to be used while executing it.</p>
<p>The clCreateKernel API creates a kernel object from a program object by using the name of the kernel function passed with program object. The arguments to kernel objects are set by the following APIs:</p>
<p>clSetKernelArg: used to set all the kernel arguments except SVM pointers.</p>
<p>clSetKernelArgSVMPointer: introduced in OpenCL2.0 as a new API to set
SVM pointers as the argument value.</p>
<p><strong>Example:</strong></p>
<p>A sample kernel definition is shown below.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>kernel void sample_kernel( global const uchar *normalPtr, global uchar *svmPtr)
{
  …
}
</pre></div>
</div>
<p>To create a kernel object for the above kernel, you must pass the program object corresponding to the kernel to the clCreateKernel function. Assuming that the program object containing the above kernel function has been created and built as program, a kernel object for the above kernel would be created as follows:</p>
<p><code class="docutils literal notranslate"><span class="pre">cl_kernel</span> <span class="pre">kernel</span> <span class="pre">=</span> <span class="pre">clCreateKernel(program,</span> <span class="pre">&quot;sample_kernel&quot;,</span> <span class="pre">NULL);</span></code></p>
<p>Suppose a buffer object and an SVM array have been created as follows:</p>
<p><code class="docutils literal notranslate"><span class="pre">cl_mem</span> <span class="pre">buffer</span> <span class="pre">=</span> <span class="pre">clCreateBuffer(context,</span> <span class="pre">CL_MEM_READ_ONLY,</span> <span class="pre">length</span> <span class="pre">*</span> <span class="pre">sizeof(cl_uchar),</span> <span class="pre">NULL,</span> <span class="pre">NULL);</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">cl_uchar</span> <span class="pre">*svmPtr</span> <span class="pre">=</span> <span class="pre">clSVMAlloc(context,</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">CL_MEM_READ_WRITE,</span> <span class="pre">length</span> <span class="pre">*</span> <span class="pre">sizeof(cl_uchar),</span> <span class="pre">0);</span></code></p>
<p>Now, to set the kernel arguments for the kernel object, the buffer (or SVM array in OpenCL 2.0) and the corresponding index must be passed to the kernel as first and second argument, respectively:</p>
<p><code class="docutils literal notranslate"><span class="pre">clSetKernelArg(kernel,</span> <span class="pre">0,</span> <span class="pre">sizeof(cl_mem),</span> <span class="pre">(void</span> <span class="pre">*)&amp;buffer);</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">clSetKernelArgSVMPointer(kernel,</span> <span class="pre">1,</span> <span class="pre">(void</span> <span class="pre">*)(</span> <span class="pre">svmPtr));</span></code></p>
</div>
<div class="section" id="creating-a-command-queue">
<h3>Creating a command queue<a class="headerlink" href="#creating-a-command-queue" title="Permalink to this headline">¶</a></h3>
<p>In order to run kernels or any other commands in a device, the host must create a command queue associated with the device and then en-queue the commands to that command queue. A command queue is associated with only one device; however, a device can have one or more command queues. The device executes the commands in-order or out-of-order depending on the mode set during command creation.</p>
<p>A command queue (host or device) is created by using the clCreateCommandQueueWithProperties API (clCreateCommandQueue in OpenCL 1.x, deprecated in OpenCL 2.0) by specifying the device ID of the targeted device within the context; and the queue properties, which specify the type of the queue (host or device) and the mode of command execution (in-order or out-of-order). For details, see the clCreateCommandQueueWithProperties or clCreateCommandQueue API in the OpenCL specification.</p>
<p><strong>Example: To create a default host-side command queue</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cl_queue_properties</span> <span class="o">*</span><span class="n">props</span> <span class="o">=</span> <span class="n">NULL</span><span class="p">;</span> <span class="n">cl_command_queue</span> <span class="n">commandQueue</span> <span class="o">=</span> <span class="n">clCreateCommandQueueWithProperties</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">deviceId</span><span class="p">,</span> <span class="n">props</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>
</pre></div>
</div>
<p><strong>Example: To create a host-side out-of-order command queue with profiling enabled</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cl_queue_properties</span> <span class="n">prop</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span> <span class="n">CL_QUEUE_PROPERTIES</span><span class="p">,</span> <span class="n">CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE</span> <span class="o">|</span> <span class="n">CL_QUEUE_PROFILING_ENABLE</span><span class="p">,</span> <span class="mi">0</span><span class="p">};</span>
<span class="n">cl_command_queue</span> <span class="n">commandQueue</span> <span class="o">=</span> <span class="n">clCreateCommandQueueWithProperties</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">deviceId</span><span class="p">,</span> <span class="n">props</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>
</pre></div>
</div>
<p><strong>Example: To create a default device-side out-of-order command queue with a specific size</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cl_queue_properties</span> <span class="n">prop</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span> <span class="n">CL_QUEUE_PROPERTIES</span><span class="p">,</span> <span class="n">CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE</span> <span class="o">|</span> <span class="n">CL_QUEUE_ON_DEVICE</span> <span class="o">|</span> <span class="n">CL_QUEUE_ON_DEVICE_DEFAULT</span><span class="p">,</span> <span class="n">CL_QUEUE_SIZE</span><span class="p">,</span> <span class="n">maxQueueSize</span><span class="p">,</span> <span class="mi">0</span> <span class="p">};</span>

<span class="n">cl_command_queue</span> <span class="n">commandQueue</span> <span class="o">=</span> <span class="n">clCreateCommandQueueWithProperties</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">deviceId</span><span class="p">,</span> <span class="n">props</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="running-a-kernel-from-the-host">
<h3>Running a Kernel (from the host)<a class="headerlink" href="#running-a-kernel-from-the-host" title="Permalink to this headline">¶</a></h3>
<p>After a command queue has been created, the queue can be used to en-queue the commands to the associated device. The clEnqueueNDRangeKernel API en-queues a command to execute a kernel to a device. During the kernel en- queue, one must specify the total number of kernel instances or work-items to be executed by the device and the size of each work-group or block. This information is set by the work_dim, global_work_size, local_work_size and global_work_offset arguments. Like any other command en-queuing API, the clEnqueueNDRangeKernel returns an event object that conveys information about the en-queued kernel and can be used to synchronization other commands dependent on this kernel. In this API, a list of events that need to complete before this particular command can be executed can be specified.</p>
<p>For example, suppose a kernel object and command queue, named “kernel” and “commandQueue” respectively, have already been created. Suppose you want to launch the kernel over a 2-D dimensional space having total work-items
{1024x1024} and each block/group size {16x16}. To do this, the kernel can be en-queued into the command queue as follows:</p>
<blockquote>
<div><div class="line-block">
<div class="line">cl_uint workDim = 2;</div>
<div class="line">size_t globalWorkSize[] = {1024, 1024};</div>
<div class="line">size_t localWorkSize[] = {16, 16};</div>
</div>
<div class="line-block">
<div class="line">clEnqueueNDRangeKernel(commandQueue, kernel, workDim, NULL, globalWorkSize, localWorkSize, 0, NULL, NULL);</div>
</div>
</div></blockquote>
</div>
</div>
<div class="section" id="a-note-on-thread-safety">
<span id="note-on-thread-safety"></span><h2>A note on thread safety<a class="headerlink" href="#a-note-on-thread-safety" title="Permalink to this headline">¶</a></h2>
<p>As per the OpenCL specification, all OpenCL API calls except clSetKernelArg and clSetKernelArgSVMPointer, are thread safe. clSetKernelArg and clSetKernelArgSVMPointer are safe to call from any host thread. As long as concurrent calls operate on different cl_kernel objects, clSetKernelArg and clSetKernelArgSVMPointer are also safe to call re-entrantly. However, if clSetKernelArg or clSetKernelArgSVMPointer are called from multiple host threads on the same cl_kernel object at the same time, the behavior of the cl_kernel object is undefined.</p>
<p>For information about additional limitations, see the OpenCL specification.</p>
</div>
<div class="section" id="toolchain-considerations">
<span id="id10"></span><h2>Toolchain considerations<a class="headerlink" href="#toolchain-considerations" title="Permalink to this headline">¶</a></h2>
<p>The compiler tool-chain provides a common framework for both CPUs and GPUs, sharing the front-end and some high-level compiler transformations. The back-ends are optimized for the device type (CPU or GPU). The kernels are compiled by the OpenCL compiler to either CPU binaries or GPU binaries, depending on the target device.</p>
<p>For CPU processing, the OpenCL runtime uses the LLVM AS to generate x86 binaries. The OpenCL runtime automatically determines the number of processing elements, or cores, present in the CPU and distributes the OpenCL kernel between them.</p>
<p>For GPU processing, the OpenCL compiler generates an intermediate representation, called AMDIL or HSAIL, depending on whether the OpenCL 1.2 or OpenCL 2.0 compile-with flag is specified. The OpenCL Runtime layer links the needed libraries and passes the complete IL to the Shader compiler for compilation to GPU-specific binaries.</p>
</div>
</div>
<div class="section" id="profiling-opencl">
<span id="id11"></span><h1>Profiling OpenCL<a class="headerlink" href="#profiling-opencl" title="Permalink to this headline">¶</a></h1>
<p>This chapter discusses how to profile OpenCL programs running on AMD GPU and CPU compute devices. The preferred method is to debug with AMD CodeXL, as described in  “AMD CodeXL GPU Debugger.” The second method, described in  “Debugging CPU Kernels with GDB,” is to use experimental features provided by ROCm (GNU project debugger, GDB) to debug kernels on x86 CPUs running Linux.</p>
<div class="section" id="downloading-and-installing-codexl-and-radeon-compute-profiler">
<span id="amd-codexl-gpu"></span><h2>Downloading and installing CodeXL and Radeon Compute Profiler<a class="headerlink" href="#downloading-and-installing-codexl-and-radeon-compute-profiler" title="Permalink to this headline">¶</a></h2>
<p>Download the latest version of CodeXL from the CodeXL home page:
<a class="reference external" href="http://developer.amd.com/tools-and-sdks/opencl-zone/codexl/">http://developer.amd.com/tools-and-sdks/opencl-zone/codexl/</a></p>
<p>Radeon Compute Profiler is a performance analysis tool that gathers data from the API run-time and GPU for OpenCL™ and ROCm/HSA applications</p>
<p>RCP is installed when you you use rocm-dev upon instal of the driver.  You can access the source code at <a class="reference external" href="https://github.com/GPUOpen-Tools/RCP">https://github.com/GPUOpen-Tools/RCP</a></p>
</div>
<div class="section" id="installing-codexl-on-ubuntu-and-other-debian-based-linux-distributions">
<h2>Installing CodeXL on Ubuntu and other Debian based Linux distributions<a class="headerlink" href="#installing-codexl-on-ubuntu-and-other-debian-based-linux-distributions" title="Permalink to this headline">¶</a></h2>
<p>Either install the tar archive, or install the .deb package.</p>
<p><strong>Tar archive:</strong></p>
<ol class="arabic simple">
<li>Download the AMD_CodeXL_Linux*.tar.gz 64-bit Linux tar package at <a class="reference external" href="https://github.com/GPUOpen-Tools/CodeXL/releases">https://github.com/GPUOpen-Tools/CodeXL/releases</a></li>
<li>Run:
$ tar –xvzf CodeXL_Linux*.tar.gz</li>
</ol>
<p><strong>Debian package :</strong></p>
<ol class="arabic simple">
<li>Download the <code class="docutils literal notranslate"><span class="pre">amdcodexl-*.deb</span> <span class="pre">64-bit</span> <span class="pre">Linux</span> <span class="pre">Debian</span> <span class="pre">package.</span></code></li>
<li>Run: <a href="#id12"><span class="problematic" id="id13">``</span></a>$ sudo dpkg -i amdcodexl_x.x.x-1_amd64.deb ``</li>
<li>Run: <code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">sudo</span> <span class="pre">apt-get</span> <span class="pre">-f</span> <span class="pre">install</span></code></li>
</ol>
<p>Or build the project from source code <a class="reference external" href="https://github.com/GPUOpen-Tools/CodeXL">https://github.com/GPUOpen-Tools/CodeXL</a></p>
</div>
<div class="section" id="using-codexl-for-profiling">
<h2>Using CodeXL for profiling<a class="headerlink" href="#using-codexl-for-profiling" title="Permalink to this headline">¶</a></h2>
<p>Two modes in CodeXL are particularly useful for profiling:</p>
<ul class="simple">
<li>GPU Profile Mode</li>
<li>Analyze Mode</li>
</ul>
<div class="section" id="gpu-profile-mode">
<h3>GPU Profile Mode<a class="headerlink" href="#gpu-profile-mode" title="Permalink to this headline">¶</a></h3>
<p>The GPU Profile Mode helps developers analyze and profile OpenCL™ host and device code. Developers can profile the entire application or only the kernels by using one of the following modes:</p>
<blockquote>
<div><ul class="simple">
<li>Entire application profile: Collect application trace mode</li>
<li>Kernel profile: Collect GPU performance counter mode</li>
</ul>
</div></blockquote>
<p><strong>GPU Profile views:</strong></p>
<p>While running your application in the GPU Profile mode, CodeXL collects valuable information, which is summarized in different views:</p>
<blockquote>
<div><ul class="simple">
<li><strong>API trace:</strong> View API calls with inputs and outputs View API input arguments and output results Find API hotspots
Determine top ten data transfer and kernel execution operations
Identify failed API calls, resource leaks and best practices</li>
</ul>
<blockquote>
<div><img alt="../_images/4.2.png" class="align-center" src="../_images/4.2.png" />
</div></blockquote>
<ul>
<li><p class="first"><strong>Timeline visualization:</strong> Visualize host and device execution in a timeline chart</p>
<p>View number of OpenCL™ contexts and command queues created and the relationships between these items</p>
<p>View data transfer operations and kernel executions on the device</p>
<p>Determine proper synchronization and load balancing</p>
</li>
</ul>
<blockquote>
<div><img alt="../_images/4.3.png" class="align-center" src="../_images/4.3.png" />
</div></blockquote>
<ul>
<li><p class="first"><strong>Warnings/Errors:</strong> View performance suggestions</p>
<blockquote>
<div><p>Includes a helpful list of best practices</p>
<p>Includes recommendations to improve program performance</p>
</div></blockquote>
</li>
<li><p class="first"><strong>Summary pages:</strong> Find top bottlenecks</p>
<p>I/O bound</p>
<p>Compute bound</p>
</li>
</ul>
<img alt="../_images/4.4.png" class="align-center" src="../_images/4.4.png" />
<ul class="simple">
<li><strong>Kernel occupancy:</strong> Estimate OpenCL™ kernel occupancy for AMD APUs and GPUs</li>
</ul>
<blockquote>
<div><p>Visual indication of the limiting kernel resources for number of wavefronts in flight</p>
<p>View the maximum number of wavefronts in flight limited by</p>
<p>–Work group size</p>
<p>–Number of allocated scalar or vector registers</p>
<p>–Amount of allocated LDS</p>
<p>–View the maximum resource limit for the GPU device</p>
</div></blockquote>
<img alt="../_images/4.5.png" class="align-center" src="../_images/4.5.png" />
<ul class="simple">
<li><strong>performance counter:</strong> view kernel performance bottlenecks</li>
</ul>
</div></blockquote>
<img alt="../_images/4.6.png" class="align-center" src="../_images/4.6.png" />
</div>
<div class="section" id="analyze-mode">
<h3>Analyze Mode<a class="headerlink" href="#analyze-mode" title="Permalink to this headline">¶</a></h3>
<p>The Analyze Mode provides a nice way to begin writing your kernel and to compile it to any supported device without the need to have the actual device installed on your machine. Upon successful compilation, the Statistics View can be used to gather useful statistics regarding the GPU usage of kernels.</p>
<p>The Analyze Mode allows a user to do the following:</p>
<ul class="simple">
<li><dl class="first docutils">
<dt><strong>Edit your OpenCL™ kernel inside CodeXL editor</strong></dt>
<dd>Create a new file
Drag and drop an existing OpenCL™ kernel file</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>Highlight keywords</strong></dt>
<dd>The CodeXL editor highlights keywords for easier editing</dd>
</dl>
</li>
</ul>
<img alt="../_images/4.7.png" class="align-center" src="../_images/4.7.png" />
<ul class="simple">
<li>Choose your target device
The Analyze Mode enables to compile to any supported device target, without the need to install the device</li>
<li>Fix OpenCL™ compiler errors and warnings in which the kernel file is the only input
View OpenCL compilation errors and fix immediately.</li>
<li>Edit OpenCL™ Compiler options with an easy options tab
CodeXL summarizes all the OpenCL options so that it is easy to use them.</li>
</ul>
<img alt="../_images/4.8.png" class="align-center" src="../_images/4.8.png" />
<ul class="simple">
<li>View IL and ISA compilation results</li>
</ul>
<img alt="../_images/4.9.png" class="align-center" src="../_images/4.9.png" />
<ul class="simple">
<li>Statistics view: AMD Compiler gathers statistics for the use of GPU resources
Better understanding this data helps tune your kernel for better performance even before running on real GPU
The Statistics tab helps detect where bottlenecks are even before running your application</li>
</ul>
<img alt="../_images/4.10.png" class="align-center" src="../_images/4.10.png" />
</div>
</div>
</div>
<div class="section" id="opencl-static-c-programming-language">
<span id="opencl-static"></span><h1>OpenCL Static C++ Programming Language<a class="headerlink" href="#opencl-static-c-programming-language" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<span id="id14"></span><h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>This extension defines the OpenCL Static C++ kernel language, which is a form of the ISO/IEC Programming languages C++ specification1. This language supports overloading and templates that can be resolved at compile time (hence
static), while restricting the use of language features that require dynamic/runtime resolving. The language also is extended to support most of the features described in Section 6 of the OpenCL 1.2 specification: new data types (vectors, images, samples, etc.), OpenCL 1.2 Built-in functions, and more.</p>
<div class="section" id="supported-features">
<h3>Supported Features<a class="headerlink" href="#supported-features" title="Permalink to this headline">¶</a></h3>
<p>The following list contains the major static C++ features supported by this extension.</p>
<blockquote>
<div><ul>
<li><p class="first">Kernel and function overloading.</p>
</li>
<li><dl class="first docutils">
<dt>Inheritance:</dt>
<dd><div class="first last line-block">
<div class="line">– Strict inheritance.</div>
<div class="line">– Friend classes.</div>
<div class="line">– Multiple inheritance.</div>
</div>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Templates:</dt>
<dd><div class="first last line-block">
<div class="line">–Kernel templates.</div>
<div class="line">–Member templates.</div>
<div class="line">–Template default argument.</div>
<div class="line">–Limited class templates (the virtual. keyword is not exposed).</div>
<div class="line">–Partial template specialization</div>
</div>
</dd>
</dl>
</li>
<li><p class="first">Namespaces.</p>
</li>
<li><p class="first">References.</p>
</li>
<li><p class="first">this operator.</p>
</li>
</ul>
</div></blockquote>
<p>Note that supporting templates and overloading highly improve the efficiency of writing code: it allows developers to avoid replication of code when not necessary.
Using kernel template and kernel overloading requires support from the runtime API as well. AMD provides a simple extension to <code class="docutils literal notranslate"><span class="pre">clCreateKernel</span></code>, which enables the user to specify the desired kernel.</p>
</div>
<div class="section" id="unsupported-features">
<h3>Unsupported Features<a class="headerlink" href="#unsupported-features" title="Permalink to this headline">¶</a></h3>
<p>Static C++ features not supported by this extension are:</p>
<ul class="simple">
<li>Virtual functions (methods marked with the virtual keyword).</li>
<li>Abstract classes (a class defined only of pure virtual functions).</li>
<li>Dynamic memory allocation (non-placement new/delete support is not provided).</li>
<li>Exceptions (no support for throw/catch).</li>
<li>The :: operator.</li>
<li>STL and other standard C++ libraries.</li>
<li>The language specified in this extension can be easily expanded to support these features.</li>
</ul>
</div>
<div class="section" id="relations-with-iso-iec-c">
<h3>Relations with ISO/IEC C++<a class="headerlink" href="#relations-with-iso-iec-c" title="Permalink to this headline">¶</a></h3>
<p>This extension focuses on documenting the differences between the OpenCL Static C++ kernel language and the ISO/IEC Programming languages C++ specification. Where possible, this extension leaves technical definitions to the ISO/IEC specification.</p>
</div>
</div>
<div class="section" id="additions-and-changes-to-section-5-the-opencl-c-runtime">
<span id="opencl-c-runtime"></span><h2>Additions and Changes to Section 5 - The OpenCL C Runtime<a class="headerlink" href="#additions-and-changes-to-section-5-the-opencl-c-runtime" title="Permalink to this headline">¶</a></h2>
<div class="section" id="additions-and-changes-to-section-5-7-1-creating-kernel-objects">
<h3>Additions and Changes to Section 5.7.1 - Creating Kernel Objects<a class="headerlink" href="#additions-and-changes-to-section-5-7-1-creating-kernel-objects" title="Permalink to this headline">¶</a></h3>
<p>In the static C++ kernel language, a kernel can be overloaded, templated, or both. The syntax explaining how to do it is defined in Sections 5.3.4 and 5.3.5, below.</p>
<p>To support these cases, the following error codes were added; these can be returned by clCreateKernel.</p>
<ul class="simple">
<li>CL_INVALID_KERNEL_TEMPLATE_TYPE_ARGUMENT_AMD if a kernel template argument is not a valid type (is neither a valid OpenCL C type or   a user defined type in the same source file).</li>
<li>CL_INVALID_KERNEL_TYPE_ARGUMENT_AMD if a kernel type argument, used for overloading resolution, is not a valid type (is neither a     valid OpenCL C type or user-defined type in the same source program).</li>
</ul>
</div>
<div class="section" id="passing-classes-between-host-and-device">
<h3>Passing Classes between Host and Device<a class="headerlink" href="#passing-classes-between-host-and-device" title="Permalink to this headline">¶</a></h3>
<p>This extension allows a developer to pass classes between the host and the device. The mechanism used to pass the class to the device and back are the existing buffer object APIs. The class that is passed maintains its state (public and private members), and the compiler implicitly changes the class to use either the host-side or device-side methods.</p>
<p>On the host side, the application creates the class and an equivalent memory object with the same size (using the sizeof function). It then can use the class methods to set or change values of the class members. When the class is ready, the application uses a standard buffer API to move the class to the device (either Unmap or Write), then sets the buffer object as the appropriate kernel argument and enqueues the kernel for execution. When the kernel finishes the execution, the application can map back (or read) the buffer object into the class and continue working on it.</p>
</div>
</div>
<div class="section" id="additions-and-changes-to-section-6-the-opencl-1-2-c-programming-language">
<span id="c-programming-language"></span><h2>Additions and Changes to Section 6 - The OpenCL 1.2 C Programming Language<a class="headerlink" href="#additions-and-changes-to-section-6-the-opencl-1-2-c-programming-language" title="Permalink to this headline">¶</a></h2>
<div class="section" id="building-c-kernels">
<h3>Building C++ Kernels<a class="headerlink" href="#building-c-kernels" title="Permalink to this headline">¶</a></h3>
<p>To compile a program that contains static C++ kernels and functions, the application must add the following compile option to clBuildProgramWithSource:</p>
<p>-x language</p>
<p>where language is defined as one of the following:</p>
<blockquote>
<div><ul>
<li><p class="first">clc – the source language is considered to be OpenCL C, as defined in the
The OpenCL Programming Language version 1.21.</p>
</li>
<li><p class="first">clc++ - the source language is considered to be OpenCL C++, as defined in the following sections of the this document.</p>
<blockquote>
<div><p>x clc++ is required if the input language is static C++. -x clc++ may not be used with -cl-std=CL2.0 and may only be used with
cl-std=CL1.2 if-cl-std=CLX.Y is used.</p>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="classes-and-derived-classes">
<h3>Classes and Derived Classes<a class="headerlink" href="#classes-and-derived-classes" title="Permalink to this headline">¶</a></h3>
<p>OpenCL C is extended to support classes and derived classes as per Sections
9 and 10 of the static C++ language specification, with the limitation that virtual functions and abstracts classes are not supported. The virtual keyword is reserved, and the OpenCL C++ compiler is required to report a compile time
error if it is used in the input program.</p>
<p>This limitation restricts class definitions to be fully statically defined. There is nothing prohibiting a future version of OpenCL C++ from relaxing this restriction, pending performance implications.</p>
<p>A class definition can not contain any address space qualifier, either for members or for methods:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">myClass</span><span class="p">{</span>
<span class="n">public</span><span class="p">:</span>
  <span class="nb">int</span> <span class="n">myMethod1</span><span class="p">(){</span> <span class="k">return</span> <span class="n">x</span><span class="p">;}</span>
<span class="n">void</span> <span class="n">private</span><span class="p">:</span>
  <span class="n">__local</span> <span class="n">myMethod2</span><span class="p">(){</span><span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;}</span>
  <span class="nb">int</span> <span class="n">x</span><span class="p">;</span>
  <span class="n">local</span> <span class="n">y</span><span class="p">;</span> <span class="o">//</span> <span class="n">illegal</span>
<span class="p">};</span>
</pre></div>
</div>
<p>The class invocation inside a kernel, however, can be either in private or local address space:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kernel</span> <span class="n">void</span> <span class="n">myKernel</span><span class="p">()</span>
<span class="p">{</span>
  <span class="n">myClass</span> <span class="n">c1</span><span class="p">;</span>
  <span class="n">local</span> <span class="n">myClass</span> <span class="n">c2</span><span class="p">;</span>
  <span class="o">...</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Classes can be passed as arguments to kernels, by defining a buffer object at the size of the class, and using it. The device invokes the adequate device- specific methods, and accesses the class members passed from the host.</p>
<p>OpenCL C kernels (defined with __kernel) may not be applied to a class constructor, destructor, or method, except in the case that the class method is defined static and thus does not require object construction to be invoked.</p>
</div>
<div class="section" id="namespaces">
<h3>Namespaces<a class="headerlink" href="#namespaces" title="Permalink to this headline">¶</a></h3>
<p>Namespaces are support without change as per [1].</p>
</div>
<div class="section" id="overloading">
<h3>Overloading<a class="headerlink" href="#overloading" title="Permalink to this headline">¶</a></h3>
<p>As defined in the static C++ language specification, when two or more different declarations are specified for a single name in the same scope, that name is said to be overloaded. By extension, two declarations in the same scope that declare the same name but with different types are called overloaded declarations. Only kernel and function declarations can be overloaded, not object and type declarations.</p>
<p>As per of the static C++ language specification, a number of restrictions limit how functions can be overloaded; these restrictions are defined formally in Section 13 of the static C++ language specification. Note that kernels and functions cannot be overloaded by return type.</p>
<p>Also, the rules for well-formed programs as defined by Section 13 of the static C++ language specification are lifted to apply to both kernel and function declarations.</p>
<p>The overloading resolution is per Section 13.1 of the static C++ language specification, but extended to account for vector types. The algorithm for “best viable function”, Section 13.3.3 of the static C++ language specification, is extended for vector types by inducing a partial-ordering as a function of the partial-ordering of its elements. Following the existing rules for vector types in the OpenCL 1.2 specification, explicit conversion between vectors is not allowed. (This reduces the number of possible overloaded functions with respect to vectors, but this is not expected to be a particular burden to developers because explicit conversion can always be applied at the point of function evocation.)</p>
<p>For overloaded kernels, the following syntax is used as part of the kernel name:</p>
<p>foo(type1,…,typen)</p>
<p>where type1,…,typen must be either an OpenCL scalar or vector type, or can be a user-defined type that is allocated in the same source file as the kernel foo.</p>
<p>To allow overloaded kernels, use the following syntax:</p>
<blockquote>
<div>__attribute ((mangled_name(myMangledName)))</div></blockquote>
<p>The kernel mangled_name is used as a parameter to pass to the clCreateKernel() API. This mechanism is needed to allow overloaded kernels without changing the existing OpenCL kernel creation API.</p>
</div>
<div class="section" id="templates">
<h3>Templates<a class="headerlink" href="#templates" title="Permalink to this headline">¶</a></h3>
<p>OpenCL C++ provides unrestricted support for C++ templates, as defined in Section 14 of the static C++ language specification. The arguments to templates are extended to allow for all OpenCL base types, including vectors and pointers qualified with OpenCL C address spaces (i.e.     global,     local,     private, and     constant).</p>
<p>OpenCL C++ kernels (defined with     kernel) can be templated and can be called from within an OpenCL C (C++) program or as an external entry point (from the host).</p>
<p>For kernel templates, the following syntax is used as part of the kernel name
(assuming a kernel called foo):</p>
<p>foo&lt;type1,…,typen&gt;</p>
<p>where type1,…,typen must be either OpenCL scalar or vector type, or can be a user-defined type that is allocated in the same    source file as the kernel foo. In this case a kernel is both overloaded and templated:</p>
<p>foo&lt;type1,…,typen&gt;(typen+1,…,typem)</p>
<p>Note that here overloading resolution is done by first matching non-templated arguments in order of appearance in the definition, then substituting template parameters. This allows intermixing of template and non-template arguments in the signature.</p>
<p>To support template kernels, the same mechanism for kernel overloading is used. Use the following syntax:</p>
<blockquote>
<div>attribute ((mangled_name(myMangledName)))</div></blockquote>
<p>The kernel mangled_name is used as a parameter to passed to the clCreateKernel() API. This mechanism is needed to allow template kernels without changing the existing OpenCL kernel creation API. An implementation is not required to detect name collision with the user-specified kernel_mangled names involved.</p>
</div>
<div class="section" id="exceptions">
<h3>Exceptions<a class="headerlink" href="#exceptions" title="Permalink to this headline">¶</a></h3>
<p>Exceptions, as per Section 15 of the static C++ language specification, are not supported. The keywords try, catch, and throw are reserved, and the OpenCL C++ compiler must produce a static compile time error if they are used in the input program.</p>
</div>
<div class="section" id="libraries">
<h3>Libraries<a class="headerlink" href="#libraries" title="Permalink to this headline">¶</a></h3>
<p>Support for the general utilities library, as defined in Sections 20-21 of the static C++ language specification, is not provided. The standard static C++ libraries and STL library are not supported.</p>
</div>
<div class="section" id="dynamic-operation">
<h3>Dynamic Operation<a class="headerlink" href="#dynamic-operation" title="Permalink to this headline">¶</a></h3>
<p>Features related to dynamic operation are not supported:</p>
<blockquote>
<div><ul class="simple">
<li>the virtual modifier.
OpenCL C++ prohibits the use of the virtual modifier. Thus, virtual member functions and virtual inheritance are not supported.</li>
<li>Dynamic cast that requires runtime check.</li>
<li>Dynamic storage allocation and deallocation.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="opencl-c-built-in-functions">
<h3>OpenCL C Built-in Functions<a class="headerlink" href="#opencl-c-built-in-functions" title="Permalink to this headline">¶</a></h3>
<p>All the all OpenCL 1.2 built-in functions are supported.
None of the new built-in functions added in OpenCL 2.0 are supported.</p>
</div>
</div>
<div class="section" id="examples">
<span id="id15"></span><h2>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h2>
<div class="section" id="passing-a-class-from-the-host-to-the-device-and-back">
<h3>Passing a Class from the Host to the Device and Back<a class="headerlink" href="#passing-a-class-from-the-host-to-the-device-and-back" title="Permalink to this headline">¶</a></h3>
<p>The class definition must be the same on the host code and the device code, besides the members’ type in the case of vectors. If the class includes vector data types, the definition must conform to the table that appears on Section 6.1.2</p>
<p>of the OpenCL Programming Specification 1.2, Corresponding API type for
OpenCL Language types.</p>
<p><strong>Example Kernel Code</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Class</span> <span class="n">Test</span>
<span class="p">{</span>
  <span class="n">setX</span> <span class="p">(</span><span class="nb">int</span> <span class="n">value</span><span class="p">);</span>
  <span class="n">private</span><span class="p">:</span>
  <span class="nb">int</span> <span class="n">x</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">kernel</span> <span class="n">foo</span> <span class="p">(</span> <span class="k">global</span> <span class="n">Test</span><span class="o">*</span> <span class="n">InClass</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">get_global_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="n">InClass</span><span class="o">-&gt;</span><span class="n">setX</span><span class="p">(</span><span class="mi">5</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>Example Host Code</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Class Test
{
setX (int value);
private:
int x;
}

MyFunc ()
{
  tempClass = new(Test);
  ... // Some OpenCL startup code – create context, queue, etc.
  cl_mem classObj = clCreateBuffer(context, CL_MEM_USE_HOST_PTR, sizeof(Test), &amp;tempClass, event);
  clEnqueueMapBuffer(...,classObj,...);
  tempClass.setX(10);
  clEnqueueUnmapBuffer(...,classObj,...); //class is passed to the Device
  clEnqueueNDRange(..., fooKernel, ...);
  clEnqueueMapBuffer(...,classObj,...); //class is passed back to the Host
}
</pre></div>
</div>
</div>
<div class="section" id="kernel-overloading">
<h3>Kernel Overloading<a class="headerlink" href="#kernel-overloading" title="Permalink to this headline">¶</a></h3>
<p>This example shows how to define and use mangled_name for kernel overloading, and how to choose the right kernel from the host code. Assume the following kernels are defined:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">__attribute__</span><span class="p">((</span><span class="n">mangled_name</span><span class="p">(</span><span class="n">testAddFloat4</span><span class="p">)))</span> <span class="n">kernel</span> <span class="n">void</span>
<span class="n">testAdd</span><span class="p">(</span><span class="k">global</span> <span class="n">float4</span> <span class="o">*</span> <span class="n">src1</span><span class="p">,</span> <span class="k">global</span> <span class="n">float4</span> <span class="o">*</span> <span class="n">src2</span><span class="p">,</span> <span class="k">global</span> <span class="n">float4</span> <span class="o">*</span> <span class="n">dst</span><span class="p">)</span>
<span class="p">{</span>
  <span class="nb">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">get_global_id</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
  <span class="n">dst</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">src1</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+</span> <span class="n">src2</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>
<span class="p">}</span>
<span class="n">__attribute</span> <span class="p">((</span><span class="n">mangled_name</span><span class="p">(</span><span class="n">testAddInt8</span><span class="p">)))</span> <span class="n">kernel</span> <span class="n">void</span> <span class="n">testAdd</span><span class="p">(</span><span class="k">global</span> <span class="n">int8</span> <span class="o">*</span> <span class="n">src1</span><span class="p">,</span> <span class="k">global</span> <span class="n">int8</span> <span class="o">*</span> <span class="n">src2</span><span class="p">,</span> <span class="k">global</span> <span class="n">int8</span> <span class="o">*</span> <span class="n">dst</span><span class="p">)</span>
<span class="p">{</span>
  <span class="nb">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">get_global_id</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
  <span class="n">dst</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">src1</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+</span> <span class="n">src2</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The names testAddFloat4 and testAddInt8 are the external names for the two kernel instants. When calling <code class="docutils literal notranslate"><span class="pre">clCreateKernel</span></code>, passing one of these kernel names leads to the correct overloaded kernel.</p>
</div>
<div class="section" id="kernel-template">
<h3>Kernel Template<a class="headerlink" href="#kernel-template" title="Permalink to this headline">¶</a></h3>
<p>This example defines a kernel template, testAdd. It also defines two explicit instants of the kernel template, testAddFloat4 and testAddInt8. The names testAddFloat4 and testAddInt8 are the external names for the two kernel template instants that must be used as parameters when calling to the clCreateKernel API.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">template</span> <span class="o">&lt;</span><span class="k">class</span> <span class="nc">T</span><span class="o">&gt;</span>
<span class="n">kernel</span> <span class="n">void</span> <span class="n">testAdd</span><span class="p">(</span><span class="k">global</span> <span class="n">T</span> <span class="o">*</span> <span class="n">src1</span><span class="p">,</span> <span class="k">global</span> <span class="n">T</span> <span class="o">*</span> <span class="n">src2</span><span class="p">,</span> <span class="k">global</span> <span class="n">T</span> <span class="o">*</span> <span class="n">dst</span><span class="p">)</span>
<span class="p">{</span>
  <span class="nb">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">get_global_id</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
  <span class="n">dst</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">src1</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+</span> <span class="n">src2</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>
<span class="p">}</span>

<span class="n">template</span>  <span class="n">attribute</span> <span class="p">((</span><span class="n">mangled_name</span><span class="p">(</span><span class="n">testAddFloat4</span><span class="p">)))</span> <span class="n">kernel</span> <span class="n">void</span> <span class="n">testAdd</span><span class="p">(</span><span class="k">global</span> <span class="n">float4</span> <span class="o">*</span> <span class="n">src1</span><span class="p">,</span> <span class="k">global</span> <span class="n">float4</span> <span class="o">*</span> <span class="n">src2</span><span class="p">,</span> <span class="k">global</span> <span class="n">float4</span> <span class="o">*</span> <span class="n">dst</span><span class="p">);</span>

<span class="n">template</span>  <span class="n">attribute</span> <span class="p">((</span><span class="n">mangled_name</span><span class="p">(</span><span class="n">testAddInt8</span><span class="p">)))</span> <span class="n">kernel</span> <span class="n">void</span> <span class="n">testAdd</span><span class="p">(</span><span class="k">global</span> <span class="n">int8</span> <span class="o">*</span> <span class="n">src1</span><span class="p">,</span> <span class="k">global</span> <span class="n">int8</span> <span class="o">*</span> <span class="n">src2</span><span class="p">,</span> <span class="k">global</span> <span class="n">int8</span> <span class="o">*</span> <span class="n">dst</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="opencl-2-0">
<span id="id16"></span><h1>OpenCL 2.0<a class="headerlink" href="#opencl-2-0" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<span id="id17"></span><h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>The OpenCL 2.0 specification is a significant evolution of OpenCL. It introduces features that allow closer collaboration between the host and OpenCL devices, such as Shared Virtual Memory (SVM) and device-side enqueue. Other features, such as pipes and new image-related additions provide effective ways of expressing heterogeneous programming constructs.</p>
<p>The following sections highlight the salient features of OpenCL 2.0 and provide usage guidelines.</p>
<blockquote>
<div><ul class="simple">
<li>Shared Virtual Memory (SVM)</li>
<li>Generic Address Space</li>
<li>Device-side enqueue and workgroup/sub-group level functions</li>
<li>Atomics and synchronization</li>
<li>Pipes</li>
<li>Program-scope global Variables</li>
<li>Image Enhancements</li>
<li>Non-uniform work group size</li>
</ul>
</div></blockquote>
<p>Sample code is included wherever appropriate; complete samples illustrating the
OpenCL 2.0 and 2.1 features are provided with the ROCm 2.0 OpenCL Language Runtime and Compiler .</p>
<p>For guidelines on how to migrate from OpenCL 1.2 to OpenCL 2.1 and for information about querying for image- and device-specific extensions, see Portability considerations.</p>
<p>For a list of the new and deprecated functions,  “New and deprecated functions in OpenCL 2.0.”</p>
</div>
<div class="section" id="shared-virtual-memory-svm">
<span id="shared-virtual-memory"></span><h2>Shared Virtual Memory (SVM)<a class="headerlink" href="#shared-virtual-memory-svm" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id18">
<h3>Overview<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h3>
<p>In OpenCL 1.2, the host and OpenCL devices do not share the same virtual address space. Consequently, the host memory, the device memory, and communication between the host and the OpenCL devices, need to be explicitly specified and managed. Buffers may need to be copied over to the OpenCL</p>
<p>device memory for processing and copied back after processing. To access locations within a buffer (or regions within an image), the appropriate offsets must be passed to and from the OpenCL devices; a host memory pointer cannot be used on the OpenCL device.</p>
<p>In OpenCL 2.0, the host and OpenCL devices may share the same virtual address space. Buffers need not be copied over between devices. When the host and the OpenCL devices share the address space, communication between the host and the devices can occur via shared memory (pointers). This simplifies programming in heterogeneous contexts.</p>
<p>Support for SVM does not imply or require that the host and the OpenCL devices in an OpenCL 2.0 compliant architecture share actual physical memory. The OpenCL runtime manages the transfer of data between the host and the OpenCL devices; the process is transparent to the programmer, who sees a unified address space.</p>
<p>A caveat, however, concerns situations in which the host and the OpenCL devices access the same region of memory at the same time. It would be highly inefficient for the host and the OpenCL devices to have a consistent view of the memory for each load/store from any device/host. In general, the memory model of the language or architecture implementation determines how or when a memory location written by one thread or agent is visible to another. The memory model also determines to what extent the programmer can control the scope of such accesses.</p>
<p>OpenCL 2.0 adopts the memory model defined in C++11 with some extensions. The memory orders taken from C++11 are: “relaxed”, “acquire”, “release”, “acquire-release”, and “sequential consistent”.</p>
<p>OpenCL 2.0 introduces a new (C++11-based) set of atomic operations with specific memory-model based semantics. Atomic operations are indivisible: a thread or agent cannot see partial results. The atomic operations supported are:</p>
<blockquote>
<div><ul class="simple">
<li>atomic_load/store</li>
<li>atomic_init</li>
<li>atomic_work_item_fence</li>
<li>atomic_exchange</li>
<li>atomic_compare_exchange</li>
<li>atomic_fetch_&lt;op&gt;, where &lt;op&gt; is “add”, “sub”, “xor”, “and”, or “or”</li>
</ul>
</div></blockquote>
<p>OpenCL 2.0 introduces the concept of “memory scope”, which limits the extent to which atomic operations are visible. For example:</p>
<blockquote>
<div><ul class="simple">
<li>“workgroup” scope means that the updates are to be visible only within the work group</li>
<li>“device” scope means that the updates are to be visible only within the device (across workgroups within the device)</li>
<li>“all-svm-devices” scope means the updates are available across devices (GPUs and the host/CPU).</li>
</ul>
</div></blockquote>
<p>OpenCL 2.0 further differentiates between coarse-grained SVM buffer sharing and fine-grained SVM (buffer and system) sharing mechanisms. These mechanisms define the granularity at which the SVM buffers are shared.</p>
<p>Updates to coarse-grained or fine-grained SVM are visible to other devices at synchronization points:</p>
<blockquote>
<div><ul class="simple">
<li>For coarse-grained SVM, the synchronization points are: the mapping or un- mapping of the SVM memory and kernel launch or completion. This means that any updates are visible only at the end of the kernel or at the point of un-mapping the region of      memory.
Coarse-grained buffer memory has a fixed virtual address for all the devices it is allocated on. In the AMD implementation, the      physical memory is allocated on Device Memory.</li>
<li>For fine-grained SVM, the synchronization points include those defined for coarse-grained SVM as well as atomic operations. This      means that updates are visible at the level of atomic operations on the SVM buffer (for fine- grained buffer SVM, allocated with        the CL_MEM_SVM_ATOMICS flag) or the SVM system, i.e. anywhere in the SVM (for fine-grained system SVM).
Fine-grained buffer memory has the same virtual address for all devices it is allocated on. In the AMD implementation, the           physical memory is allocated on the Device-Visible Host Memory. If the fine grain buffer is allocated with the CL_MEM_SVM_ATOMICS       flag, the memory will be GPU-CPU coherent.</li>
</ul>
</div></blockquote>
<p>The OpenCL 2.0 specification mandates coarse-grained SVM but not fine- grained SVM.</p>
<p>For details, see the OpenCL 2.0 specification.</p>
</div>
<div class="section" id="usage">
<h3>Usage<a class="headerlink" href="#usage" title="Permalink to this headline">¶</a></h3>
<p>In OpenCL 2.0, SVM buffers shared between the host and OpenCL devices are created by calling clSVMAlloc (or malloc/new in the case of fine-grain system support). The contents of such buffers may include pointers (into SVM buffers). Pointer-based data structures are especially useful in heterogenous programming scenarios. A typical scenario is as follows:</p>
<ol class="arabic simple">
<li>Host creates SVM buffer(s) with clSVMAlloc</li>
<li>Host maps the SVM buffer(s) with the blocking call clEnqueueSVMMap</li>
<li>Host fills/updates the SVM buffer(s) with data structures, including pointers</li>
<li>Host unmaps the SVM buffer(s) by using clEnqueueSVMUnmap</li>
<li>Host enqueues processing kernels, passing SVM buffers to the kernels with calls to clSetKernelArgSVMPointer and/or clSetKernelExecInfo</li>
<li>The OpenCL 2.0 device processes the structures in SVM buffer(s) including following/updating pointers.</li>
<li>Repeat step 2 through 6 as necessary.</li>
</ol>
<p>Note that the map and unmap operations in Steps 2 and 4 may be eliminated if the SVM buffers are created by using the CL_MEM_SVM_FINE_GRAIN_BUFFER flag, which may not be supported on all devices.</p>
</div>
<div class="section" id="coarse-grained-memory">
<h3>Coarse-grained memory<a class="headerlink" href="#coarse-grained-memory" title="Permalink to this headline">¶</a></h3>
<p>Some applications do not require fine-grained atomics to ensure that the SVM is consistent across devices after each read/write access. After the initial map/creation of the buffer, the GPU or any other devices typically read from memory. Even if the GPU or other devices write to memory, they may not require a consistent view of the memory.</p>
<p>For example, while searching in parallel on a binary search tree , coarse-grain buffers are usually sufficient. In general, coarse-grain buffers provide faster access compared to fine grain buffers as the memory is not required to be consistent across devices.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>for (i = 0; i &lt; keys_per_wi; i++) {
key = search_keys[init_id + i]; tmp_node = root;
  while (1) {
    if (!tmp_node || (tmp_node-&gt;value == key))
      break;
    tmp_node = (key &lt; tmp_node-&gt;value) ? tmp_node-&gt;left : tmp_node-&gt;right;
  }
  found_nodes[init_id + i] = tmp_node;
}
</pre></div>
</div>
<p>In the above example, the binary search tree root is created using coarse- grain SVM on the host:</p>
<p><code class="docutils literal notranslate"><span class="pre">svmTreeBuf</span> <span class="pre">=</span> <span class="pre">clSVMAlloc(context,</span> <span class="pre">CL_MEM_READ_WRITE,</span> <span class="pre">numNodes*sizeof(node),</span> <span class="pre">0);</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">svmSearchBuf</span> <span class="pre">=</span> <span class="pre">clSVMAlloc(context,</span> <span class="pre">CL_MEM_READ_WRITE,</span> <span class="pre">numKeys*sizeof(searchKey),</span> <span class="pre">0);</span></code></p>
<p>The host creates two buffers, svmTreeBuf and svmSearchBuf, to hold the given tree and the search keys, respectively. After populating the given tree, these two buffers are passed to the kernel as parameters.</p>
<p>The next task is to create the tree and populate the svmTreeBuf using <code class="docutils literal notranslate"><span class="pre">clSVMEnqueueMap</span></code> and <code class="docutils literal notranslate"><span class="pre">clSVMEnqueueUnmap</span></code>. The host-code method, cpuCreateBinaryTree, illustrates this mechanism; note the calls to these map/unmap APIs.</p>
<p>The host then creates the keys to be searched in svmSearchBuf, as the cpuInitSearchKeys method illustrates. Next, it enqueues the kernel to search the binary tree for the given keys in the svmSearchBuf, and it sets the parameters to the kernel using clSetKernelArgSVMPointer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">int</span> <span class="n">status</span> <span class="o">=</span> <span class="n">clSetKernelArgSVMPointer</span><span class="p">(</span><span class="n">sample_kernel</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">void</span> <span class="o">*</span><span class="p">)(</span><span class="n">svmTreeBuf</span><span class="p">));</span>

<span class="n">status</span> <span class="o">=</span> <span class="n">clSetKernelArgSVMPointer</span><span class="p">(</span><span class="n">sample_kernel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">void</span> <span class="o">*</span><span class="p">)(</span><span class="n">svmSearchBuf</span><span class="p">));</span>
</pre></div>
</div>
<p>Note that the routine passes both svmTreeBuf and svmSearchBuf to the kernel as parameters. The following node structure demonstrates how to create the tree on the host using pointers to the left and right children:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">typedef</span> <span class="n">struct</span> <span class="n">nodeStruct</span>
<span class="p">{</span>
  <span class="nb">int</span> <span class="n">value</span><span class="p">;</span>
  <span class="n">struct</span> <span class="n">nodeStruct</span><span class="o">*</span> <span class="n">left</span><span class="p">;</span>
  <span class="n">struct</span> <span class="n">nodeStruct</span><span class="o">*</span> <span class="n">right</span><span class="p">;</span>
<span class="p">}</span> <span class="n">node</span><span class="p">;</span>
</pre></div>
</div>
<p>At this point, the advantage of using SVM becomes clear. Because the structure and its nodes are SVM memory, all the pointer values in these nodes are valid on the GPUs as well.</p>
<p>The kernel running on the OpenCL 2.0 device can directly search the tree as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">while</span><span class="p">(</span><span class="n">NULL</span> <span class="o">!=</span> <span class="n">searchNode</span><span class="p">)</span>
<span class="p">{</span>
   <span class="k">if</span><span class="p">(</span><span class="n">currKey</span><span class="o">-&gt;</span><span class="n">key</span> <span class="o">==</span> <span class="n">searchNode</span><span class="o">-&gt;</span><span class="n">value</span><span class="p">)</span>
   <span class="p">{</span>
     <span class="o">/*</span> <span class="n">rejoice</span> <span class="n">on</span> <span class="n">finding</span> <span class="n">key</span> <span class="o">*/</span>
     <span class="n">currKey</span><span class="o">-&gt;</span><span class="n">oclNode</span>  <span class="o">=</span> <span class="n">searchNode</span><span class="p">;</span>
     <span class="n">searchNode</span>        <span class="o">=</span> <span class="n">NULL</span><span class="p">;</span>
   <span class="p">}</span>
   <span class="k">else</span> <span class="k">if</span><span class="p">(</span><span class="n">currKey</span><span class="o">-&gt;</span><span class="n">key</span> <span class="o">&lt;</span> <span class="n">searchNode</span><span class="o">-&gt;</span><span class="n">value</span><span class="p">)</span>
   <span class="p">{</span>
     <span class="o">/*</span> <span class="n">move</span> <span class="n">left</span> <span class="o">*/</span>
     <span class="n">searchNode</span> <span class="o">=</span> <span class="n">searchNode</span><span class="o">-&gt;</span><span class="n">left</span><span class="p">;</span>
   <span class="p">}</span>
   <span class="k">else</span>
   <span class="p">{</span>
     <span class="o">/*</span> <span class="n">move</span> <span class="n">right</span> <span class="o">*/</span>
     <span class="n">searchNode</span> <span class="o">=</span> <span class="n">searchNode</span><span class="o">-&gt;</span><span class="n">right</span><span class="p">;</span>
   <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Each work item searches one element in svmSearchKeys in parallel and sets oclNode in the searchKey structure for that node.</p>
<p>Updates to the tree occur on the host (CPU) or on the GPU, but not on both simultaneously.</p>
<p>Because the tree is created on the host, and because OpenCL 1.2 disallows SVM, implementing these steps is difficult in OpenCL 1.2. In OpenCL 1.2, you must store the tree as arrays, copy the arrays to the GPU memory (specifying the appropriate offsets), and then copy the arrays back to the host.</p>
<p>The “data” is the tree created by the host as a coarse-grain buffer and is passed to the kernel as an input pointer.</p>
<img alt="../_images/6.1.png" class="align-center" src="../_images/6.1.png" />
<p>The above table shows the performance of the 2.0 implementation over the 1.2 implementation. As you can see, the GPU times mentioned under the OpenCL 1.2 column include the GPU run time, time to transfer the buffers from the host
to the device, the time required to transform the buffers into arrays and offsets, and the time required to transfer the buffers from the device back to the host, respectively.</p>
<p>Finally, more than 5M nodes could not be allocated in 1.2, as the allowable memory allocation was limited by the amount of memory that could be used on the device. Overall, the 2.0 version exceeds the 1.2 version in both performance and usability.</p>
</div>
</div>
<div class="section" id="generic-address-space">
<span id="generi"></span><h2>Generic Address Space<a class="headerlink" href="#generic-address-space" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id19">
<h3>Overview<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h3>
<p>In OpenCL 1.2, all pointer parameters in a function definition must have address spaces associated with them. (The default address space is the private address space.) This necessitates creating an explicit version of the function for each desired address space.</p>
<p>OpenCL 2.0 introduces a new address space called the generic address space. Data cannot be stored in the generic address space, but a pointer to this space can reference data located in the private, local, or global address spaces. A function with generic pointer arguments may be called with pointers to any address space except the constant address space. Pointers that are declared without pointing to a named address space, point to the generic address space. However, such pointers must be associated with a named address space before they can be used. Functions may be written with arguments and return values that point to the generic address space, improving readability and programmability.</p>
</div>
<div class="section" id="id20">
<h3>Usage<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="generic-example">
<h3>Generic example<a class="headerlink" href="#generic-example" title="Permalink to this headline">¶</a></h3>
<p>In OpenCL 1.2, the developer needed to write three functions for a pointer p that can reference the local, private, or global address space:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>void fooL (local int *p) { … }
void fooP (private int *p) { … }
void fooG (global int *p) { … }
</pre></div>
</div>
<p>In OpenCL 2.0, the developer needs to write only one function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">void</span> <span class="n">foo</span> <span class="p">(</span><span class="nb">int</span> <span class="o">*</span><span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
<p>As foo is a generic function, the compiler will accept calls to it with pointers to any address space except the constant address space.
Note The generic address space feature also allows one to define a pointer-based data structure that can apply to different address spaces. In OpenCL 1.2, different structure types must be defined for different address spaces; in OpenCL 2.0, a single structure suffices, as shown below.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">struct</span> <span class="n">node</span><span class="p">{</span>
<span class="n">struct</span> <span class="n">node</span><span class="o">*</span> <span class="nb">next</span><span class="p">;</span>    <span class="o">//</span> <span class="n">generic</span> <span class="n">address</span>      <span class="n">space</span> <span class="n">pointer</span>
<span class="p">}</span> <span class="p">;</span>
</pre></div>
</div>
</div>
<div class="section" id="opencl-example">
<h3>OpenCL example<a class="headerlink" href="#opencl-example" title="Permalink to this headline">¶</a></h3>
<p>OpenCL sample, addMul2d is a generic function that uses generic address spaces for its operands. The function computes the convolution sum of two vectors. Two kernels compute the convolution: one uses data in the global address space (convolution2DUsingGlobal); the other uses the local address space (sepiaToning2DUsingLocal). The use of a single function improves the readability of the source.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">float4</span> <span class="n">addMul2D</span> <span class="p">(</span><span class="n">uchar4</span> <span class="o">*</span><span class="n">src</span><span class="p">,</span> <span class="nb">float</span> <span class="o">*</span><span class="nb">filter</span><span class="p">,</span> <span class="n">int2</span> <span class="n">filterDim</span><span class="p">,</span> <span class="nb">int</span> <span class="n">width</span><span class="p">)</span>
<span class="p">{</span>
  <span class="nb">int</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">;</span>
  <span class="n">float4</span> <span class="nb">sum</span> <span class="o">=</span> <span class="p">(</span><span class="n">float4</span><span class="p">)(</span><span class="mi">0</span><span class="p">);</span>
  <span class="k">for</span><span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">filterDim</span><span class="o">.</span><span class="n">y</span><span class="p">);</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
  <span class="p">{</span>
    <span class="k">for</span><span class="p">(</span><span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">filterDim</span><span class="o">.</span><span class="n">x</span><span class="p">);</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span>
    <span class="p">{</span>
      <span class="nb">sum</span> <span class="o">+=</span> <span class="p">(</span><span class="n">convert_float4</span><span class="p">(</span><span class="n">src</span><span class="p">[(</span><span class="n">i</span><span class="o">*</span><span class="n">width</span><span class="p">)</span><span class="o">+</span><span class="n">j</span><span class="p">]))</span><span class="o">*</span><span class="p">((</span><span class="n">float4</span><span class="p">)(</span><span class="nb">filter</span><span class="p">[(</span><span class="n">i</span><span class="o">*</span><span class="n">filterDim</span><span class="o">.</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span><span class="n">j</span><span class="p">]));</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="nb">sum</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="device-side-enqueue-and-workgroup-sub-group-level-functions">
<span id="device-side-enqueue"></span><h2>Device-side enqueue and workgroup/sub-group level functions<a class="headerlink" href="#device-side-enqueue-and-workgroup-sub-group-level-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id21">
<h3>Device-side enqueue<a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h3>
<p>In OpenCL 1.2, a kernel cannot be enqueued from a currently running kernel. Enqueuing a kernel requires returning control to the host – potentially undermining performance.</p>
<p>OpenCL 2.0 allows kernels to enqueue other kernels. It provides a new construct, “clang blocks,” and new built-in functions that allow a parent kernel to queue child</p>
<p>kernels. In addition, OpenCL 2.0 deprecates the run-time API call <code class="docutils literal notranslate"><span class="pre">clCreateCommandQueue</span></code>, in favor of a new call, <code class="docutils literal notranslate"><span class="pre">clCreateCommandQueueWithProperties</span></code>, that can create device-side command queues.</p>
<p>Because it eliminates the overhead of returning kernel-launch control to the host, device-side enqueue can in many cases improve application performance. Some platforms (such as AMD’s) provide a standard way of enqueuing work to the hardware, which can further improve the performance. Device-side enqueue has been observed to reduce by the overhead of enqueuing by more than 3x in some cases.</p>
<p>Applications that are inherently recursive or that require additional processing can derive particular benefit. A classic example of the latter case is a tree search that discovers new nodes when traversing from the root to the leaves.</p>
<p>Device enqueue is also useful in determining when all the workgroups of the parent kernel have finished executing. Doing so in OpenCL 1.2 requires waiting on a completion event from that kernel. If the host needs the result of a computation, the routine may also need to wait on the host. Since OpenCL 2.0 allows the parent kernel to launch child kernels, it can eliminate this delay.</p>
</div>
<div class="section" id="workgroup-subgroup-level-functions">
<h3>Workgroup/subgroup-level functions<a class="headerlink" href="#workgroup-subgroup-level-functions" title="Permalink to this headline">¶</a></h3>
<p>OpenCL 2.0 introduces new built-in functions that operate at the workgroup or subgroup level. (A workgroup comprises one or more subgroups; the vendor handles the exact subgroup implementation.) For example, on AMD platforms, a subgroup maps to a “wavefront”. (For details, see the AMD OpenCL User Guide.)</p>
<p>Basically, a wavefront is an execution unit on the GPU. The OpenCL specification requires that all work items in a workgroup/subgroup executing the kernel handle these new functions; otherwise, their results may be undefined.</p>
<p>OpenCL 2.0 defines the following new built-in functions. Note that it also defines similar functions for subgroups under the cl_khr_subgroups extensions in CL_DEVICE_EXTENSIONS.</p>
<ol class="arabic simple">
<li>work_group_all and work_group_any: These functions test a given predicate on all work items in the workgroup. The “all” version effectively performs an AND operation on all predicates and returns the result to all work items; similarly, the “any” operation performs an OR operation. Thus, using the “all” function returns true if the predicate is true for all work items; “any” returns true if it is true for at least one work item.</li>
<li>work_group_broadcast: This function broadcasts a local value from each work item to all the others in the workgroup.</li>
<li>work_group_reduce: Given an operation, work_group_reduce performs the reduction operation on all work items and returns the result. The operation can be min, max or add. For example, when called for an array using the add operation, the function returns the sum of the array elements.</li>
<li>work_group_inclusive/exclusive_scan: The “scan” operation is a prefix operation, which performs a reduction up to the work-item ID. If it includes the current ID, the function applies an inclusive scan; otherwise, if it covers everything up to but not including the current work item, it applies an exclusive scan. Again, the operation can be min, max or add.</li>
</ol>
<p>OpenCL 2.0 introduces a Khronos sub-group extension. Sub-groups are a logical abstraction of the hardware SIMD execution model akin to wavefronts, warps, or vectors and permit programming closer to the hardware in a vendor-independent manner.  This extension includes a set of cross-sub-group built-in functions that
match the set of the cross-work-group built-in functions specified above.</p>
</div>
<div class="section" id="id22">
<h3>Usage<a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="iterate-until-convergence">
<h3>Iterate until convergence<a class="headerlink" href="#iterate-until-convergence" title="Permalink to this headline">¶</a></h3>
<p>Suppose a complex process requires 4 kernels, A, B, C, and Check, and that these kernels must be run in order repeatedly until the Check kernel produces a value indicating that the process has converged.</p>
<p>In OpenCL 1.2, the host side code to perform this might be structured as follows:</p>
<ol class="arabic simple">
<li>Enqueue kernel A</li>
<li>Enqueue kernel B</li>
<li>Enqueue kernel C</li>
<li>Enqueue kernel Check</li>
<li>Enqueue blocking map of Check result, e.g. with clEnqueueMapBuffer</li>
<li>If Check result is not “Converged” then: Enqueue unmap of Check result</li>
<li>Go to Step 1</li>
</ol>
<p>However, with device-side enqueue in OpenCL 2.0, the Check kernel may be altered to enqueue blocks that carry out A, B, C, and Check when it detects that convergence has not been reached. This avoids a potentially costly interaction with the host on each iteration.  Also, a slight modification of Check might allow the replacement of the entire loop above with a single host-side enqueue of the Check kernel.</p>
</div>
<div class="section" id="data-dependent-refinement">
<h3>Data-dependent refinement<a class="headerlink" href="#data-dependent-refinement" title="Permalink to this headline">¶</a></h3>
<p>Consider a search or computational process that works from coarse levels to increasingly finer levels that operates something like this:</p>
<ol class="arabic simple">
<li>Search/Compute over current region</li>
<li>Loop over sub-regions in current region</li>
<li><dl class="first docutils">
<dt>If a sub-region is interesting:</dt>
<dd><ul class="first last">
<li>Refine the sub-region</li>
<li>Apply a process to the refined sub-region</li>
</ul>
</dd>
</dl>
</li>
</ol>
<p>With OpenCL 1.2, this process would require a complex interaction between the host and the OpenCL device. The device-side kernel would need to somehow mark the sub-regions requiring further work, and the host side code would need to scan all of the sub-regions looking for the marked ones and then enqueue a kernel for each marked sub-region. This process is made more difficult by the lack of globally visible atomic operations in OpenCL 1.2.</p>
<p>However, with OpenCL 2.0, rather than just marking each interesting sub-region, the kernel can instead launch a new sub-kernel to process each marked sub- region. This significantly simplifies the code and improves efficiency due to the elimination of the interactions with, and dependence on, the host.</p>
</div>
<div class="section" id="binary-search-using-device-side-enqueuenote">
<h3>Binary search using device-side enqueueNote<a class="headerlink" href="#binary-search-using-device-side-enqueuenote" title="Permalink to this headline">¶</a></h3>
<p>The power of device enqueue is aptly illustrated in the example of binary search. To make the problem interesting, multiple keys in a sorted array will be searched for. The versions written for OpenCL 1.2 and 2.0 will also be compared with respect to programmability and performance.</p>
<p>A binary search looks for a given key in a sorted sequence by dividing the sequence in two equal parts and then recursively checking the part that contains the key. Because a typical GPU processes more than two work items, we divide the sequence into several parts (globalThreads), and each work item searches its part for the key. Furthermore, to make things more interesting, a large number of keys are searched. At every recursion stage, the amount of work varies with the chunk size. Thus, the algorithm is a good candidate for device- side enqueue.</p>
<p>The OpenCL 1.2 version of the code that performs binary search is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kernel</span> <span class="n">void</span> <span class="n">binarySearch_mulkeys</span><span class="p">(</span> <span class="k">global</span> <span class="nb">int</span> <span class="o">*</span><span class="n">keys</span><span class="p">,</span> <span class="k">global</span> <span class="n">uint</span>
<span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="n">const</span> <span class="n">unsigned</span> <span class="nb">int</span> <span class="n">numKeys</span><span class="p">,</span> <span class="k">global</span> <span class="nb">int</span> <span class="o">*</span><span class="n">output</span><span class="p">)</span>
<span class="p">{</span>
  <span class="nb">int</span> <span class="n">gid</span> <span class="o">=</span> <span class="n">get_global_id</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
  <span class="nb">int</span> <span class="n">lBound</span> <span class="o">=</span> <span class="n">gid</span> <span class="o">*</span> <span class="mi">256</span><span class="p">;</span>
  <span class="nb">int</span> <span class="n">uBound</span> <span class="o">=</span> <span class="n">lBound</span> <span class="o">+</span> <span class="mi">255</span><span class="p">;</span>
  <span class="k">for</span><span class="p">(</span><span class="nb">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">numKeys</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
  <span class="p">{</span>
    <span class="k">if</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="nb">input</span><span class="p">[</span><span class="n">lBound</span><span class="p">]</span> <span class="o">&amp;&amp;</span> <span class="n">keys</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span><span class="nb">input</span><span class="p">[</span><span class="n">uBound</span><span class="p">])</span>
      <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">lBound</span><span class="p">;</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The search for multiple keys is done sequentially, while the sorted array is divided into 256 sized chunks. The NDRange is the size of the array divided by the chunk size. Each work item checks whether the key is present in the range and if the key is present, updates the output array.</p>
<p>The issue with the above approach is that if the input array is very large, the number of work items (NDRange) would be very large. The array is not divided into smaller, more-manageable chunks.</p>
<p>In OpenCL 2.0, the device enqueue feature offers clear advantages in binary search performance.</p>
<p>The kernel is rewritten in OpenCL 2.0 to enqueue itself. (For full details, see the complete sample in the AMD Compute SDK.) Each work item in the binarySearch_device_enqueue_multiKeys_child kernel searches its portion of the sequence for the keys; if it finds one, it updates the array bounds for that key and also sets a variable, , to declare that another enqueue is necessary. If all work items report failure, the search stops and reports that the sequence contains no keys.</p>
<p>Finally, the kernel launches itself again using device enqueue, but with new bounds:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">void</span> <span class="p">(</span><span class="o">^</span><span class="n">binarySearch_device_enqueue_wrapper_blk</span><span class="p">)(</span><span class="n">void</span><span class="p">)</span> <span class="o">=</span>
 <span class="o">^</span><span class="p">{</span><span class="n">binarySearch_device_enqueue_multiKeys_child</span><span class="p">(</span><span class="n">outputArray</span><span class="p">,</span>
                                               <span class="n">sortedArray</span><span class="p">,</span>
                                               <span class="n">subdivSize</span><span class="p">,</span>
                                               <span class="n">globalLowerIndex</span><span class="p">,</span>
                                               <span class="n">keys</span><span class="p">,</span><span class="n">nKeys</span><span class="p">,</span>
                                               <span class="n">parentGlobalids</span><span class="p">,</span>
                                               <span class="n">globalThreads</span><span class="p">);</span>
  <span class="p">};</span>
<span class="nb">int</span> <span class="n">err_ret</span> <span class="o">=</span> <span class="n">enqueue_kernel</span><span class="p">(</span><span class="n">defQ</span><span class="p">,</span><span class="n">CLK_ENQUEUE_FLAGS_WAIT_KERNEL</span><span class="p">,</span><span class="n">ndrange1</span><span class="p">,</span><span class="n">binarySe</span> <span class="n">arch_device_enqueue_wrapper_blk</span><span class="p">);</span>
</pre></div>
</div>
<p>It also checks for missing keys; absent any such keys, the search stops by forgoing further enqueues:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/****</span> <span class="n">Search</span> <span class="n">continues</span> <span class="n">only</span> <span class="k">if</span> <span class="n">at</span> <span class="n">least</span> <span class="n">one</span> <span class="n">key</span> <span class="ow">is</span> <span class="n">found</span> <span class="ow">in</span> <span class="n">previous</span> <span class="n">search</span> <span class="o">****/</span>
<span class="nb">int</span> <span class="n">Flag</span> <span class="o">=</span> <span class="n">atomic_load_explicit</span><span class="p">(</span><span class="o">&amp;</span><span class="p">,</span><span class="n">memory_order_seq_cst</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">Flag</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span><span class="p">;</span>
</pre></div>
</div>
<p>The advantage is that when the input array is large, the OpenCL 2.0 version divides the input array into 1024-sized chunks. The chunk in which the given key falls is found and another kernel is enqueued which further divides it into 1024- sized chunks, and so on. In OpenCL 1.2, as the whole array is taken as the NDRange, a huge number of work groups require processing.</p>
<p>The following figure shows how the OpenCL 2.0 version compares to the OpenCL 1.2 as the array increases beyond a certain size.</p>
<img alt="../_images/6.2.png" class="align-center" src="../_images/6.2.png" />
<p>Note: These numbers are for an A10-7850K (3.7GHz) processor with 4GB of RAM running Windows 8.1.</p>
<p>The above figure shows the performance benefit of using OpenCL 2.0 over the same sample using OpenCL 1.2. In OpenCL 2.0, the reduced number of kernel launches from the host allow superior performance. The kernel enqueues are much more efficient when done from the device.</p>
<p>Device enqueue is a powerful feature, as the examples above help show. It can be especially useful when repeatedly applying a set of kernels to a data structure in accordance with a condition. For applications with dynamic data parallelism at run time-such as when searching a large space for which the amount of parallelism or the problem size is statically unknown from the outset-device enqueue offers many benefits.</p>
<p>The above examples also exemplify the new workgroup and subgroup functions that OpenCL 2.0 introduces. These functions can efficiently perform computation at the workgroup level because they can map directly to hardware instructions at the workgroup/subgroup level.</p>
</div>
</div>
<div class="section" id="atomics-and-synchronization">
<span id="atomics"></span><h2>Atomics and synchronization<a class="headerlink" href="#atomics-and-synchronization" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id23">
<h3>Overview<a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h3>
<p>In OpenCL 1.2, only work-items in the same workgroup can synchronize. OpenCL 2.0 introduces a new and detailed memory model which allows
developers to reason about the effects of their code on memory, and in particular understand whether atomic operations and fences used for synchronization ensure the visibility of variables being used to communicate between threads. In conjunction with the new memory model, OpenCL 2.0 adds a new set of atomic built-in functions and fences derived from C++11 (although the set of types is restricted), and also deprecates the 1.2 atomic built in functions and fences.</p>
<p>These additions allow synchronization between work-items in different work- groups, as well as fine-grained synchronization with the host using atomic operations on memory in fine-grained SVM buffers (allocated with the CL_MEM_SVM_ATOMICS flag) for fine-grained SVM system memory.</p>
</div>
<div class="section" id="id24">
<h3>Usage<a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h3>
<p>The following examples to illustrate the use of atomics are part of the AMD Compute SDK.</p>
</div>
<div class="section" id="atomic-loads-stores">
<h3>Atomic Loads/Stores<a class="headerlink" href="#atomic-loads-stores" title="Permalink to this headline">¶</a></h3>
<p>This sample illustrates atomic loads/stores with the use of memory orders.</p>
<p>The first step is to create this memory on the host:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">buffer</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span> <span class="o">*</span> <span class="p">)</span> <span class="n">clSVMAlloc</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">CL_MEM_SVM_FINE_GRAIN_BUFFER</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">sizeof</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="mi">4</span><span class="p">);</span>

<span class="n">atomicBuffer</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span> <span class="o">*</span> <span class="p">)</span> <span class="n">clSVMAlloc</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">CL_MEM_SVM_FINE_GRAIN_BUFFER</span> <span class="o">|</span> <span class="n">CL_MEM_SVM_ATOMICS</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">sizeof</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="mi">4</span><span class="p">);</span>
</pre></div>
</div>
<p>Note the flags sent as parameters: <code class="docutils literal notranslate"><span class="pre">CL_MEM_SVM_FINE_GRAIN_BUFFER</span></code> and <code class="docutils literal notranslate"><span class="pre">CL_MEM_SVM_ATOMICS.</span></code> The following kernel runs on all work items in parallel. It will atomically load atomicBuffer[0], check whether its value is 99, and wait till it is 99. The acquire memory order is used to indicate that the latest update must be done on the host and to ensure that the local L1 cache is not read from. This will be made 99 by the host (CPU) by</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="p">::</span><span class="n">atomic_store_explicit</span> <span class="p">((</span><span class="n">std</span><span class="p">::</span><span class="n">atomic</span><span class="o">&lt;</span><span class="nb">int</span><span class="o">&gt;</span> <span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">atomicBuffer</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">99</span><span class="p">,</span> <span class="n">std</span><span class="p">::</span><span class="n">memory_order_release</span><span class="p">);</span>
</pre></div>
</div>
<p>The host uses the C++11 compiler and the same memory model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">__kernel</span> <span class="n">void</span> <span class="n">ldstore</span><span class="p">(</span><span class="n">volatile</span> <span class="k">global</span> <span class="nb">int</span> <span class="o">*</span><span class="n">buffer</span><span class="p">,</span> <span class="k">global</span> <span class="nb">int</span><span class="o">*</span> <span class="n">atomicBuffer</span><span class="p">)</span>
<span class="p">{</span>
  <span class="nb">int</span> <span class="n">i</span><span class="p">;</span>
  <span class="k">while</span> <span class="p">(</span><span class="n">atomic_load_explicit</span> <span class="p">((</span><span class="k">global</span> <span class="n">atomic_int</span><span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">atomicBuffer</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">memory_order_acquire</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">99</span><span class="p">);</span>
  <span class="n">i</span> <span class="o">=</span> <span class="n">get_global_id</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
  <span class="n">buffer</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">i</span><span class="p">;</span>
  <span class="n">atomic_store_explicit</span> <span class="p">((</span><span class="k">global</span> <span class="n">atomic_int</span><span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">atomicBuffer</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">(</span><span class="mi">100</span><span class="o">+</span><span class="n">i</span><span class="p">),</span> <span class="n">memory_order_release</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The kernel next stores (100+i), where i is the ID of the work-item into atomicBuffer[i]. The order used is memory_order_release which ensures that the updated copy reaches the CPU which is waiting for it to report PASS for the test.</p>
<p>After the atomic operation, the updates on fine-grain variables (such as buffer) will also be available at the host. The CPU checks for the following to ensure that the results are OK:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">N</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span>
  <span class="k">while</span><span class="p">(</span><span class="n">std</span><span class="p">::</span><span class="n">atomic_load_explicit</span> <span class="p">((</span><span class="n">std</span><span class="p">::</span><span class="n">atomic</span><span class="o">&lt;</span><span class="nb">int</span><span class="o">&gt;*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">atomicBuffer</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">std</span><span class="p">::</span><span class="n">memory_order_acquire</span><span class="p">)</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">100</span><span class="o">+</span><span class="n">i</span><span class="p">));</span>
  <span class="o">/*</span> <span class="n">check</span> <span class="n">the</span> <span class="n">results</span> <span class="n">now</span> <span class="o">*/</span>
  <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">N</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span>
  <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">buffer</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">64</span><span class="o">+</span><span class="n">i</span><span class="p">))</span>
      <span class="n">printf</span><span class="p">(</span><span class="s2">&quot; Test Failed </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">);</span>
    <span class="k">else</span>
      <span class="n">printf</span> <span class="p">(</span><span class="s2">&quot; Test Passed! </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">);</span>
  <span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="atomic-compare-and-exchange-cas">
<h3>Atomic Compare and Exchange (CAS)<a class="headerlink" href="#atomic-compare-and-exchange-cas" title="Permalink to this headline">¶</a></h3>
<p>This sample illustrates the use of the atomic CAS operation typically used for “lock-free” programming, in which a critical section can be created without having to use waiting mutexes/semaphores. The following kernel simultaneously inserts the IDs of various work items into the “list” array by using atomic CAS operation. The same loop also runs on the host and inserts the other half (N) work items. In this way, 2*N numbers are inserted into this “list”.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>__kernel void linkKernel( global int *list) {
  int head, i;
  i = get_global_id(0) + 1;
  head = list[0];
  if (i != get_global_size(0)) {
    do {
      list[i] = head;
    } while (!atomic_compare_exchange_strong((global atomic_int *) &amp;list[0], &amp;head,i), memory_order_release, memory_order_acquire,            memory_scope_system);
  }
}
</pre></div>
</div>
<p>Note how there is no wait to enter the critical section, but list[0] and head are updated atomically. On the CPU too, a similar loop runs. Again note that the variables “list”and “head” must be in fine-grain SVM buffers. memory_order_release and memory_scope_system are used to ensure that the CPU gets the updates – hence the name “platform atomics.”</p>
</div>
<div class="section" id="atomic-fetch">
<h3>Atomic Fetch<a class="headerlink" href="#atomic-fetch" title="Permalink to this headline">¶</a></h3>
<p>This sample illustrates the use of the atomic fetch operation. The fetch operation is an RMW (Read-Modify-Write) operation. The following kernel computes the maximum of the N numbers in array “A”. The result of the intermediate comparisons is computed and the result is placed in a Boolean array “B”. After the matrix “B” is computed, the row (i) is computed. The row which has all 1s will be the maximum (C[i]).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">__kernel</span> <span class="n">void</span> <span class="n">atomicMax</span><span class="p">(</span><span class="n">volatile</span> <span class="k">global</span> <span class="nb">int</span> <span class="o">*</span><span class="n">A</span><span class="p">,</span> <span class="k">global</span> <span class="nb">int</span> <span class="o">*</span><span class="n">B</span><span class="p">,</span> <span class="k">global</span> <span class="nb">int</span> <span class="o">*</span><span class="n">C</span><span class="p">,</span> <span class="k">global</span> <span class="nb">int</span> <span class="o">*</span><span class="n">P</span><span class="p">)</span>
<span class="p">{</span>
  <span class="nb">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">get_global_id</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
  <span class="nb">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">get_global_id</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
  <span class="nb">int</span> <span class="n">N</span> <span class="o">=</span> <span class="o">*</span><span class="n">P</span><span class="p">,</span> <span class="n">k</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
    <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">N</span><span class="o">+</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
  <span class="k">else</span>
    <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">N</span><span class="o">+</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">j</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
  <span class="p">{</span>
    <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">k</span><span class="o">&lt;</span><span class="n">N</span><span class="p">;</span><span class="n">k</span><span class="o">++</span><span class="p">)</span>
      <span class="n">atomic_fetch_and_explicit</span><span class="p">((</span><span class="k">global</span> <span class="n">atomic_int</span> <span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">N</span><span class="o">+</span><span class="n">k</span><span class="p">],</span> <span class="n">memory_order_release</span><span class="p">,</span> <span class="n">memory_scope_device</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Similarly, another sample includes the following kernel that increments 2*N times, N times in the kernel and another N times on the host:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">__kernel</span> <span class="n">void</span> <span class="n">counter</span><span class="p">(</span> <span class="k">global</span> <span class="nb">int</span> <span class="o">*</span><span class="n">count</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">atomic_fetch_add</span><span class="p">((</span><span class="n">atomic</span> <span class="n">_int</span><span class="p">)</span><span class="n">count</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
  <span class="o">//</span><span class="p">(</span><span class="o">*</span><span class="n">count</span><span class="p">)</span><span class="o">++</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Note: If atomic_fetch_add is not used and instead an incrementing count (as performed in the commented line) is used, the sum will not be computed correctly.</p>
</div>
</div>
<div class="section" id="pipes">
<span id="id25"></span><h2>Pipes<a class="headerlink" href="#pipes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id26">
<h3>Overview<a class="headerlink" href="#id26" title="Permalink to this headline">¶</a></h3>
<p>OpenCL 2.0 introduces a new mechanism, pipes, for passing data between kernels. A pipe is essentially a structured buffer containing some space for a set of “packets”–kernel-specified type objects, and for bookkeeping information. As the name suggests, these packets of data are ordered in the pipe (as a FIFO).</p>
<p>Pipes are accessed via special read_pipe and write_pipe built-in functions. A given kernel may either read from or write to a pipe, but not both.  Pipes are only “coherent” at the standard synchronization points; the result of concurrent accesses to the same pipe by multiple kernels (even if permitted by hardware) is undefined. A pipe cannot be accessed from the host side; it can only be accessed by using the kernel built-in functions.</p>
<p>Pipes are created on the host with a call to <code class="docutils literal notranslate"><span class="pre">clCreatePipe</span></code>, and may be passed between kernels. Pipes may be particularly useful when combined with device- size enqueue for dynamically constructing computational data flow graphs.</p>
<p>There are two types of pipes: a read pipe, from which a number of packets can be read; and a write pipe, to which a number of packets can be written.</p>
<p>Note: A pipe specified as read-only cannot be written into and a pipe specified as write-only cannot be read from. A pipe cannot be read from and written into at the same time.</p>
</div>
<div class="section" id="functions-for-accessing-pipes">
<h3>Functions for accessing pipes<a class="headerlink" href="#functions-for-accessing-pipes" title="Permalink to this headline">¶</a></h3>
<p>A new host API function has been added into the OpenCL 2.0 spec to create the
Pipe.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cl_mem</span>        <span class="n">clCreatePipe</span> <span class="p">(</span> <span class="n">cl_context</span> <span class="n">context</span><span class="p">,</span> <span class="n">cl_mem_flags</span> <span class="n">flags</span><span class="p">,</span>  <span class="n">cl_uint</span> <span class="n">pipe_packet_size</span><span class="p">,</span> <span class="n">cl_uint</span> <span class="n">pipe_max_packets</span><span class="p">,</span>  <span class="n">const</span> <span class="n">cl_pipe_properties</span> <span class="o">*</span> <span class="n">properties</span><span class="p">,</span> <span class="n">cl_int</span> <span class="o">*</span><span class="n">errcode_ret</span><span class="p">)</span>
</pre></div>
</div>
<p>The memory allocated in the above function can be passed to kernels as read- only or write-only pipes. The pipe objects can only be passed as kernel arguments or kernel functions and cannot be declared inside a kernel or as program-scoped objects.</p>
<p>Also, a set of built-in functions have been added to operate on the pipes. The important ones are:</p>
<p>read_pipe (pipe p, gentype * ptr: for reading packet from pipe p into ptr.</p>
<p>write_pipe (pipe p, gentype * ptr: for writing packet pointed to by ptr to pipe p.</p>
<p>To ensure you have enough space in the pipe structure for reading and writing (before you actually do it), you can use built-in functions to “reserve” enough space. For example, you could reserve room by calling reserve_read_pipe or reserve_write_pipe. These functions return a reservation ID, which can be used when the actual operations are performed. Similarly, the standard has built-in functions for workgroup level reservations, such as work_group_reserve_read_pipe and work_group_reserve_write_pipe and for the workgroup order (in the program). These workgroup built-in functions operate at the workgroup level. Ordering across workgroups is undefined. Calls to commit_read_pipe and commit_write_pipe, as the names suggest, commit the actual operations (read/write).</p>
</div>
<div class="section" id="id27">
<h3>Usage<a class="headerlink" href="#id27" title="Permalink to this headline">¶</a></h3>
<p>The following example code illustrates a typical usage of pipes in the example code. The code contains two kernels: producer_kernel, which writes to the pipe, and consumer_kernel, which reads from the same pipe. In the example, the producer writes a sequence of random numbers; the consumer reads them and creates a histogram.</p>
<p>The host creates the pipe, which both kernels will use, as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">rngPipe</span> <span class="o">=</span> <span class="n">clCreatePipe</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">CL_MEM_READ_WRITE</span><span class="p">,</span> <span class="n">szPipePkt</span><span class="p">,</span>
<span class="n">szPipe</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>
</pre></div>
</div>
<p>This code makes a pipe that the program kernels can access (read/write). The host creates two kernels, producer_kernel and consumer_kernel. The producer kernel first reserves enough space for the write pipe:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span><span class="n">reserve</span> <span class="n">space</span> <span class="ow">in</span> <span class="n">pipe</span> <span class="k">for</span> <span class="n">writing</span> <span class="n">random</span> <span class="n">numbers</span><span class="o">.</span>
<span class="n">reserve_id_t</span> <span class="n">rid</span> <span class="o">=</span> <span class="n">work_group_reserve_write_pipe</span><span class="p">(</span><span class="n">rng_pipe</span><span class="p">,</span> <span class="n">szgr</span><span class="p">);</span>
</pre></div>
</div>
<p>Next, the kernel writes and commits to the pipe by invoking the following functions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">write_pipe</span><span class="p">(</span><span class="n">rng_pipe</span><span class="p">,</span><span class="n">rid</span><span class="p">,</span><span class="n">lid</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">gfrn</span><span class="p">);</span> <span class="n">work_group_commit_write_pipe</span><span class="p">(</span><span class="n">rng_pipe</span><span class="p">,</span> <span class="n">rid</span><span class="p">);</span> <span class="n">Similarly</span><span class="p">,</span> <span class="n">the</span> <span class="n">consumer</span> <span class="n">kernel</span> <span class="n">reads</span> <span class="kn">from</span> <span class="nn">the</span>       <span class="n">pipe</span><span class="p">:</span>
<span class="o">//</span><span class="n">reserve</span> <span class="n">pipe</span> <span class="k">for</span> <span class="n">reading</span>
<span class="n">reserve_id_t</span> <span class="n">rid</span> <span class="o">=</span> <span class="n">work_group_reserve_read_pipe</span><span class="p">(</span><span class="n">rng_pipe</span><span class="p">,</span> <span class="n">szgr</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">is_valid_reserve_id</span><span class="p">(</span><span class="n">rid</span><span class="p">))</span> <span class="p">{</span>
<span class="o">//</span><span class="n">read</span> <span class="n">random</span> <span class="n">number</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">pipe</span><span class="o">.</span> <span class="n">read_pipe</span><span class="p">(</span><span class="n">rng_pipe</span><span class="p">,</span><span class="n">rid</span><span class="p">,</span><span class="n">lid</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rn</span><span class="p">);</span> <span class="n">work_group_commit_read_pipe</span><span class="p">(</span><span class="n">rng_pipe</span><span class="p">,</span> <span class="n">rid</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The consumer_kernel then uses this set of random number and constructs the histogram. The CPU creates the same histogram and verifies whether the histogram created by the kernel is correct. Here, lid is the local id of the work item, obtained by get_local_id(0).</p>
<p>The example code demonstrates how you can use a pipe as a convenient data structure that allows two kernels to communicate.</p>
<p>In OpenCL 1.2, this kind of communication typically involves the host – although kernels can communicate without returning control to the host. Pipes, however, ease programming by reducing the amount of code that some applications require.</p>
</div>
</div>
<div class="section" id="program-scope-global-variables">
<span id="id28"></span><h2>Program-scope global Variables<a class="headerlink" href="#program-scope-global-variables" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id29">
<h3>Overview<a class="headerlink" href="#id29" title="Permalink to this headline">¶</a></h3>
<p>OpenCL 1.2 permits the declaration of only constant address space variables at program scope.</p>
<p>OpenCL 2.0 permits the declaration of variables in the global address space at program (i.e. outside function) scope. These variables have the lifetime of the program in which they appear, and may be initialized.  The host cannot directly access program-scope variables; a kernel must be used to read/write their contents from/to a buffer created on the host.</p>
<p>Program-scope global variables can save data across kernel executions. Using program-scope variables can potentially eliminate the need to create buffers on the host and pass them into each kernel for processing. However, there is a limit to the size of such variables. The developer must ensure that the total size does not exceed the value returned by the device info query: CL_DEVICE_MAX_GLOBAL_VARIABLE_SIZE.</p>
</div>
</div>
<div class="section" id="image-enhancements">
<span id="id30"></span><h2>Image Enhancements<a class="headerlink" href="#image-enhancements" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id31">
<h3>Overview<a class="headerlink" href="#id31" title="Permalink to this headline">¶</a></h3>
<p>OpenCL 2.0 introduces significant enhancements for processing images.</p>
<p>A read_write access qualifier for images has been added. The qualifier allows reading from and writing to certain types of images (verified against clGetSupportedImageFormats by using the CL_MEM_KERNEL_READ_AND_WRITE flag) in the same kernel, but reads must be sampler-less.  An atomic_work_item_fence with the CLK_IMAGE_MEM_FENCE flag and the memory_scope_work_item memory scope is required between reads and writes to the same image to ensure that the writes are visible to subsequent reads. If multiple work-items are writing to and reading from multiple locations in an image, a call to work_group_barrier with the CLK_IMAGE_MEM_FENCE flag is required.</p>
<p>OpenCL 2.0 also allows 2D images to be created from a buffer or another 2D image and makes the ability to write to 3D images a core feature. This extends the power of image operations to more situations.</p>
<p>The function <code class="docutils literal notranslate"><span class="pre">clGetSupportedImageFormats</span></code> returns a list of the image formats supported by the OpenCL platform. The Image format has two parameters, channel order and data type. The following lists some image formats OpenCL supports:</p>
<p>Channel orders: CL_A, CL_RG, CL_RGB, CL_RGBA
Channel data type: CL_UNORM_INT8, CL_FLOAT.</p>
<p>OpenCL 2.0 provides improved image support, specially support for sRGB
images and depth images.</p>
</div>
<div class="section" id="srgb">
<h3>sRGB<a class="headerlink" href="#srgb" title="Permalink to this headline">¶</a></h3>
<p>sRGB is a standard RGB color space that is used widely on monitors, printers, digital cameras, and the Internet. Because the linear RGB value is used in most image processing algorithms, processing the images often requires converting sRGB to linear RGB.</p>
<p>OpenCL 2.0 provides a new feature for handling this conversion directly. Note that only the combination of data type CL_UNORM_INT8 and channel order CL_sRGBA is mandatory in OpenCL 2.0. The AMD implementations support this combination. The remaining combinations are optional in OpenCL 2.0.</p>
<p>When not using the mandatory combination (CL_sRGBA, CL_UNORM_INT8), the clGetSupportedImageFormats function must be used to get a list of supported image formats and data types before using the sRGB image,</p>
<p>Creating sRGB image objects is similar to creating an image object of existing supported channel order with OpenCL 2.0. The following snippet shows how to create CL_sRGBA image objects by using the read_image call.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cl_image_format</span> <span class="n">imageFormat</span><span class="p">;</span>
<span class="n">imageFormat</span><span class="o">.</span><span class="n">image_channel_data_type</span> <span class="o">=</span> <span class="n">CL_UNORM_INT8</span><span class="p">;</span>
<span class="n">imageFormat</span><span class="o">.</span><span class="n">image_channel_order</span> <span class="o">=</span> <span class="n">CL_sRGBA</span>
<span class="n">cl_mem</span> <span class="n">imageObj</span> <span class="o">=</span> <span class="n">clCreateImage</span><span class="p">(</span>
<span class="n">context</span><span class="p">,</span>      <span class="o">//</span> <span class="n">A</span> <span class="n">valid</span> <span class="n">OpenCL</span> <span class="n">context</span>
<span class="n">CL_MEM_READ_ONY</span> <span class="o">|</span> <span class="n">CL_MEM_COPY_HOST_PTR</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">imageFormat</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">desc</span><span class="p">,</span> <span class="o">//</span><span class="n">cl_image_desc</span>
<span class="n">pSrcImage</span><span class="p">,</span>    <span class="o">//</span> <span class="n">An</span> <span class="n">pointer</span> <span class="n">to</span> <span class="n">the</span> <span class="n">image</span> <span class="n">data</span>
<span class="o">&amp;</span><span class="n">retErr</span><span class="p">);</span>     <span class="o">//</span> <span class="n">Returned</span> <span class="n">error</span> <span class="n">code</span>
</pre></div>
</div>
<p>A new sRGB image can also be created based on an existing RGB image object, so that the kernel can implicitly convert the sRGB image data to RGB. This is useful when the viewing pixels are sRGB but share the same data as the existing RGB image.</p>
<p>After an sRGB image object has been created, the read_imagef call can be used in the kernel to read it transparently. read_imagef implicitly converts sRGB values into linear RGB. Converting sRGB into RGB in the kernel explicitly is not necessary if the device supports OpenCL 2.0. Note that only read_imagef can be used for reading sRGB image data because only the CL_UNORM_INT8 data type is supported with OpenCL 2.0.</p>
<p>The following is a kernel sample that illustrates how to read an sRGB image object.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">Read</span> <span class="n">sRGBA</span> <span class="n">image</span> <span class="nb">object</span> <span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">and</span> <span class="n">convert</span> <span class="n">it</span> <span class="n">to</span> <span class="n">linear</span> <span class="n">RGB</span>
<span class="n">values</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="n">kernel</span> <span class="n">void</span> <span class="n">sample_kernel</span><span class="p">(</span> <span class="n">read_only</span> <span class="n">image2d_t</span> <span class="nb">input</span><span class="p">,</span> <span class="n">sampler_t</span> <span class="n">imageSampler</span><span class="p">,</span>   <span class="k">global</span> <span class="nb">float</span> <span class="o">*</span><span class="n">xOffsets</span><span class="p">,</span>  <span class="k">global</span> <span class="nb">float</span> <span class="o">*</span><span class="n">yOffsets</span><span class="p">,</span>
<span class="k">global</span> <span class="n">float4</span> <span class="o">*</span><span class="n">results</span>        <span class="p">)</span>       <span class="o">//</span> <span class="nb">input</span><span class="p">:</span> <span class="n">sRGBA</span> <span class="n">image</span> <span class="nb">object</span>
<span class="p">{</span>
<span class="nb">int</span> <span class="n">tidX</span> <span class="o">=</span> <span class="n">get_global_id</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">tidY</span> <span class="o">=</span> <span class="n">get_global_id</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="nb">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">tidY</span><span class="o">*</span><span class="n">get_image_width</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">+</span> <span class="n">tidX</span><span class="p">;</span>
<span class="n">int2</span> <span class="n">coords</span> <span class="o">=</span> <span class="p">(</span><span class="n">int2</span><span class="p">)(</span> <span class="n">xOffsets</span><span class="p">[</span><span class="n">offset</span><span class="p">],</span> <span class="n">yOffsets</span><span class="p">[</span><span class="n">offset</span><span class="p">]);</span>
<span class="n">results</span><span class="p">[</span><span class="n">offset</span><span class="p">]</span> <span class="o">=</span> <span class="n">read_imagef</span><span class="p">(</span> <span class="nb">input</span><span class="p">,</span> <span class="n">imageSampler</span><span class="p">,</span> <span class="n">coords</span>    <span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>OpenCL 2.0 does not include writing sRGB images directly, but provides the cl_khr_srgb_image_writes extension. The AMD implementations do not support this extension as of this writing.</p>
<p>In order to write sRGB pixels in a kernel, explicit conversion from linear RGB to sRGB must be implemented in the kernel.</p>
<p>clFillImage is an exception for writing sRGB image directly. The AMD OpenCL platform supports clFillImage for filling linear RGB image to sRGB image directly.</p>
</div>
<div class="section" id="depth-images">
<h3>Depth images<a class="headerlink" href="#depth-images" title="Permalink to this headline">¶</a></h3>
<p>As with other image formats, clCreateImage is used for creating depth image objects. However, the channel order must be set to CL_DEPTH, as illustrated below. For the data type of depth image, OpenCL 2.0 supports only CL_FLOAT and CL_UNORM_INT16.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cl_image_format</span> <span class="n">imageFormat</span><span class="p">;</span>
<span class="n">imageFormat</span><span class="o">.</span><span class="n">image_channel_data_type</span> <span class="o">=</span> <span class="n">CL_UNORM_INT16</span><span class="p">;</span>
<span class="n">imageFormat</span><span class="o">.</span><span class="n">image_channel_order</span> <span class="o">=</span> <span class="n">CL_DEPTH</span>
<span class="n">cl_mem</span> <span class="n">imageObj</span> <span class="o">=</span> <span class="n">clCreateImage</span><span class="p">(</span>
<span class="n">valid</span> <span class="n">OpenCL</span> <span class="n">contextcontext</span><span class="p">,</span>  <span class="o">//</span> <span class="n">A</span>
<span class="n">CL_MEM_READ_ONLY</span> <span class="o">|</span> <span class="n">CL_MEM_COPY_HOST_PTR</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">imageFormat</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">desc</span><span class="p">,</span> <span class="o">//</span><span class="n">cl_image_desc</span>
<span class="n">pSrcImage</span><span class="p">,</span> <span class="o">//</span> <span class="n">A</span> <span class="n">pointer</span> <span class="n">to</span> <span class="n">the</span> <span class="n">image</span> <span class="n">data</span>
<span class="o">&amp;</span><span class="n">retErr</span><span class="p">);</span> <span class="o">//</span> <span class="n">Returned</span> <span class="n">error</span> <span class="n">code</span>
</pre></div>
</div>
<p>In OpenCL 2.0, depth images must be of type image2d or image2d array.
clCreateImage will fail for other dimensions when creating depth image.</p>
<p>A depth image object can be read by using the read_imagef call in the kernel. For write, write_imagef must be used. read_image(i|ui) and write_image(i|ui) are not supported for depth images.</p>
<p>OpenCL 2.0 C introduces two data types, image2d_depth_t and image2d_array_depth_t for declaring depth images. The following kernel code sample illustrates how to read depth image objects.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">Read</span> <span class="n">depth</span> <span class="n">image</span> <span class="nb">object</span> <span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="n">based</span> <span class="n">on</span> <span class="n">sampler</span> <span class="ow">and</span> <span class="n">offset</span> <span class="ow">and</span> <span class="n">save</span> <span class="n">it</span> <span class="p">(</span><span class="n">results</span><span class="p">)</span>
 <span class="n">kernel</span> <span class="n">void</span> <span class="n">sample_kernel</span><span class="p">(</span> <span class="n">read_only</span> <span class="n">image2d_depth_t</span> <span class="nb">input</span><span class="p">,</span> <span class="n">sampler_t</span> <span class="n">imageSampler</span><span class="p">,</span>  <span class="k">global</span> <span class="nb">float</span> <span class="o">*</span><span class="n">xOffsets</span><span class="p">,</span>  <span class="k">global</span> <span class="nb">float</span>
<span class="o">*</span><span class="n">yOffsets</span><span class="p">,</span>  <span class="k">global</span> <span class="nb">float</span> <span class="o">*</span><span class="n">results</span>     <span class="p">)</span>
<span class="p">{</span>
<span class="nb">int</span> <span class="n">tidX</span> <span class="o">=</span> <span class="n">get_global_id</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">tidY</span> <span class="o">=</span> <span class="n">get_global_id</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="nb">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">tidY</span><span class="o">*</span><span class="n">get_image_width</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">+</span> <span class="n">tidX</span><span class="p">;</span>
<span class="n">int2</span> <span class="n">coords</span> <span class="o">=</span> <span class="p">(</span><span class="n">int2</span><span class="p">)(</span> <span class="n">xOffsets</span><span class="p">[</span><span class="n">offset</span><span class="p">],</span> <span class="n">yOffsets</span><span class="p">[</span><span class="n">offset</span><span class="p">]);</span>
<span class="n">results</span><span class="p">[</span><span class="n">offset</span><span class="p">]</span> <span class="o">=</span> <span class="n">read_imagef</span><span class="p">(</span> <span class="nb">input</span><span class="p">,</span> <span class="n">imageSampler</span><span class="p">,</span> <span class="n">coords</span>    <span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The AMD OpenCL 2.0 platform fully supports the cl_khr_depth_images extension but not the cl_khr_gl_depth_images extension. Consequently, the AMD OpenCL platform does not support creating a CL depth image from a GL depth or depth-stencil texture.</p>
</div>
</div>
<div class="section" id="non-uniform-work-group-size">
<span id="id32"></span><h2>Non-uniform work group size<a class="headerlink" href="#non-uniform-work-group-size" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id33">
<h3>Overview<a class="headerlink" href="#id33" title="Permalink to this headline">¶</a></h3>
<p>Prior to OpenCL 2.0, each work-group size needed to divide evenly into the corresponding global size. This requirement is relaxed in OpenCL 2.0; the last work-group in each dimension is allowed to be smaller than all of the other work- groups in the “uniform” part of the NDRange. This can reduce the effort required to map problems onto NDRanges.</p>
<p>A consequence is that kernels may no longer assume that calls to get_work_group_size return the same value in all work-groups.  However, a new call (get_enqueued_local_size) has been added to obtain the size in the uniform part, which is specified using the local_work_size argument to the clEnqueueNDRangeKernel.</p>
<p>A new compile time option (-cl-uniform-work-group-size) has been added to optimize the computation for cases in which the work-group size is known to, or required to, divide evenly into the global size.</p>
</div>
</div>
<div class="section" id="portability-considerations">
<span id="id34"></span><h2>Portability considerations<a class="headerlink" href="#portability-considerations" title="Permalink to this headline">¶</a></h2>
<div class="section" id="migrating-from-opencl-1-2-to-opencl-2-0">
<h3>Migrating from OpenCL 1.2 to OpenCL 2.0<a class="headerlink" href="#migrating-from-opencl-1-2-to-opencl-2-0" title="Permalink to this headline">¶</a></h3>
<p>OpenCL 2.0 is backward compatible with OpenCL 1.2. Applications written on OpenCL 1.2 should run on OpenCL 2.0 without requiring any changes to the application.</p>
<p>OpenCL 2.0 includes changes in the runtime and the compiler. In the runtime, some new functions (such as for SVM) have been added. In the compiler, the - cl-std=CL2.0 option is needed in order to compile OpenCL 2.0 kernels.</p>
<p>If a program uses the OpenCL 2.0 functions and if one compiles a kernel by using the cl-std=CL2.0 option, the program will not build or compile on OpenCL 1.2 platforms. If a program uses only OpenCL 1.2 functions and if one</p>
<p>compiles a kernel without the cl-std=CL2.0 option, then the program should run on OpenCL 2.0 platforms.</p>
</div>
<div class="section" id="identifying-implementation-specifics">
<h3>Identifying implementation specifics<a class="headerlink" href="#identifying-implementation-specifics" title="Permalink to this headline">¶</a></h3>
<p>Applications can query for the OpenCL extensions and use the values returned from the OpenCL functions.</p>
<p>For instance, clGetSupportedImageFormats will return all image formats supported by OpenCL. The supported images may differ across implementations. Similarly, clGetDeviceInfo with the CL_DEVICE_EXTENSIONS parameter returns all the supported extensions. The supported extensions may differ across implementations and between different versions of OpenCL.</p>
</div>
</div>
</div>
<div class="section" id="opencl-optional-extensions">
<span id="opencl-extentions"></span><h1>OpenCL Optional Extensions<a class="headerlink" href="#opencl-optional-extensions" title="Permalink to this headline">¶</a></h1>
<p>The OpenCL extensions are associated with the devices and can be queried for a specific device. Extensions can be queried for platforms also, but that means that all devices in the platform support those extensions.</p>
<div class="section" id="extension-name-convention">
<h2>Extension Name Convention<a class="headerlink" href="#extension-name-convention" title="Permalink to this headline">¶</a></h2>
<p>The name of extension is standardized and must contain the following elements without spaces in the name (in lower case):</p>
<blockquote>
<div><ul class="simple">
<li>cl_khr_&lt;extension_name&gt; - for extensions approved by Khronos Group. For example: <code class="docutils literal notranslate"><span class="pre">cl_khr_fp64</span></code></li>
<li>cl_ext_&lt;extension_name&gt; - for extensions provided collectively by multiple vendors. For example: <code class="docutils literal notranslate"><span class="pre">cl_ext_device_fission</span></code></li>
<li>cl_&lt;vendor_name&gt;_&lt;extension_name&gt; – for extension provided by a specific vendor. For example: <code class="docutils literal notranslate"><span class="pre">cl_amd_media_ops</span></code></li>
</ul>
</div></blockquote>
<p>The OpenCL Specification states that all API functions of the extension must have names in the form of cl&lt;FunctionName&gt;KHR, cl&lt;FunctionName&gt;EXT, or cl&lt;FunctionName&gt;&lt;VendorName&gt;. All enumerated values must be in the form of CL_&lt;enum_name&gt;_KHR, CL_&lt;enum_name&gt;_EXT, or CL_&lt;enum_name&gt;_&lt;VendorName&gt;.</p>
<div class="section" id="querying-extensions-for-a-platform">
<h3>Querying Extensions for a Platform<a class="headerlink" href="#querying-extensions-for-a-platform" title="Permalink to this headline">¶</a></h3>
<p>To query supported extensions for the OpenCL platform, use the clGetPlatformInfo() function, with the param_name parameter set to the enumerated value CL_PLATFORM_EXTENSIONS. This returns the extensions as a character string with extension names separated by spaces. To find out if a specific extension is supported by this platform, search the returned string for the required substring.</p>
</div>
<div class="section" id="querying-extensions-for-a-device">
<h3>Querying Extensions for a Device<a class="headerlink" href="#querying-extensions-for-a-device" title="Permalink to this headline">¶</a></h3>
<p>To get the list of devices to be queried for supported extensions, use one of the following:</p>
<ul class="simple">
<li>Query for available platforms using clGetPlatformIDs(). Select one, and query for a list of available devices with clGetDeviceIDs().</li>
<li>For a specific device type, call clCreateContextFromType(), and query a list of devices by calling clGetContextInfo() with the param_name parameter set to the enumerated value CL_CONTEXT_DEVICES.</li>
</ul>
<p>After the device list is retrieved, the extensions supported by each device can be queried with function call clGetDeviceInfo() with parameter param_name being set to enumerated value CL_DEVICE_EXTENSIONS.</p>
<p>The extensions are returned in a char string, with extension names separated by a space. To see if an extension is present, search the string for a specified substring.</p>
</div>
<div class="section" id="using-extensions-in-kernel-programs">
<h3>Using Extensions in Kernel Programs<a class="headerlink" href="#using-extensions-in-kernel-programs" title="Permalink to this headline">¶</a></h3>
<p>There are special directives for the OpenCL compiler to enable or disable available extensions supported by the OpenCL implementation, and, specifically, by the OpenCL compiler. The directive is defined as follows.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#pragma OPENCL EXTENSION &lt;extention_name&gt; : &lt;behavior&gt;</span>
<span class="c1">#pragma OPENCL EXTENSION all: &lt;behavior&gt;</span>
</pre></div>
</div>
<p>The &lt;extension_name&gt; is described in Section A.1, “Extension Name
Convention.”. The second form allows to address all extensions at once. The &lt;behavior&gt; token can be either:</p>
<ul class="simple">
<li><strong>enable</strong> - the extension is enabled if it is supported, or the error is reported if the specified extension is not supported or token “all” is used.</li>
<li><strong>disable</strong> - the OpenCL implementation/compiler behaves as if the specified extension does not exist.</li>
<li><strong>all</strong> - only core functionality of OpenCL is used and supported, all extensions are ignored. If the specified extension is not supported then a warning is issued by the compiler.</li>
</ul>
<p>The order of directives in #pragma OPENCL EXTENSION is important: a later directive with the same extension name overrides any previous one.</p>
<p>The initial state of the compiler is set to ignore all extensions as if it was explicitly set with the following directive:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#pragma OPENCL EXTENSION all : disable</span>
</pre></div>
</div>
<p>This means that the extensions must be explicitly enabled to be used in kernel programs.</p>
<p>Each extension that affects kernel code compilation must add a defined macro with the name of the extension. This allows the kernel code to be compiled differently, depending on whether the extension is supported and enabled, or not. For example, for extension cl_khr_fp64 there should be a #define directive for macro cl_khr_fp64, so that the following code can be preprocessed:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#ifdef cl_khr_fp64</span>
<span class="o">//</span> <span class="n">some</span> <span class="n">code</span>
<span class="c1">#else</span>
<span class="o">//</span> <span class="n">some</span> <span class="n">code</span>
<span class="c1">#endif</span>
</pre></div>
</div>
</div>
<div class="section" id="getting-extension-function-pointers">
<h3>Getting Extension Function Pointers<a class="headerlink" href="#getting-extension-function-pointers" title="Permalink to this headline">¶</a></h3>
<p>Use the following function to get an extension function pointer.</p>
<p><code class="docutils literal notranslate"><span class="pre">void*</span> <span class="pre">clGetExtensionFunctionAddress(const</span> <span class="pre">char*</span> <span class="pre">FunctionName).</span></code></p>
<p>This returns the address of the extension function specified by the FunctionName string. The returned value must be appropriately cast to a function pointer type, specified in the extension spec and header file.</p>
<p>A return value of NULL means that the specified function does not exist in the CL implementation. A non-NULL return value does not guarantee that the extension function actually exists – queries described in sec. 2 or 3 must be done to ensure the extension is supported.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">clGetExtensionFunctionAddress()</span></code> function cannot be used to get core API function addresses.</p>
</div>
<div class="section" id="list-of-supported-extensions-that-are-khronos-approved">
<h3>List of Supported Extensions that are Khronos-Approved<a class="headerlink" href="#list-of-supported-extensions-that-are-khronos-approved" title="Permalink to this headline">¶</a></h3>
<p>For a complete list of the supported extensions, see the OpenCL 1.2 and
OpenCL 2.0 specification documents. The typical extensions in OpenCL 1.2 are:</p>
<ul class="simple">
<li>cl_khr_global_int32_base_atomics – basic atomic operations on 32-bit integers in global memory.</li>
<li>cl_khr_global_int32_extended_atomics – extended atomic operations on 32-bit integers in global memory.</li>
<li>cl_khr_local_int32_base_atomics – basic atomic operations on 32-bit integers in local memory.</li>
<li>cl_khr_local_int32_extended_atomics – extended atomic operations on 32-bit integers in local memory.</li>
<li>cl_khr_int64_base_atomics – basic atomic operations on 64-bit integers in both global and local memory.</li>
<li>cl_khr_int64_extended_atomics – extended atomic operations on 64-bit integers in both global and local memory.</li>
<li>cl_khr_3d_image_writes – supports kernel writes to 3D images.</li>
<li>cl_khr_byte_addressable_store – this eliminates the restriction of not allowing writes to a pointer (or array elements) of types      less than 32-bit wide in kernel program.</li>
<li>cl_khr_gl_sharing – allows association of OpenGL context or share group with CL context for interoperability.</li>
<li>cl_khr_icd – the OpenCL Installable Client Driver (ICD) that lets developers select from multiple OpenCL runtimes which may be        installed on a system.</li>
<li>cl_khr_d3d10_sharing - allows association of D3D10 context or share group with CL context for interoperability.</li>
<li>cl_dx9_media_sharing</li>
<li>Cl_khr_fp16</li>
<li>cl_khr_gl_event</li>
</ul>
<p>The typical extensions in OpenCL 2.0 are:</p>
<blockquote>
<div><ul class="simple">
<li>cl_khr_int64_base_atomics</li>
<li>cl_khr_int64_extended_atomics</li>
<li>cl_khr_fp16</li>
<li>cl_khr_gl_sharing</li>
<li>cl_khr_gl_event</li>
<li>cl_khr_d3d10_sharing</li>
<li>cl_dx9_media_sharing</li>
<li>cl_khr_d3d11_sharing</li>
<li>cl_khr_gl_depth_images</li>
<li>cl_khr_gl_msaa_sharing</li>
<li>cl_khr_initialize_memory</li>
<li>cl_khr_terminate_context</li>
<li>cl_khr_spir</li>
<li>cl_khr_icd</li>
<li>cl_khr_subgroups</li>
<li>cl_khr_mipmap_image</li>
<li>cl_khr_mipmap_image_writes</li>
<li>cl_khr_egl_image</li>
<li>cl_khr_egl_event</li>
<li>cl_khr_device_enqueue_local_arg_types</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="cl-ext-extensions">
<h3>cl_ext Extensions<a class="headerlink" href="#cl-ext-extensions" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>cl_ext_device_fission - Support for device fission in OpenCL™. For more information about this extension, see: <a class="reference external" href="http://www.khronos.org/registry/cl/extensions/ext/cl_ext_device_fission.txt">http://www.khronos.org/registry/cl/extensions/ext/cl_ext_device_fission.txt</a></li>
<li>cl_ext_atomic_counters_32 - Support for 32-bit atomic counters. For more information about this extension, see: <a class="reference external" href="https://www.khronos.org/registry/cl/extensions/ext/cl_ext_atomic_counters_32.txt">https://www.khronos.org/registry/cl/extensions/ext/cl_ext_atomic_counters_32.txt</a></li>
</ul>
</div>
</div>
<div class="section" id="amd-vendor-specific-extensions">
<h2>AMD Vendor-Specific Extensions<a class="headerlink" href="#amd-vendor-specific-extensions" title="Permalink to this headline">¶</a></h2>
<p>This section describes the AMD vendor-specific extensions.</p>
<div class="section" id="cl-amd-fp64">
<h3>cl_amd_fp64<a class="headerlink" href="#cl-amd-fp64" title="Permalink to this headline">¶</a></h3>
<p>Before using double data types, double-precision floating point operators, and/or double-precision floating point routines in OpenCL™ C kernels, include the
#pragma OPENCL EXTENSION cl_amd_fp64 : enable directive. See Table A.1 for a list of supported routines.</p>
</div>
<div class="section" id="cl-amd-vec3">
<h3>cl_amd_vec3<a class="headerlink" href="#cl-amd-vec3" title="Permalink to this headline">¶</a></h3>
<p>This extension adds support for vectors with three elements: float3, short3, char3, etc. This data type was added to OpenCL 1.1 as a core feature. For more details, see section 6.1.2 in the OpenCL 1.1 or OpenCL 1.2 spec.</p>
</div>
<div class="section" id="cl-amd-device-persistent-memory">
<h3>cl_amd_device_persistent_memory<a class="headerlink" href="#cl-amd-device-persistent-memory" title="Permalink to this headline">¶</a></h3>
<p>This extension adds support for the new buffer and image creation flag CL_MEM_USE_PERSISTENT_MEM_AMD. Buffers and images allocated with this flag reside in host-visible device memory. This flag is mutually exclusive with the flags CL_MEM_ALLOC_HOST_PTR and CL_MEM_USE_HOST_PTR.</p>
</div>
<div class="section" id="cl-amd-device-attribute-query">
<h3>cl_amd_device_attribute_query<a class="headerlink" href="#cl-amd-device-attribute-query" title="Permalink to this headline">¶</a></h3>
<p>This extension provides a means to query AMD-specific device attributes. To enable this extension, include the #pragma OPENCL EXTENSION cl_amd_device_attribute_query : enable directive. Once the extension is enabled, and the clGetDeviceInfo parameter &lt;param_name&gt; is set to CL_DEVICE_PROFILING_TIMER_OFFSET_AMD, the offset in nano-seconds between an event timestamp and Epoch is returned.</p>
</div>
<div class="section" id="cl-device-profiling-timer-offset-amd">
<h3>cl_device_profiling_timer_offset_amd<a class="headerlink" href="#cl-device-profiling-timer-offset-amd" title="Permalink to this headline">¶</a></h3>
<p>This query enables the developer to get the offset between event timestamps in nano-seconds. To use it, compile the kernels with the #pragma OPENCL EXTENSION cl_amd_device_attribute_query : enable directive. For</p>
<p>kernels complied with this pragma, calling clGetDeviceInfo with &lt;param_name&gt; set to CL_DEVICE_PROFILING_TIMER_OFFSET_AMD returns the offset in nano- seconds between event timestamps.</p>
</div>
<div class="section" id="cl-amd-device-topology">
<h3>cl_amd_device_topology<a class="headerlink" href="#cl-amd-device-topology" title="Permalink to this headline">¶</a></h3>
<p>This query enables the developer to get a description of the topology used to connect the device to the host. Currently, this query works only in Linux. Calling clGetDeviceInfo with &lt;param_name&gt; set to CL_DEVICE_TOPOLOGY_AMD returns the following 32-bytes union of structures.</p>
<p>typedef union
{
struct { cl_uint type; cl_uint data[5]; } raw;
struct { cl_uint type; cl_char unused[17]; cl_char bus; cl_char
device; cl_char function; } pcie; } cl_device_topology_amd;</p>
<p>The type of the structure returned can be queried by reading the first unsigned int of the returned data. The developer can use this type to cast the returned union into the right structure type.</p>
<p>Currently, the only supported type in the structure above is PCIe (type value =
1). The information returned contains the PCI Bus/Device/Function of the device, and is similar to the result of the lspci command in Linux. It enables the developer to match between the OpenCL device ID and the physical PCI connection of the card.</p>
</div>
<div class="section" id="cl-amd-device-board-name">
<h3>cl_amd_device_board_name<a class="headerlink" href="#cl-amd-device-board-name" title="Permalink to this headline">¶</a></h3>
<p>This query enables the developer to get the name of the GPU board and model of the specific device. Currently, this is only for GPU devices.</p>
<p>Calling clGetDeviceInfo with &lt;param_name&gt; set to
CL_DEVICE_BOARD_NAME_AMD returns a 128-character value.</p>
</div>
<div class="section" id="cl-amd-compile-options">
<h3>cl_amd_compile_options<a class="headerlink" href="#cl-amd-compile-options" title="Permalink to this headline">¶</a></h3>
<p>This extension adds the following options, which are not part of the OpenCL specification.</p>
<ul class="simple">
<li>-g — This is an experimental feature that lets you use the GNU project debugger, GDB, to debug kernels on x86 CPUs running Linux or cygwin/minGW under Windows. For more details, see Chapter 4, “Debugging and Profiling OpenCL.” This option does not affect the default optimization of the OpenCL code.</li>
<li>-O0 — Specifies to the compiler not to optimize. This is equivalent to the
OpenCL standard option -cl-opt-disable.</li>
<li>-f[no-]bin-source — Does [not] generate OpenCL source in the .source section. For more information, see Appendix C, “OpenCL Binary Image Format (BIF) v2.0.” By default, the source is NOT generated.</li>
<li>-f[no-]bin-llvmir — Does [not] generate LLVM IR in the .llvmir section.
For more information, see Appendix C, “OpenCL Binary Image Format (BIF)
v2.0.” By default, LLVM IR IS generated.</li>
<li>-f[no-]bin-amdil — Does [not] generate AMD IL in the .amdil section. For more information, see Appendix C, “OpenCL Binary Image Format (BIF) v2.0.” By Default, AMD IL is NOT generated.</li>
<li>-f[no-]bin-exe — Does [not] generate the executable (ISA) in .text section.
For more information, see Appendix C, “OpenCL Binary Image Format (BIF)
v2.0.” By default, the executable IS generated.</li>
<li>-f[no-]bin-hsail Does [not] generate HSAIL/BRIG in the binary. By default, HSA IL/BRIG is NOT generated.</li>
</ul>
<p>To avoid source changes,  there are two environment variables that can be used to change CL options during the runtime.</p>
<ul class="simple">
<li>AMD_OCL_BUILD_OPTIONS — Overrides the CL options specified in clBuildProgram().</li>
<li>AMD_OCL_BUILD_OPTIONS_APPEND — Appends options to the options specified in clBuildProgram().</li>
</ul>
</div>
<div class="section" id="cl-amd-offline-devices">
<h3>cl_amd_offline_devices<a class="headerlink" href="#cl-amd-offline-devices" title="Permalink to this headline">¶</a></h3>
<p>To generate binary images offline, it is necessary to access the compiler for every device that the runtime supports, even if the device is currently not installed on the system. When, during context creation, CL_CONTEXT_OFFLINE_DEVICES_AMD is passed in the context properties, all supported devices, whether online or offline, are reported and can be used to create OpenCL binary images.</p>
</div>
<div class="section" id="cl-amd-event-callback">
<h3>cl_amd_event_callback<a class="headerlink" href="#cl-amd-event-callback" title="Permalink to this headline">¶</a></h3>
<p>This extension provides the ability to register event callbacks for states other than cl_complete. The full set of event states are allowed: cl_queued, cl_submitted, and cl_running. This extension is enabled automatically and does not need to be explicitly enabled through #pragma when using the AMD Compute SDK.</p>
</div>
<div class="section" id="cl-amd-popcnt">
<h3>cl_amd_popcnt<a class="headerlink" href="#cl-amd-popcnt" title="Permalink to this headline">¶</a></h3>
<p>This extension introduces a “population count” function called popcnt. This extension was taken into core OpenCL 1.2, and the function was renamed popcount. The core 1.2 popcount function (documented in section 6.12.3 of the OpenCL Specification) is identical to the AMD extension popcnt function.</p>
</div>
<div class="section" id="cl-amd-media-ops">
<h3>cl_amd_media_ops<a class="headerlink" href="#cl-amd-media-ops" title="Permalink to this headline">¶</a></h3>
<p>This extension adds the following built-in functions to the OpenCL language. Note: For OpenCL scalar types, n = 1; for vector types, it is {2, 4, 8, or 16}.</p>
<p>For more information, see: <a class="reference external" href="http://www.khronos.org/registry/cl/extensions/amd/cl_amd_media_ops.txt">http://www.khronos.org/registry/cl/extensions/amd/cl_amd_media_ops.txt</a></p>
</div>
<div class="section" id="cl-amd-printf">
<h3>cl_amd_printf<a class="headerlink" href="#cl-amd-printf" title="Permalink to this headline">¶</a></h3>
<p>The OpenCL™ Specification 1.1 and 1.2 support the optional AMD extension cl_amd_printf, which provides printf capabilities to OpenCL C programs. To use this extension, an application first must include:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#pragma OPENCL EXTENSION cl_amd_printf : enable.</span>
</pre></div>
</div>
<p>Built-in function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>printf( constant char * restrict format, …);
</pre></div>
</div>
<p>This function writes output to the stdout stream associated with the host application. The format string is a character sequence that:</p>
<p>–is null-terminated and composed of zero and more directives,</p>
<p>–ordinary characters (i.e. not %), which are copied directly to the output stream unchanged, and</p>
<p>–conversion specifications, each of which can result in fetching zero or more arguments, converting them, and then writing the final result to the output stream.</p>
<p>The format string must be resolvable at compile time; thus, it cannot be dynamically created by the executing program. (Note that the use of variadic arguments in the built-in printf does not imply its use in other built- ins; more importantly, it is not valid to use printf in user-defined functions or kernels.)</p>
<p>The OpenCL C printf closely matches the definition found as part of the C99 standard. Note that conversions introduced in the format string with % are supported with the following guidelines:</p>
<ul class="simple">
<li>A 32-bit floating point argument is not converted to a 64-bit double, unless the extension cl_khr_fp64 is supported and enabled, as defined in section 9.3 of the OpenCL Specification 1.1. This includes the double variants if cl_khr_fp64 is supported and defined in the corresponding compilation unit.</li>
<li>64-bit integer types can be printed using %ld / %lx / %lu .</li>
<li>%lld / %llx / %llu are not supported and reserved for 128-bit integer types (long long).</li>
<li>All OpenCL vector types (section 6.1.2 of the OpenCL Specification 1.1) can be explicitly passed and printed using the modifier vn, where n can be 2, 3, 4, 8, or 16. This modifier appears before the original conversion specifier for the vector’s component type (for example, to print a float4 %v4f). Since vn is a conversion specifier, it is valid to apply optional flags, such as field width and precision, just as it is when printing the component types.      Since a vector is an aggregate type, the comma separator is used between the components: 0:1, … , n-2:n-1.</li>
</ul>
</div>
<div class="section" id="cl-amd-predefined-macros">
<h3>cl_amd_predefined_macros<a class="headerlink" href="#cl-amd-predefined-macros" title="Permalink to this headline">¶</a></h3>
<p>The following macros are predefined when compiling OpenCL™ C kernels. These macros are defined automatically based on the device for which the code is being compiled.</p>
<p>GPU devices:</p>
<blockquote>
<div><div class="line-block">
<div class="line">__Barts__</div>
<div class="line">__Bheem__</div>
<div class="line">__Bonaire__</div>
<div class="line">__Caicos__</div>
<div class="line">__Capeverde__</div>
<div class="line">__Carrizo__</div>
<div class="line">__Cayman__</div>
<div class="line">__Cedar__</div>
<div class="line">__Cypress__</div>
<div class="line">__Devastator__</div>
<div class="line">__Hainan__</div>
<div class="line">__Iceland__</div>
<div class="line">__Juniper__</div>
<div class="line">__Kalindi__</div>
<div class="line">__Kauai__</div>
<div class="line">__Lombok__</div>
<div class="line">__Loveland__</div>
<div class="line">__Mullins__</div>
<div class="line">__Oland__</div>
<div class="line">__Pitcairn__</div>
<div class="line">__RV710__</div>
<div class="line">__RV730__</div>
<div class="line">__RV740__</div>
<div class="line">__RV770__</div>
<div class="line">__RV790__</div>
<div class="line">__Redwood__</div>
<div class="line">__Scrapper__</div>
<div class="line">__Spectre__</div>
<div class="line">__Spooky__</div>
<div class="line">__Tahiti__</div>
<div class="line">__Tonga__</div>
<div class="line">__Turks__</div>
<div class="line">__WinterPark__</div>
<div class="line">__GPU__</div>
</div>
</div></blockquote>
<p>CPU devices:</p>
<blockquote>
<div><div class="line-block">
<div class="line">__CPU__</div>
<div class="line">__X86__</div>
<div class="line">__X86_64__</div>
</div>
</div></blockquote>
<p>Note that     GPU  or     CPU  are predefined whenever a GPU or CPU device is the compilation target.</p>
<p>An example kernel is provided below.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#pragma OPENCL EXTENSION cl_amd_printf : enable const char* getDeviceName() {</span>
<span class="c1">#ifdef   Cayman</span>
<span class="k">return</span> <span class="s2">&quot;Cayman&quot;</span><span class="p">;</span>
<span class="c1">#elif   Barts</span>
<span class="k">return</span> <span class="s2">&quot;Barts&quot;</span><span class="p">;</span>
<span class="c1">#elif   Cypress</span>
<span class="k">return</span> <span class="s2">&quot;Cypress&quot;</span><span class="p">;</span>
<span class="c1">#elif defined(  Juniper  )</span>
<span class="k">return</span> <span class="s2">&quot;Juniper&quot;</span><span class="p">;</span>
<span class="c1">#elif defined(  Redwood  )</span>
<span class="k">return</span> <span class="s2">&quot;Redwood&quot;</span><span class="p">;</span>
<span class="c1">#elif defined(  Cedar  )</span>
<span class="k">return</span> <span class="s2">&quot;Cedar&quot;</span><span class="p">;</span>
<span class="c1">#elif defined(  ATI_RV770  )</span>
<span class="k">return</span> <span class="s2">&quot;RV770&quot;</span><span class="p">;</span>
<span class="c1">#elif defined(  ATI_RV730  )</span>
<span class="k">return</span> <span class="s2">&quot;RV730&quot;</span><span class="p">;</span>
<span class="c1">#elif defined(  ATI_RV710  )</span>
<span class="k">return</span> <span class="s2">&quot;RV710&quot;</span><span class="p">;</span>
<span class="c1">#elif defined(  Loveland  )</span>
<span class="k">return</span> <span class="s2">&quot;Loveland&quot;</span><span class="p">;</span>
<span class="c1">#elif defined(  GPU  )</span>
<span class="k">return</span> <span class="s2">&quot;GenericGPU&quot;</span><span class="p">;</span>
<span class="c1">#elif defined(  X86  )</span>
<span class="k">return</span> <span class="s2">&quot;X86CPU&quot;</span><span class="p">;</span>
<span class="c1">#elif defined(  X86_64  )</span>
<span class="k">return</span> <span class="s2">&quot;X86-64CPU&quot;</span><span class="p">;</span>
<span class="c1">#elif defined(  CPU  )</span>
<span class="k">return</span> <span class="s2">&quot;GenericCPU&quot;</span><span class="p">;</span>
<span class="c1">#else</span>
<span class="c1">#endif</span>
<span class="p">}</span>
<span class="k">return</span> <span class="s2">&quot;UnknownDevice&quot;</span><span class="p">;</span>
<span class="n">kernel</span> <span class="n">void</span> <span class="n">test_pf</span><span class="p">(</span><span class="k">global</span> <span class="nb">int</span><span class="o">*</span> <span class="n">a</span><span class="p">)</span>
<span class="p">{</span>
<span class="n">printf</span><span class="p">(</span><span class="s2">&quot;Device Name: </span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">getDeviceName</span><span class="p">());</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="cl-amd-bus-addressable-memory">
<h3>cl_amd_bus_addressable_memory<a class="headerlink" href="#cl-amd-bus-addressable-memory" title="Permalink to this headline">¶</a></h3>
<p>This extension defines an API for peer-to-peer transfers between AMD GPUs and other PCIe device, such as third-party SDI I/O devices. Peer-to-peer transfers have extremely low latencies by not having to use the host’s main memory or the CPU (see Figure A.1). This extension allows sharing a memory allocated by the graphics driver to be used by other devices on the PCIe bus (peer-to-peer transfers) by exposing a write-only bus address. It also allows memory allocated on other PCIe devices (non-AMD GPU) to be directly accessed by AMD GPUs. One possible use of this is for a video capture device to directly write into the GPU memory using its DMA.This extension is supported only on AMD FirePro™ professional graphics cards.</p>
<img alt="Programming_Guides/images/a.1.png" class="align-center" src="Programming_Guides/images/a.1.png" />
</div>
<div class="section" id="supported-functions-for-cl-amd-fp64-cl-khr-fp64">
<h3>Supported Functions for cl_amd_fp64 / cl_khr_fp64<a class="headerlink" href="#supported-functions-for-cl-amd-fp64-cl-khr-fp64" title="Permalink to this headline">¶</a></h3>
<p>AMD OpenCL is now cl_khr_fp64-compliant on devices compliant with OpenCL 1.1 and greater. Thus, cl_amd_fp64 is now a synonym for cl_khr_fp64 on all supported devices.</p>
</div>
<div class="section" id="extension-support-by-device">
<h3>Extension Support by Device<a class="headerlink" href="#extension-support-by-device" title="Permalink to this headline">¶</a></h3>
<p>Table A.1 and Table A.2 list the extension support for selected devices.</p>
<table border="1" class="docutils">
<colgroup>
<col width="27%" />
<col width="6%" />
<col width="8%" />
<col width="9%" />
<col width="10%" />
<col width="9%" />
<col width="10%" />
<col width="8%" />
<col width="13%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Extensions</th>
<th class="head">Brazos</th>
<th class="head">Llano</th>
<th class="head">Trinity</th>
<th class="head">Cape Verde3</th>
<th class="head">Turks4</th>
<th class="head">Cayman5</th>
<th class="head">Barts6</th>
<th class="head">Cypress7</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>cl_khr_*_atomics (32-bit)</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>cl_ext_atomic_counters_32</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-even"><td>cl_khr_gl_sharing</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>cl_khr_byte_addressable_store</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-even"><td>cl_ext_device_fission</td>
<td>onlyCPU</td>
<td>only CPU</td>
<td>onlyCPU</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="row-odd"><td>cl_amd_device_attribute_query</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-even"><td>cl_khr_fp64</td>
<td>onlyCPU</td>
<td>only CPU</td>
<td>onlyCPU</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>cl_amd_fp64</td>
<td>onlyCPU</td>
<td>only CPU</td>
<td>onlyCPU</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="row-even"><td>cl_amd_vec3</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>cl_khr_d3d10_sharing</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-even"><td>cl_amd_media_ops</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>cl_amd_printf</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-even"><td>cl_amd_popcnt</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>cl_khr_3d_image_writes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<p><strong>Table A.1 Extension Support for AMD GPU Devices 1</strong></p>
<ol class="arabic simple">
<li>AMD Radeon™ HD 79XX series.</li>
<li>AMD Radeon™ HD 78XX series.</li>
<li>AMD Radeon™ HD 77XX series.</li>
<li>AMD Radeon™ HD 75XX series and AMD Radeon™ HD 76XX series.</li>
<li>AMD Radeon™ HD 69XX series.</li>
<li>AMD Radeon™ HD 68XX series.</li>
<li>ATI Radeon™ HD 59XX series and 58XX series, AMD FirePro™ V88XX series and V87XX series.</li>
</ol>
<p>Note that an atomic counter is a device-level counter that can be added / decremented by different work-items, where the atomicity of the operation is guaranteed. The access to the counter is done only through add/dec built-in functions; thus, no two work-items have the same value returned in the case that a given kernel only increments or decrements the counter. (Also see <a class="reference external" href="http://www.khronos.org/registry/cl/extensions/ext/cl_ext_atomic_counters_32.txt">http://www.khronos.org/registry/cl/extensions/ext/cl_ext_atomic_counters_32.txt</a>.)</p>
<table border="1" class="docutils">
<colgroup>
<col width="35%" />
<col width="13%" />
<col width="12%" />
<col width="11%" />
<col width="31%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Extension</td>
<td>Juniper 1</td>
<td>Redwood 2</td>
<td>Cedar 3</td>
<td>x86 CPU with SSE2 or later</td>
</tr>
<tr class="row-even"><td>cl_khr_*_atomics</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>cl_ext_atomic_counters_32</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr class="row-even"><td>cl_khr_gl_sharing</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>cl_khr_byte_addressable_store</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-even"><td>cl_ext_device_fission</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>cl_amd_device_attribute_query</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-even"><td>cl_khr_fp64</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>cl_amd_fp64 4</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-even"><td>cl_amd_vec3</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>Images</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-even"><td>cl_khr_d3d10_sharing</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>cl_amd_media_ops</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-even"><td>cl_amd_media_ops2</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>cl_amd_printf</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-even"><td>cl_amd_popcnt</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>cl_khr_3d_image_writes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-even"><td>Platform Extensions</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>cl_khr_icd</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-even"><td>cl_amd_event_callback</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>cl_amd_offline_devices</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
</tr>
</tbody>
</table>
<p><strong>Table A.2 Extension Support for Older AMD GPUs and CPUs</strong></p>
<ol class="arabic simple">
<li>ATI Radeon™ HD 5700 series, AMD Mobility Radeon™ HD 5800 series, AMD FirePro™ V5800 series, AMD Mobility FirePro™ M7820.</li>
<li>ATI Radeon™ HD 5600 Series, ATI Radeon™ HD 5600 Series, ATI Radeon™ HD 5500 Series, AMD Mobility Radeon™ HD 5700 Series, AMD Mobility Radeon™ HD 5600 Series, AMD FirePro™ V4800 Series, AMD FirePro™ V3800 Series, AMD Mobility FirePro™ M5800</li>
<li>ATI Radeon™ HD 5400 Series, AMD Mobility Radeon™ HD 5400 Series</li>
<li>Available on all devices that have double-precision, including all Southern Island devices.</li>
<li>Environment variable CPU_IMAGE_SUPPORT must be set.</li>
</ol>
</div>
</div>
</div>
<div class="section" id="the-opencl-installable-client-driver-icd">
<span id="icd"></span><h1>The OpenCL Installable Client Driver (ICD)<a class="headerlink" href="#the-opencl-installable-client-driver-icd" title="Permalink to this headline">¶</a></h1>
<p>The OpenCL Installable Client Driver (ICD) is installed as part of the AMD Graphics driver software stack as well as the AMD Compute SDK.</p>
<div class="section" id="id35">
<h2>Overview<a class="headerlink" href="#id35" title="Permalink to this headline">¶</a></h2>
<p>The ICD allows multiple OpenCL implementations to co-exist; also, it allows applications to select between these implementations at runtime.</p>
<p>Use the <code class="docutils literal notranslate"><span class="pre">clGetPlatformIDs()</span></code> and <code class="docutils literal notranslate"><span class="pre">clGetPlatformInfo()</span></code> functions to see the list of available OpenCL implementations, and select the one that is best for your requirements. It is recommended that developers offer their users a choice on first run of the program or whenever the list of available platforms changes.</p>
<p>A properly implemented ICD and OpenCL library is transparent to the end-user.</p>
</div>
<div class="section" id="using-icd">
<h2>Using ICD<a class="headerlink" href="#using-icd" title="Permalink to this headline">¶</a></h2>
<p>Sample code that is part of the SDK contains examples showing how to query the platform API and call the functions that require a valid platform parameter.</p>
<p>This is a pre-ICD code snippet.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">context</span> <span class="o">=</span> <span class="n">clCreateContextFromType</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span>
                                  <span class="n">dType</span><span class="p">,</span>
                                  <span class="n">NULL</span><span class="p">,</span>
                                  <span class="n">NULL</span><span class="p">,</span>
                                  <span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>
</pre></div>
</div>
<p>The ICD-compliant version of this code follows.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>/*
 * Have a look at the available platforms and pick either
 * the AMD one if available or a reasonable default.
*/

cl_uint numPlatforms;
cl_platform_id platform = NULL;
status = clGetPlatformIDs(0, NULL, &amp;numPlatforms);
if(!sampleCommon-&gt;checkVal(status, CL_SUCCESS, &quot;clGetPlatformIDs failed.&quot;))
{
  return SDK_FAILURE;
}
if (0 &lt; numPlatforms)
{
  cl_platform_id* platforms = new cl_platform_id[numPlatforms];
  status = clGetPlatformIDs(numPlatforms, platforms, NULL);
  if(!sampleCommon-&gt;checkVal(status, CL_SUCCESS, &quot;clGetPlatformIDs failed.&quot;))
  {
    return SDK_FAILURE;
  }
  for (unsigned i = 0; i &lt; numPlatforms; ++i)
  {
    char pbuf[100];
    status = clGetPlatformInfo(platforms[i], CL_PLATFORM_VENDOR, sizeof(pbuf), pbuf, NULL);

    if(!sampleCommon-&gt;checkVal(status, CL_SUCCESS, &quot;clGetPlatformInfo failed.&quot;))
    {
      return SDK_FAILURE;
    }

    platform = platforms[i];
    if (!strcmp(pbuf, &quot;Advanced Micro Devices, Inc.&quot;))
    {
      break;
    }
  }
  delete[] platforms;
}
/*
* If we could find our platform, use it. Otherwise pass a NULL and
get whatever the
* implementation thinks we should be using.
*/

cl_context_properties cps[3] =
{
  CL_CONTEXT_PLATFORM, (cl_context_properties)platform, 0
};
/* Use NULL for backward compatibility */
cl_context_properties* cprops = (NULL == platform) ? NULL : cps;

context = clCreateContextFromType(cprops, dType, NULL, NULL, &amp;status);
</pre></div>
</div>
<p>Another example of a pre-ICD code snippet follows.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">status</span> <span class="o">=</span> <span class="n">clGetDeviceIDs</span><span class="p">(</span><span class="n">NULL</span><span class="p">,</span> <span class="n">CL_DEVICE_TYPE_DEFAULT</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numDevices</span><span class="p">);</span>
</pre></div>
</div>
<p>The ICD-compliant version of the code snippet is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">status</span><span class="o">=</span> <span class="n">clGetDeviceiDs</span><span class="p">(</span><span class="n">platform</span><span class="p">,</span> <span class="n">CL_DEVICE_TYPE_DEFAULT</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">NULL</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">nurnDevices</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="opencl-binary-image-format-bif-v2-0">
<span id="bif"></span><h1>OpenCL Binary Image Format (BIF) v2.0<a class="headerlink" href="#opencl-binary-image-format-bif-v2-0" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id36">
<h2>Overview<a class="headerlink" href="#id36" title="Permalink to this headline">¶</a></h2>
<p>OpenCL Binary Image Format (BIF) 2.0 is in the ELF format. BIF2.0 allows the OpenCL binary to contain the OpenCL source program, the LLVM IR, and the executable. The BIF defines the following special sections:</p>
<blockquote>
<div><ul class="simple">
<li>.source: for storing the OpenCL source program.</li>
<li>.llvmir: for storing the OpenCL immediate representation (LLVM IR).</li>
<li>.comment: for storing the OpenCL version and the driver version that created the binary.</li>
</ul>
</div></blockquote>
<p>The BIF can have other special sections for debugging, etc. It also contains several ELF special sections, such as:</p>
<blockquote>
<div><ul class="simple">
<li>.text for storing the executable.</li>
<li>.rodata for storing the OpenCL runtime control data.</li>
<li>other ELF special sections required for forming an ELF (for example: <code class="docutils literal notranslate"><span class="pre">.strtab,</span> <span class="pre">.symtab,</span> <span class="pre">.shstrtab</span></code> ).</li>
</ul>
</div></blockquote>
<p>By default, OpenCL generates a binary that has LLVM IR, and the executable for the GPU (,.llvmir, .amdil, and .text sections), as well as LLVM IR and the executable for the CPU (.llvmir and .text sections). The BIF binary always contains a .comment section, which is a readable C string. The default behavior can be changed with the BIF options described in Section C.2, “BIF Options,” page C-3.</p>
<p>The LLVM IR enables recompilation from LLVM IR to the target. When a binary is used to run on a device for which the original program was not generated and the original device is feature-compatible with the current device, OpenCL recompiles the LLVM IR to generate a new code for the device. Note that the LLVM IR is only universal within devices that are feature-compatible in the same device type, not across different device types. This means that the LLVM IR for the CPU is not compatible with the LLVM IR for the GPU. The LLVM IR for a GPU works only for GPU devices that have equivalent feature sets.</p>
<p>BIF2.0 is supported since Stream SDK 2.2.</p>
<p>Executable and Linkable Format (ELF) Header</p>
<p>For the ELF binary to be considered valid, the AMD OpenCL runtime expects certain values to be specified. The following header fields must be set for all binaries that are created outside of the OpenCL framework.</p>
<table border="1" class="docutils">
<colgroup>
<col width="28%" />
<col width="25%" />
<col width="47%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field</th>
<th class="head">Value</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>e_ident[EI_CLASS]</td>
<td>ELFCLASS32,ELFCLASS64</td>
<td>BIF can be either 32-bit ELF or 64bit ELF.</td>
</tr>
<tr class="row-odd"><td>e_ident[EI_DATA]</td>
<td>ELFDATA2LSB</td>
<td>BIF is stored in little Endian order.</td>
</tr>
<tr class="row-even"><td>e_ident[EI_OSABI]</td>
<td>ELFOSABI_NONE</td>
<td>Not used.</td>
</tr>
<tr class="row-odd"><td>e_ident[EI_ABIVERSION]</td>
<td>0</td>
<td>Not used.</td>
</tr>
<tr class="row-even"><td>e_type</td>
<td>ET_NONE</td>
<td>Not used.</td>
</tr>
<tr class="row-odd"><td>e_machine</td>
<td>oclElfTargets</td>
<td>Enum CPU/GPU machine ID.</td>
</tr>
<tr class="row-even"><td>E_version</td>
<td>EV_CURRENT</td>
<td>Must be EV_CURRENT.</td>
</tr>
<tr class="row-odd"><td>e_entry</td>
<td>0</td>
<td>Not used.</td>
</tr>
<tr class="row-even"><td>E_phoff</td>
<td>0</td>
<td>Not used.</td>
</tr>
<tr class="row-odd"><td>e_flags</td>
<td>0</td>
<td>Not used.</td>
</tr>
<tr class="row-even"><td>E_phentsize</td>
<td>0</td>
<td>Not used.</td>
</tr>
<tr class="row-odd"><td>E_phnum</td>
<td>0</td>
<td>Not used.</td>
</tr>
</tbody>
</table>
<p><strong>Table C.1 ELF Header Fields</strong></p>
<p>The fields not shown in Table C.1 are given values according to the ELF Specification. The e_machine value is defined as one of the oclElfTargets enumerants; the values for these are:</p>
<dl class="docutils">
<dt>e_machine =    1001 + CaltargetEnum</dt>
<dd><div class="first last line-block">
<div class="line">2002</div>
<div class="line">2003</div>
</div>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">typedef</span> <span class="n">enum</span> <span class="n">CALtargetEnum</span> <span class="p">{</span>
<span class="n">CAL_TARGET_600</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">R600</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_610</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">RV610</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_630</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">RV630</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_670</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">RV670</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_7XX</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">R700</span> <span class="k">class</span> <span class="nc">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_770</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">RV770</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_710</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">RV710</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_730</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">RV730</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_CYPRESS</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">CYPRESS</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_JUNIPER</span> <span class="o">=</span> <span class="mi">9</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">JUNIPER</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_REDWOOD</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">REDWOOD</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_CEDAR</span><span class="o">=</span> <span class="mi">11</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">CEDAR</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_SUMO</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">SUMO</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_SUPERSUMO</span> <span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">SUPERSUMO</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_WRESTLER</span> <span class="o">=</span> <span class="mi">14</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">WRESTLER</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_CAYMAN</span> <span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">CAYMAN</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_KAUAI</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">KAUAI</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_BARTS</span> <span class="o">=</span> <span class="mi">17</span> <span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">BARTS</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_TURKS</span> <span class="o">=</span> <span class="mi">18</span> <span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">TURKS</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_CAICOS</span> <span class="o">=</span> <span class="mi">19</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">CAICOS</span> <span class="n">GPU</span> <span class="n">ISA</span> <span class="o">*/</span>
<span class="n">CAL_TARGET_TAHITI</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span><span class="o">/**&lt;</span> <span class="n">TAHITI</span> <span class="n">GPU</span> <span class="n">ISA</span><span class="o">*/</span>
<span class="n">CAL_TARGET_PITCAIRN</span> <span class="o">=</span> <span class="mi">21</span><span class="p">,</span><span class="o">/**&lt;</span> <span class="n">PITCAIRN</span> <span class="n">GPU</span> <span class="n">ISA</span><span class="o">*/</span>
<span class="n">CAL_TARGET_CAPEVERDE</span> <span class="o">=</span> <span class="mi">22</span><span class="p">,</span><span class="o">/**&lt;</span> <span class="n">CAPE</span> <span class="n">VERDE</span> <span class="n">GPU</span> <span class="n">ISA</span><span class="o">*/</span>
<span class="n">CAL_TARGET_DEVASTATOR</span> <span class="o">=</span> <span class="mi">23</span><span class="p">,</span><span class="o">/**&lt;</span> <span class="n">DEVASTATOR</span> <span class="n">GPU</span> <span class="n">ISA</span><span class="o">*/</span>
<span class="n">CAL_TARGET_SCRAPPER</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">SCRAPPER</span> <span class="n">GPU</span> <span class="n">ISA</span><span class="o">*/</span>
<span class="n">CAL_TARGET_OLAND</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">OLAND</span> <span class="n">GPU</span> <span class="n">ISA</span><span class="o">*/</span>
<span class="n">CAL_TARGET_BONAIRE</span> <span class="o">=</span> <span class="mi">26</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">BONAIRE</span> <span class="n">GPU</span> <span class="n">ISA</span><span class="o">*/</span>
<span class="n">CAL_TARGET_KALINDI</span> <span class="o">=</span> <span class="mi">29</span><span class="p">,</span> <span class="o">/**&lt;</span> <span class="n">KALINDI</span> <span class="n">GPU</span> <span class="n">ISA</span><span class="o">*/</span>
<span class="p">};</span>
</pre></div>
</div>
<div class="section" id="bitness">
<h3>Bitness<a class="headerlink" href="#bitness" title="Permalink to this headline">¶</a></h3>
<p>The BIF can be either 32-bit ELF format or a 64-bit ELF format. For the GPU, OpenCL generates a 32-bit BIF binary; it can read either 32-bit BIF or 64-bit BIF binary. For the CPU, OpenCL generates and reads only 32-bit BIF binaries if the host application is 32-bit (on either 32-bit OS or 64-bit OS). It generates and reads only 64-bit BIF binary if the host application is 64-bit (on 64-bit OS).</p>
</div>
<div class="section" id="bif-options">
<h3>BIF Options<a class="headerlink" href="#bif-options" title="Permalink to this headline">¶</a></h3>
<p>OpenCL provides the following options to control what is contained in the binary.</p>
<p>-f[no-]bin-source — [not] generate OpenCL source in .source section.</p>
<p>-f[no-]bin-llvmir — [not] generate LLVM IR in .llvmir section.</p>
<p>-f[no-]bin-exe — [not] generate the executable (ISA) in .text section. The option syntax follows the GCC option syntax.
By default, OpenCL generates the .llvmir section, .amdil section, and .text
section. The following are examples for using these options: Example 1: Generate executable for execution:</p>
<p>clBuildProgram(program, 0, NULL, “-fno-bin-llvmir -fno-bin-amdil”, NULL,
NULL);</p>
<p>Example  2: Generate  only LLVM IR:</p>
<p>clBuildProgram(program, 0,  NULL,   “-fno-bin-exe -fno-bin-amdil”, NULL,
NULL);</p>
<p>This binary can recompile  for all the other devices  of the same device type.</p>
</div>
</div>
</div>
<div class="section" id="hardware-overview-of-pre-gcn-devices">
<span id="pre-gcn-devices"></span><h1>Hardware overview of pre-GCN devices<a class="headerlink" href="#hardware-overview-of-pre-gcn-devices" title="Permalink to this headline">¶</a></h1>
<p>This chapter provides a hardware overview of pre-GCN devices. Pre-GCN devices include the Evergreen and Northern Islands families that are based on VLIW.</p>
<p>A general OpenCL device comprises compute units, each of which can have multiple processing elements. A work-item (or SPMD kernel instance) executes on a single processing element. The processing elements within a compute unit can execute in lock-step using SIMD execution. Compute units, however, execute independently (see Figure D.1).</p>
<p>AMD GPUs consist of multiple compute units. The number of them and the way they are structured varies with the device family, as well as device designations within a family. Each of these processing elements possesses ALUs. For devices in the Northern Islands and Southern Islands families, these ALUs are arranged in four (in the Evergreen family, there are five) processing elements with arrays of 16 ALUs. Each of these arrays executes a single instruction across each lane for each of a block of 16 work-items. That instruction is repeated over four cycles to make the 64-element vector called a wavefront. On Northern Islands and Evergreen family devices, the PE arrays execute instructions from one wavefront, so that each work-item issues four (for Northern Islands) or five (for Evergreen) instructions at once in a very-long-instruction-word (VLIW) packet.</p>
<img alt="../_images/d.1.png" class="align-center" src="../_images/d.1.png" />
<img alt="../_images/d.2.png" class="align-center" src="../_images/d.2.png" />
<p>Figure D.2 is a simplified diagram of an AMD GPU compute device. Different GPU compute devices have different characteristics (such as the number of compute units), but follow a similar design pattern.</p>
<p>GPU compute devices comprise groups of compute units. Each compute unit contains numerous processing elements, which are responsible for executing kernels, each operating on an independent data stream. Processing elements, in turn, contain numerous processing elements, which are the fundamental, programmable ALUs that perform integer, single-precision floating-point, double- precision floating-point, and transcendental operations. All processing elements within a compute unit execute the same instruction sequence in lock-step for
Evergreen and Northern Islands devices; different compute units can execute</p>
<ol class="arabic simple">
<li>Much of this is transparent to the programmer.</li>
</ol>
<p>different instructions.</p>
<p>A processing element is arranged as a five-way or four-way (depending on the
GPU type) very long instruction word (VLIW) processor (see bottom of
Figure D.2). Up to five scalar operations (or four, depending on the GPU type) can be co-issued in a VLIW instruction, each of which are executed on one of the corresponding five ALUs. ALUs can execute single-precision floating point or integer operations. One of the five ALUs also can perform transcendental operations (sine, cosine, logarithm, etc.). Double-precision floating point operations are processed (where supported) by connecting two or four of the ALUs (excluding the transcendental core) to perform a single double-precision operation. The processing element also contains one branch execution unit to handle branch instructions.</p>
<p>Different GPU compute devices have different numbers of processing elements. For example, the ATI Radeon™ HD 5870 GPU has 20 compute units, each with
16 processing elements, and each processing elements contains five ALUs; this
yields 1600 physical ALUs.</p>
</div>
<div class="section" id="opencl-opengl-interoperability">
<span id="opencl-opengl"></span><h1>OpenCL-OpenGL Interoperability<a class="headerlink" href="#opencl-opengl-interoperability" title="Permalink to this headline">¶</a></h1>
<p>This chapter explains how to establish an association between GL context and
CL context.</p>
<p>Please note the following guidelines.</p>
<ol class="arabic simple">
<li>All devices used to create the OpenCL context associated with command_queue must support acquiring shared CL/GL objects. This constraint is enforced at context-creation time.</li>
<li>clCreateContext and clCreateContextFromType fail context creation if the device list passed in cannot interoperate with the GLcontext. clCreateContext only permits GL-friendly device(s). clCreateFromContextType can only include GL-friendly device(s).</li>
<li><dl class="first docutils">
<dt>Use clGetGLContextInfoKHR to determine GL-friendly device(s) from the following parameters:</dt>
<dd><ol class="first last loweralpha">
<li>CL_CURRENT_DEVICE_FOR_GL_CONTEXT_KHR only returns the device that can interoperate with the GL context.</li>
<li>CL_DEVICES_FOR_GL_CONTEXT_KHR includes all GL-context interoperable devices.</li>
</ol>
</dd>
</dl>
</li>
<li>While it is possible to create as many GL contexts on a GPU, do not create concurrently two GL contexts for two GPUs from the      same process.</li>
<li>For OpenGL interoperability with OpenCL, there is a strict order in which the OpenCL context is created and the texture/buffershared allocations can be made. To use shared resources, the OpenGL application must create an OpenGL context and afterwards an   OpenCL context. All resources (GL buffers and textures) created after the OpenCL context was created can be shared between OpenGL and OpenCL. If resources are allocated before the OpenCL context was created, they cannot be shared between OpenGL and OpenCL.</li>
</ol>
<div class="section" id="linux-operating-system">
<h2>Linux Operating System<a class="headerlink" href="#linux-operating-system" title="Permalink to this headline">¶</a></h2>
<div class="section" id="single-gpu-environment">
<h3>Single GPU Environment<a class="headerlink" href="#single-gpu-environment" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="creating-cl-context-from-a-gl-context">
<h3>Creating CL Context from a GL Context<a class="headerlink" href="#creating-cl-context-from-a-gl-context" title="Permalink to this headline">¶</a></h3>
<p>Using GLUT</p>
<ol class="arabic simple">
<li>Use glutInit to initialize the GLUT library and to negotiate a session with the windowing system. This function also processes the command-line options depending on the windowing system.</li>
<li>Use glXGetCurrentContext to get the current rendering context (GLXContext).</li>
<li>Use glXGetCurrentDisplay to get the display ( Display * ) that is associated with the current OpenGL rendering context of the calling thread.</li>
<li>Use clGetGLContextInfoKHR (see Section 9.7 of the OpenCL Specification 1.1) and the CL_CURRENT_DEVICE_FOR_GL_CONTEXT_KHR parameter to get the device ID of the CL device associated with the OpenGL context.</li>
<li>Use clCreateContext (see Section 4.3 of the OpenCL Specification 1.1) to create the CL context (of type cl_context).</li>
</ol>
<p>The following code snippet shows how to create an interoperability context using
GLUT in Linux.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">glutInit</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="n">argv</span><span class="p">);</span> <span class="n">glutInitDisplayMode</span><span class="p">(</span><span class="n">GLUT_RGBA</span> <span class="o">|</span> <span class="n">GLUT_DOUBLE</span><span class="p">);</span> <span class="n">glutInitWindowSize</span><span class="p">(</span><span class="n">WINDOW_WIDTH</span><span class="p">,</span> <span class="n">WINDOW_HEIGHT</span><span class="p">);</span> <span class="n">glutCreateWindow</span><span class="p">(</span><span class="s2">&quot;OpenCL SimpleGL&quot;</span><span class="p">);</span>
<span class="n">gGLXContext</span> <span class="n">glCtx</span> <span class="o">=</span> <span class="n">glXGetCurrentContext</span><span class="p">();</span> <span class="n">Cl_context_properties</span> <span class="n">cpsGL</span><span class="p">[]</span> <span class="o">=</span>
<span class="p">{</span>
<span class="n">CL_CONTEXT_PLATFORM</span><span class="p">,</span>

<span class="p">(</span><span class="n">cl_context_properties</span><span class="p">)</span><span class="n">platform</span><span class="p">,</span> <span class="n">CL_GLX_DISPLAY_KHR</span><span class="p">,</span>
<span class="p">(</span><span class="n">intptr_t</span><span class="p">)</span> <span class="n">glXGetCurrentDisplay</span><span class="p">(),</span> <span class="n">CL_GL_CONTEXT_KHR</span><span class="p">,</span>
 <span class="p">(</span>    <span class="n">intptr_t</span><span class="p">)</span> <span class="n">glCtx</span><span class="p">,</span> <span class="mi">0</span><span class="p">};</span>

 <span class="n">status</span> <span class="o">=</span> <span class="n">clGetGLContextInfoKHR</span><span class="p">(</span><span class="n">cpsGL</span><span class="p">,</span> <span class="n">CL_CURRENT_DEVICE_FOR_GL_CONTEXT_KHR</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">cl_device_id</span><span class="p">),</span>
<span class="o">&amp;</span><span class="n">interopDevice</span><span class="p">,</span>
<span class="n">NULL</span><span class="p">);</span>

<span class="o">//</span> <span class="n">Create</span> <span class="n">OpenCL</span> <span class="n">context</span> <span class="kn">from</span> <span class="nn">device</span><span class="s1">&#39;s id context = clCreateContext(cpsGL,</span>
<span class="mi">1</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">interopDevice</span><span class="p">,</span>
<span class="mi">0</span><span class="p">,</span>
<span class="mi">0</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>
</pre></div>
</div>
<p>Using X Window System</p>
<ol class="arabic simple">
<li>Use XOpenDisplay to open a connection to the server that controls a display.</li>
<li>Use glXChooseFBConfig to get a list of GLX frame buffer configurations that match the specified attributes.</li>
<li>Use glXChooseVisual to get a visual that matches specified attributes.</li>
<li>Use XCreateColormap to create a color map of the specified visual type for the screen on which the specified window resides and returns the colormap ID associated with it. Note that the specified window is only used to determine the screen.</li>
<li>Use XCreateWindow to create an unmapped sub-window for a specified parent window, returns the window ID of the created window, and causes the X server to generate a CreateNotify event. The created window is placed on top in the stacking order with respect to siblings.</li>
<li>Use XMapWindow to map the window and all of its sub-windows that have had map requests. Mapping a window that has an unmapped ancestor does not display the window, but marks it as eligible for display when the ancestor becomes mapped. Such a window is called unviewable. When all its ancestors are mapped, the window becomes viewable and is visible on the screen if it is not    obscured by another window.</li>
<li>Use glXCreateContextAttribsARB to initialize the context to the initial state defined by the OpenGL specification, and returns a handle to it. This handle can be used to render to any GLX surface.</li>
<li>Use glXMakeCurrent to make argrument3 (GLXContext) the current GLX rendering context of the calling thread, replacing the previously current context if there was one, and attaches argument3 (GLXcontext) to a GLX drawable, either a window or a GLX pixmap.</li>
<li>Use clGetGLContextInfoKHR to get the OpenCL-OpenGL interoperability device corresponding to the window created in step 5.</li>
<li>Use clCreateContext to create the context on the interoperable device obtained in step 9.</li>
</ol>
<p>The following code snippet shows how to create a CL-GL interoperability context using the X Window system in Linux.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Display</span> <span class="o">*</span><span class="n">displayName</span> <span class="o">=</span> <span class="n">XOpenDisplay</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>

<span class="nb">int</span> <span class="n">nelements</span><span class="p">;</span>
<span class="n">GLXFBConfig</span> <span class="o">*</span><span class="n">fbc</span> <span class="o">=</span> <span class="n">glXChooseFBConfig</span><span class="p">(</span><span class="n">displayName</span><span class="p">,</span>
<span class="n">DefaultScreen</span><span class="p">(</span><span class="n">displayName</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">nelements</span><span class="p">);</span>
<span class="n">static</span> <span class="nb">int</span> <span class="n">attributeList</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span> <span class="n">GLX_RGBA</span><span class="p">,</span>
<span class="n">GLX_DOUBLEBUFFER</span><span class="p">,</span>
<span class="n">GLX_RED_SIZE</span><span class="p">,</span>
<span class="mi">1</span><span class="p">,</span>
<span class="n">GLX_GREEN_SIZE</span><span class="p">,</span>
<span class="mi">1</span><span class="p">,</span>
<span class="n">GLX_BLUE_SIZE</span><span class="p">,</span>
<span class="mi">1</span><span class="p">,</span>
<span class="kc">None</span>
<span class="p">};</span>
<span class="n">XVisualInfo</span> <span class="o">*</span><span class="n">vi</span> <span class="o">=</span> <span class="n">glXChooseVisual</span><span class="p">(</span><span class="n">displayName</span><span class="p">,</span>
<span class="n">DefaultScreen</span><span class="p">(</span><span class="n">displayName</span><span class="p">),</span>
<span class="n">attributeList</span><span class="p">);</span>

<span class="n">XSetWindowAttributes</span> <span class="n">swa</span><span class="p">;</span>
<span class="n">swa</span><span class="o">.</span><span class="n">colormap</span> <span class="o">=</span> <span class="n">XCreateColormap</span><span class="p">(</span><span class="n">displayName</span><span class="p">,</span>
<span class="n">RootWindow</span><span class="p">(</span><span class="n">displayName</span><span class="p">,</span> <span class="n">vi</span><span class="o">-&gt;</span><span class="n">screen</span><span class="p">),</span>
<span class="n">vi</span><span class="o">-&gt;</span><span class="n">visual</span><span class="p">,</span>
<span class="n">AllocNone</span><span class="p">);</span>
<span class="n">swa</span><span class="o">.</span><span class="n">border_pixel</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="n">swa</span><span class="o">.</span><span class="n">event_mask</span> <span class="o">=</span> <span class="n">StructureNotifyMask</span><span class="p">;</span>

<span class="n">Window</span> <span class="n">win</span> <span class="o">=</span> <span class="n">XCreateWindow</span><span class="p">(</span><span class="n">displayName</span><span class="p">,</span> <span class="n">RootWindow</span><span class="p">(</span><span class="n">displayName</span><span class="p">,</span> <span class="n">vi</span><span class="o">-&gt;</span><span class="n">screen</span><span class="p">),</span>
<span class="mi">10</span><span class="p">,</span>
<span class="mi">10</span><span class="p">,</span>
<span class="n">WINDOW_WIDTH</span><span class="p">,</span>
<span class="n">WINDOW_HEIGHT</span><span class="p">,</span>
<span class="mi">0</span><span class="p">,</span>
<span class="n">vi</span><span class="o">-&gt;</span><span class="n">depth</span><span class="p">,</span>
<span class="n">InputOutput</span><span class="p">,</span>
<span class="n">vi</span><span class="o">-&gt;</span><span class="n">visual</span><span class="p">,</span>
<span class="n">CWBorderPixel</span><span class="o">|</span><span class="n">CWColormap</span><span class="o">|</span><span class="n">CWEventMask</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">swa</span><span class="p">);</span>

<span class="n">XMapWindow</span> <span class="p">(</span><span class="n">displayName</span><span class="p">,</span> <span class="n">win</span><span class="p">);</span>
<span class="n">std</span><span class="p">::</span><span class="n">cout</span>     <span class="o">&lt;&lt;</span> <span class="s2">&quot;glXCreateContextAttribsARB &quot;</span>
<span class="o">&lt;&lt;</span> <span class="p">(</span><span class="n">void</span><span class="o">*</span><span class="p">)</span> <span class="n">glXGetProcAddress</span><span class="p">((</span><span class="n">const</span>
<span class="n">GLubyte</span><span class="o">*</span><span class="p">)</span><span class="s2">&quot;glXCreateContextAttribsARB&quot;</span><span class="p">)</span>
<span class="o">&lt;&lt;</span> <span class="n">std</span><span class="p">::</span><span class="n">endl</span><span class="p">;</span>

<span class="n">GLXCREATECONTEXTATTRIBSARBPROC</span> <span class="n">glXCreateContextAttribsARB</span> <span class="o">=</span> <span class="p">(</span><span class="n">GLXCREATECONTEXTATTRIBSARBPROC</span><span class="p">)</span>
<span class="n">glXGetProcAddress</span><span class="p">((</span><span class="n">const</span>
<span class="n">GLubyte</span><span class="o">*</span><span class="p">)</span><span class="s2">&quot;glXCreateContextAttribsARB&quot;</span><span class="p">);</span>

<span class="nb">int</span> <span class="n">attribs</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span>
<span class="n">GLX_CONTEXT_MAJOR_VERSION_ARB</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span>
<span class="n">GLX_CONTEXT_MINOR_VERSION_ARB</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="mi">0</span>
<span class="p">};</span>

<span class="n">GLXContext</span> <span class="n">ctx</span> <span class="o">=</span> <span class="n">glXCreateContextAttribsARB</span><span class="p">(</span><span class="n">displayName</span><span class="p">,</span>
<span class="o">*</span><span class="n">fbc</span><span class="p">,</span>
<span class="mi">0</span><span class="p">,</span>
<span class="n">true</span><span class="p">,</span>
<span class="n">attribs</span><span class="p">);</span>
<span class="n">glXMakeCurrent</span> <span class="p">(</span><span class="n">displayName</span><span class="p">,</span>

<span class="n">win</span><span class="p">,</span> <span class="n">ctx</span><span class="p">);</span>
<span class="n">cl_context_properties</span> <span class="n">cpsGL</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span> <span class="n">CL_CONTEXT_PLATFORM</span><span class="p">,(</span><span class="n">cl_context_properties</span><span class="p">)</span><span class="n">platform</span><span class="p">,</span> <span class="n">CL_GLX_DISPLAY_KHR</span><span class="p">,</span> <span class="p">(</span><span class="n">intptr_t</span><span class="p">)</span> <span class="n">glXGetCurrentDisplay</span><span class="p">(),</span> <span class="n">CL_GL_CONTEXT_KHR</span><span class="p">,</span> <span class="p">(</span><span class="n">intptr_t</span><span class="p">)</span> <span class="n">gGlCtx</span><span class="p">,</span> <span class="mi">0</span>
<span class="p">};</span>
<span class="n">status</span> <span class="o">=</span> <span class="n">clGetGLContextInfoKHR</span><span class="p">(</span> <span class="n">cpsGL</span><span class="p">,</span>
<span class="n">CL_CURRENT_DEVICE_FOR_GL_CONTEXT_KHR</span><span class="p">,</span>
<span class="n">sizeof</span><span class="p">(</span><span class="n">cl_device_id</span><span class="p">),</span>
<span class="o">&amp;</span><span class="n">interopDeviceId</span><span class="p">,</span>
<span class="n">NULL</span><span class="p">);</span>

<span class="o">//</span> <span class="n">Create</span> <span class="n">OpenCL</span> <span class="n">context</span> <span class="kn">from</span> <span class="nn">device</span><span class="s1">&#39;s id context = clCreateContext(cpsGL,</span>
<span class="mi">1</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">interopDeviceId</span><span class="p">,</span>
<span class="mi">0</span><span class="p">,</span>
<span class="mi">0</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="multi-gpu-configuration">
<h3>Multi-GPU Configuration<a class="headerlink" href="#multi-gpu-configuration" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="id37">
<h3>Creating CL Context from a GL Context<a class="headerlink" href="#id37" title="Permalink to this headline">¶</a></h3>
<p>Using X Window System</p>
<ol class="arabic">
<li><p class="first">Use XOpenDisplay to open a connection to the server that controls a display.</p>
</li>
<li><p class="first">Use ScreenCount to get the number of available screens.</p>
</li>
<li><p class="first">Use XCloseDisplay to close the connection to the X server for the display specified in the Display structure and destroy all windows, resource IDs (Window, Font, Pixmap, Colormap, Cursor, and GContext), or other resources that the client created on this display.</p>
</li>
<li><p class="first">Use a FOR loop to enumerate the displays. To change the display, change the value of the environment variable DISPLAY.</p>
</li>
<li><dl class="first docutils">
<dt>Inside the loop:</dt>
<dd><ol class="first loweralpha simple">
<li>Use putenv to set the environment variable DISPLAY with respect to the display number.</li>
<li>Use OpenDisplay to open a connection to the server that controls a display.</li>
<li>Use glXChooseFBConfig to get a list of GLX frame buffer configurations that match the specified attributes.</li>
</ol>
<p class="last">d. Use glXChooseVisual to get a visual that matches specified attributes. e.   Use XCreateColormap to create a color map of the specified visual type for the screen on which the specified window resides and returns the colormap ID associated with it. Note that the specified window is only used to determine the screen.
f. Use XCreateWindow to create an unmapped sub-window for a specified parent window, returns the window ID of the created window, and causes the X server to generate a CreateNotify event. The created window is placed on top in the stacking order with respect to siblings.
g. Use XMapWindow to map the window and all of its sub-windows that have had map requests. Mapping a window that has an unmapped ancestor does not display the window but marks it as eligible for display when the ancestor becomes mapped. Such a window is called unviewable. When all its ancestors are mapped, the window becomes viewable and is visible on the screen, if it is not obscured by another window.
h. Use glXCreateContextAttribsARB function to initialize the context to the initial state defined by the OpenGL specification and       return a handle to it. This handle can be used to render to any GLX surface.
i. Use glXMakeCurrent to make argrument3 (GLXContext) the current GLX rendering context of the calling thread, replacing the previously current context, if there was one, and to attach argument3 (GLXcontext) to a GLX drawable, either a window or a GLX pixmap.
j. Use clGetGLContextInfoKHR to get the number of OpenCL-OpenGL interoperability devices corresponding to the window created in f, above.
k. If the number of interoperable devices is zero, use glXDestroyContext to destroy the context created at step h, and go to step A otherwise, exit from the loop (an OpenCL-OpenGL interoperable device has been found).</p>
</dd>
</dl>
</li>
<li><p class="first">Use clGetGLContextInfoKHR to get the OpenCL-OpenGL interoperable device id.</p>
</li>
<li><p class="first">Use clCreateContext to create the context on the interoperable device obtained in the previous step.</p>
</li>
</ol>
<p>The following code segment shows how to create an OpenCL-OpenGL interoperability context on a system with multiple GPUs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">displayName</span> <span class="o">=</span> <span class="n">XOpenDisplay</span><span class="p">(</span><span class="n">NULL</span><span class="p">);</span>
<span class="nb">int</span> <span class="n">screenNumber</span> <span class="o">=</span> <span class="n">ScreenCount</span><span class="p">(</span><span class="n">displayName</span><span class="p">);</span>
<span class="n">XCloseDisplay</span><span class="p">(</span><span class="n">displayName</span><span class="p">);</span>

<span class="k">for</span> <span class="p">(</span><span class="nb">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">screenNumber</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
<span class="p">{</span>
<span class="k">if</span> <span class="p">(</span><span class="n">isDeviceIdEnabled</span><span class="p">())</span>
<span class="p">{</span>
<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">deviceId</span><span class="p">)</span>
<span class="p">{</span>
<span class="k">continue</span><span class="p">;</span>
<span class="p">}</span>
<span class="p">}</span>
<span class="n">char</span> <span class="n">disp</span><span class="p">[</span><span class="mi">100</span><span class="p">];</span>
<span class="n">sprintf</span><span class="p">(</span><span class="n">disp</span><span class="p">,</span> <span class="s2">&quot;DISPLAY=:0.</span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span>
<span class="n">putenv</span><span class="p">(</span><span class="n">disp</span><span class="p">);</span>
<span class="n">displayName</span> <span class="o">=</span> <span class="n">XOpenDisplay</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="nb">int</span> <span class="n">nelements</span><span class="p">;</span>
<span class="n">GLXFBConfig</span> <span class="o">*</span><span class="n">fbc</span> <span class="o">=</span> <span class="n">glXChooseFBConfig</span><span class="p">(</span><span class="n">displayName</span><span class="p">,</span>
<span class="n">DefaultScreen</span><span class="p">(</span><span class="n">displayName</span><span class="p">),</span>
<span class="mi">0</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">nelements</span><span class="p">);</span>
<span class="n">static</span> <span class="nb">int</span> <span class="n">attributeList</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span> <span class="n">GLX_RGBA</span><span class="p">,</span>
<span class="n">GLX_DOUBLEBUFFER</span><span class="p">,</span>
<span class="n">GLX_RED_SIZE</span><span class="p">,</span>
<span class="mi">1</span><span class="p">,</span>
<span class="n">GLX_GREEN_SIZE</span><span class="p">,</span>
<span class="mi">1</span><span class="p">,</span>
<span class="n">GLX_BLUE_SIZE</span><span class="p">,</span>
<span class="mi">1</span><span class="p">,</span>
<span class="kc">None</span>
<span class="p">};</span>

<span class="n">XVisualInfo</span> <span class="o">*</span><span class="n">vi</span> <span class="o">=</span> <span class="n">glXChooseVisual</span><span class="p">(</span><span class="n">displayName</span><span class="p">,</span> <span class="n">DefaultScreen</span><span class="p">(</span><span class="n">displayName</span><span class="p">),</span> <span class="n">attributeList</span><span class="p">);</span>
<span class="n">XSetWindowAttributes</span> <span class="n">swa</span><span class="p">;</span>
<span class="n">swa</span><span class="o">.</span><span class="n">colormap</span> <span class="o">=</span> <span class="n">XCreateColormap</span><span class="p">(</span><span class="n">displayName</span><span class="p">,</span>
<span class="n">RootWindow</span><span class="p">(</span><span class="n">displayName</span><span class="p">,</span> <span class="n">vi</span><span class="o">-&gt;</span><span class="n">screen</span><span class="p">),</span>
<span class="n">vi</span><span class="o">-&gt;</span><span class="n">visual</span><span class="p">,</span>
<span class="n">AllocNone</span><span class="p">);</span>
<span class="n">swa</span><span class="o">.</span><span class="n">border_pixel</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="n">swa</span><span class="o">.</span><span class="n">event_mask</span> <span class="o">=</span> <span class="n">StructureNotifyMask</span><span class="p">;</span>

<span class="n">win</span> <span class="o">=</span> <span class="n">XCreateWindow</span><span class="p">(</span><span class="n">displayName</span><span class="p">,</span> <span class="n">RootWindow</span><span class="p">(</span><span class="n">displayName</span><span class="p">,</span> <span class="n">vi</span><span class="o">-&gt;</span><span class="n">screen</span><span class="p">),</span>
<span class="mi">10</span><span class="p">,</span>
<span class="mi">10</span><span class="p">,</span>
<span class="n">width</span><span class="p">,</span>
<span class="n">height</span><span class="p">,</span>
<span class="mi">0</span><span class="p">,</span>
<span class="n">vi</span><span class="o">-&gt;</span><span class="n">depth</span><span class="p">,</span>
<span class="n">InputOutput</span><span class="p">,</span>
<span class="n">vi</span><span class="o">-&gt;</span><span class="n">visual</span><span class="p">,</span>
<span class="n">CWBorderPixel</span><span class="o">|</span><span class="n">CWColormap</span><span class="o">|</span><span class="n">CWEventMask</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">swa</span><span class="p">);</span>

<span class="n">XMapWindow</span> <span class="p">(</span><span class="n">displayName</span><span class="p">,</span> <span class="n">win</span><span class="p">);</span>
<span class="nb">int</span> <span class="n">attribs</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span>
<span class="p">};</span>

<span class="n">GLX_CONTEXT_MAJOR_VERSION_ARB</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">GLX_CONTEXT_MINOR_VERSION_ARB</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="mi">0</span>
<span class="n">GLXContext</span> <span class="n">ctx</span> <span class="o">=</span> <span class="n">glXCreateContextAttribsARB</span><span class="p">(</span><span class="n">displayName</span><span class="p">,</span>
<span class="o">*</span><span class="n">fbc</span><span class="p">,</span>
<span class="mi">0</span><span class="p">,</span>
<span class="n">true</span><span class="p">,</span>
<span class="n">attribs</span><span class="p">);</span>
<span class="n">glXMakeCurrent</span> <span class="p">(</span><span class="n">displayName</span><span class="p">,</span>
<span class="n">win</span><span class="p">,</span>
<span class="n">ctx</span><span class="p">);</span>

<span class="n">gGlCtx</span> <span class="o">=</span> <span class="n">glXGetCurrentContext</span><span class="p">();</span>
<span class="n">properties</span> <span class="n">cpsGL</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span>
<span class="n">CL_CONTEXT_PLATFORM</span><span class="p">,</span> <span class="p">(</span><span class="n">cl_context_properties</span><span class="p">)</span><span class="n">platform</span><span class="p">,</span>
<span class="n">CL_GLX_DISPLAY_KHR</span><span class="p">,</span> <span class="p">(</span><span class="n">intptr_t</span><span class="p">)</span> <span class="n">glXGetCurrentDisplay</span><span class="p">(),</span>
<span class="n">CL_GL_CONTEXT_KHR</span><span class="p">,</span> <span class="p">(</span><span class="n">intptr_t</span><span class="p">)</span> <span class="n">gGlCtx</span><span class="p">,</span> <span class="mi">0</span>
<span class="p">};</span>

<span class="n">size_t</span> <span class="n">deviceSize</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="n">status</span> <span class="o">=</span> <span class="n">clGetGLContextInfoKHR</span><span class="p">(</span><span class="n">cpsGL</span><span class="p">,</span>
<span class="n">CL_CURRENT_DEVICE_FOR_GL_CONTEXT_KHR</span><span class="p">,</span>
<span class="mi">0</span><span class="p">,</span>
<span class="n">NULL</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">deviceSize</span><span class="p">);</span>
<span class="nb">int</span> <span class="n">numDevices</span> <span class="o">=</span> <span class="p">(</span><span class="n">deviceSize</span> <span class="o">/</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">cl_device_id</span><span class="p">));</span>


<span class="k">if</span><span class="p">(</span><span class="n">numDevices</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
<span class="p">{</span>
<span class="n">glXDestroyContext</span><span class="p">(</span><span class="n">glXGetCurrentDisplay</span><span class="p">(),</span> <span class="n">gGlCtx</span><span class="p">);</span>
<span class="k">continue</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">else</span>
<span class="p">{</span>
<span class="o">//</span><span class="n">Interoperable</span> <span class="n">device</span> <span class="n">found</span> <span class="n">std</span><span class="p">::</span><span class="n">cout</span><span class="o">&lt;&lt;</span><span class="s2">&quot;Interoperable device found &quot;</span><span class="o">&lt;&lt;</span><span class="n">std</span><span class="p">::</span><span class="n">endl</span><span class="p">;</span> <span class="k">break</span><span class="p">;</span>
<span class="p">}</span>
<span class="p">}</span>

<span class="n">status</span> <span class="o">=</span> <span class="n">clGetGLContextInfoKHR</span><span class="p">(</span> <span class="n">cpsGL</span><span class="p">,</span> <span class="n">CL_CURRENT_DEVICE_FOR_GL_CONTEXT_KHR</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">cl_device_id</span><span class="p">),</span>
<span class="o">&amp;</span><span class="n">interopDeviceId</span><span class="p">,</span> <span class="n">NULL</span><span class="p">);</span>

<span class="o">//</span> <span class="n">Create</span> <span class="n">OpenCL</span> <span class="n">context</span> <span class="kn">from</span> <span class="nn">device</span><span class="s1">&#39;s id context = clCreateContext(cpsGL,</span>
<span class="mi">1</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">interopDeviceId</span><span class="p">,</span>
<span class="mi">0</span><span class="p">,</span>
<span class="mi">0</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="additional-gl-formats-supported">
<h3>Additional GL Formats Supported<a class="headerlink" href="#additional-gl-formats-supported" title="Permalink to this headline">¶</a></h3>
<p>The following is a list of GL formats beyond the minimum set listed in The OpenCL Extension Specification, v 1.2 that AMD supports.</p>
<table border="1" class="docutils">
<colgroup>
<col width="56%" />
<col width="44%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">AMD-Supported GL Formats</th>
<th class="head">GL internal format</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>GL_ALPHA8</td>
<td>CL_A,CL_UNORM8</td>
</tr>
<tr class="row-odd"><td>GL_R8, CL_R,</td>
<td>CL_UNORM_INT8</td>
</tr>
<tr class="row-even"><td>GL_R8UI CL_R,</td>
<td>CL_UNSIGNED_INT8</td>
</tr>
<tr class="row-odd"><td>GL_R8I CL_R,</td>
<td>CL_SIGNED_INT8</td>
</tr>
<tr class="row-even"><td>GL_RG8 CL_RG,</td>
<td>CL_UNORM_INT8</td>
</tr>
<tr class="row-odd"><td>GL_RG8UI CL_RG,</td>
<td>CL_UNSIGNED_INT8</td>
</tr>
<tr class="row-even"><td>GL_RG8I CL_RG,</td>
<td>CL_SIGNED_INT8</td>
</tr>
<tr class="row-odd"><td>GL_RGB8 CL_RGB,</td>
<td>CL_UNORM_INT8</td>
</tr>
<tr class="row-even"><td>GL_RGB8UI CL_RGB,</td>
<td>CL_UNSIGNED_INT8</td>
</tr>
<tr class="row-odd"><td>GL_RGB8I CL_RGB,</td>
<td>CL_SIGNED_INT8</td>
</tr>
<tr class="row-even"><td>GL_R16 CL_R,</td>
<td>CL_UNORM_INT16</td>
</tr>
<tr class="row-odd"><td>GL_R16UI CL_R,</td>
<td>CL_UNSIGNED_INT16</td>
</tr>
<tr class="row-even"><td>GL_R16I CL_R,</td>
<td>CL_SIGNED_INT16</td>
</tr>
<tr class="row-odd"><td>GL_RG16 CL_RG,</td>
<td>CL_UNORM_INT16</td>
</tr>
<tr class="row-even"><td>GL_RG16UI CL_RG,</td>
<td>CL_UNSIGNED_INT16</td>
</tr>
<tr class="row-odd"><td>GL_RG16I CL_RG,</td>
<td>CL_SIGNED_INT16</td>
</tr>
<tr class="row-even"><td>GL_RGB16 CL_RGB,</td>
<td>CL_UNORM_INT16</td>
</tr>
<tr class="row-odd"><td>GL_RGB16UI CL_RGB,</td>
<td>CL_UNSIGNED_INT16</td>
</tr>
<tr class="row-even"><td>GL_RGB16I CL_RGB,</td>
<td>CL_SIGNED_INT16</td>
</tr>
<tr class="row-odd"><td>GL_R32I CL_R,</td>
<td>CL_SIGNED_INT32</td>
</tr>
<tr class="row-even"><td>GL_R32UI CL_R,</td>
<td>CL_UNSIGNED_INT32</td>
</tr>
<tr class="row-odd"><td>GL_R32F CL_R,</td>
<td>CL_FLOAT</td>
</tr>
<tr class="row-even"><td>GL_RG32I CL_RG,</td>
<td>CL_SIGNED_INT32</td>
</tr>
<tr class="row-odd"><td>GL_RG32UI CL_RG,</td>
<td>CL_UNSIGNED_INT32</td>
</tr>
<tr class="row-even"><td>GL_RG32F CL_RG,</td>
<td>CL_FLOAT</td>
</tr>
<tr class="row-odd"><td>GL_RGB32I CL_RGB,</td>
<td>CL_SIGNED_INT32</td>
</tr>
<tr class="row-even"><td>GL_RGB32UI CL_RGB,</td>
<td>CL_UNSIGNED_INT32</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="new-built-in-functions-in-opencl-2-0">
<span id="functions-opencl"></span><h1>New built-in functions in OpenCL 2.0<a class="headerlink" href="#new-built-in-functions-in-opencl-2-0" title="Permalink to this headline">¶</a></h1>
<div class="section" id="list-of-functions">
<h2>List of Functions<a class="headerlink" href="#list-of-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="work-item-functions">
<h3>Work Item Functions<a class="headerlink" href="#work-item-functions" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="29%" />
<col width="71%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>get_enqueued_local_size</td>
<td>local sizes in uniform part of NDRange</td>
</tr>
<tr class="row-even"><td>get_global_linear_id</td>
<td>unique 1D index for each work item in the NDRange</td>
</tr>
<tr class="row-odd"><td>get_local_linear_id</td>
<td>unique 1D index for each work item in the work group</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="integer-functions">
<h3>Integer functions<a class="headerlink" href="#integer-functions" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><div class="line-block">
<div class="line">ctz                          :               count trailing zero bits</div>
</div>
</div></blockquote>
</div>
<div class="section" id="synchronization-functions">
<h3>Synchronization Functions<a class="headerlink" href="#synchronization-functions" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><div class="line-block">
<div class="line">work_group_barrier           :               replaces barrier, adds scope</div>
</div>
</div></blockquote>
</div>
<div class="section" id="address-space-qualifier-functions">
<h3>Address space qualifier functions<a class="headerlink" href="#address-space-qualifier-functions" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="29%" />
<col width="71%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>to_global</td>
<td>convert generic pointer to global pointer</td>
</tr>
<tr class="row-even"><td>to_local</td>
<td>convert genericpointer to local pointer</td>
</tr>
<tr class="row-odd"><td>to_private</td>
<td>convert generic pointer to private pointer</td>
</tr>
<tr class="row-even"><td>get_fence</td>
<td>get fence appropriate to address space</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="atomic-functions">
<h3>Atomic functions<a class="headerlink" href="#atomic-functions" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="52%" />
<col width="48%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>atomic_init</td>
<td>Initialize atomic value</td>
</tr>
<tr class="row-even"><td>atomic_work_item_fence</td>
<td>memory fence</td>
</tr>
<tr class="row-odd"><td>atomic_store[_explicit]</td>
<td>atomic store</td>
</tr>
<tr class="row-even"><td>atomic_load[_explicit]</td>
<td>atomic load</td>
</tr>
<tr class="row-odd"><td>atomic_exchange[_explicit]</td>
<td>atomic exchange</td>
</tr>
<tr class="row-even"><td>atomic_compare_exchange_strong[_explicit]</td>
<td>atomic compare and exchange (CAS)</td>
</tr>
<tr class="row-odd"><td>atomic_compare_exchange_weak[_explicit]</td>
<td>atomic compare and exchange (CAS)</td>
</tr>
<tr class="row-even"><td>atomic_fetch_add[_explicit]</td>
<td>atomic fetch+add</td>
</tr>
<tr class="row-odd"><td>atomic_fetch_sub[_explicit]</td>
<td>atomic fetch+sub</td>
</tr>
<tr class="row-even"><td>atomic_fetch_or[_explicit]</td>
<td>atomic fetch+or</td>
</tr>
<tr class="row-odd"><td>atomic_fetcn_xor[_explicit]</td>
<td>atomic fetch+xor</td>
</tr>
<tr class="row-even"><td>atomic_fetch_and[_explicit]</td>
<td>atomic fetch+and</td>
</tr>
<tr class="row-odd"><td>atomic_fetch_max_[explicit]</td>
<td>atomic fetch+max</td>
</tr>
<tr class="row-even"><td>atomic_fetch_min[_explicit]</td>
<td>&#160;</td>
</tr>
<tr class="row-odd"><td>atomic fetch+min</td>
<td>&#160;</td>
</tr>
<tr class="row-even"><td>atomic_flag_test_and_set[_explicit]</td>
<td>atomic flag set</td>
</tr>
<tr class="row-odd"><td>atomic_flag_clear[_explicit]</td>
<td>atomic flag clear</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="image-read-and-write-functions">
<h3>Image Read and Write Functions<a class="headerlink" href="#image-read-and-write-functions" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><div class="line-block">
<div class="line">read_imagef          :                       Read from 2D depth [array] image</div>
<div class="line">write_imagef         :                       Write to 2D depth [array] image</div>
</div>
</div></blockquote>
</div>
<div class="section" id="work-group-functions">
<h3>Work group functions<a class="headerlink" href="#work-group-functions" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="39%" />
<col width="61%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>work_group_all</td>
<td>Test all members of work group (and reduction)</td>
</tr>
<tr class="row-even"><td>work_group_any</td>
<td>Test any member of work group (or reduction)</td>
</tr>
<tr class="row-odd"><td>work_group_broadcast</td>
<td>Brodcast value to every member of work group</td>
</tr>
<tr class="row-even"><td>work_group_reduce_add</td>
<td>Sum reduction across work group</td>
</tr>
<tr class="row-odd"><td>work_group_reduce_max</td>
<td>Max reduction across work group</td>
</tr>
<tr class="row-even"><td>work_group_reduce_min</td>
<td>Min reduction across work group</td>
</tr>
<tr class="row-odd"><td>work_group_scan_exclusive_add</td>
<td>Sum exclusive scan across work group</td>
</tr>
<tr class="row-even"><td>work_group_scan_exclusive_max</td>
<td>Max exclusive scan across work group</td>
</tr>
<tr class="row-odd"><td>work_group_scan_exclusive_min</td>
<td>Min exclusive scan across work group</td>
</tr>
<tr class="row-even"><td>work_group_scan_inclusive_add Sum inclusive</td>
<td>scan across work group</td>
</tr>
<tr class="row-odd"><td>work_group_scan_inclusive_max Max inclusive</td>
<td>scan across work group</td>
</tr>
<tr class="row-even"><td>work_group_scan_inclusive_min Min inclusive</td>
<td>scan across work group</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="pipe-functions">
<h3>Pipe functions<a class="headerlink" href="#pipe-functions" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="40%" />
<col width="60%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>read_pipe</td>
<td>Read from pipe</td>
</tr>
<tr class="row-even"><td>write_pipe</td>
<td>Write to pipe</td>
</tr>
<tr class="row-odd"><td>reserve_read_pipe</td>
<td>Reserve reads from pipe</td>
</tr>
<tr class="row-even"><td>reserve_write_pipe</td>
<td>Reserve writes to pipe</td>
</tr>
<tr class="row-odd"><td>commit_read_pipe</td>
<td>Commit reserved pipe reads</td>
</tr>
<tr class="row-even"><td>commit_write_pipe</td>
<td>Commit reserved pipe writes</td>
</tr>
<tr class="row-odd"><td>is_valid_reserve_id</td>
<td>Test reservation value</td>
</tr>
<tr class="row-even"><td>work_group_reserve_read_pipe</td>
<td>Work group read reservation</td>
</tr>
<tr class="row-odd"><td>work_group_reserve_write_pipe</td>
<td>work group write reservation</td>
</tr>
<tr class="row-even"><td>work_group_commit_read_pipe</td>
<td>work group commit read reservation</td>
</tr>
<tr class="row-odd"><td>work_group_commit_write_pipe</td>
<td>work group commit write reservation</td>
</tr>
<tr class="row-even"><td>get_pipe_num_packets</td>
<td>get current number of packets in pipe</td>
</tr>
<tr class="row-odd"><td>get_pipe_max_packets</td>
<td>get capacity of pipe</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="enqueueing-kernels">
<h3>Enqueueing Kernels<a class="headerlink" href="#enqueueing-kernels" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="45%" />
<col width="55%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>enqueue_kernel</td>
<td>Enqueue block as kernel</td>
</tr>
<tr class="row-even"><td>get_kernel_work_group_size</td>
<td>Query max work group size</td>
</tr>
<tr class="row-odd"><td>get_kernel_preferred_work_group_size_m</td>
<td>Query preferred divisor of work group size multiple</td>
</tr>
<tr class="row-even"><td>enqueue_marker</td>
<td>Enqueue a marker</td>
</tr>
<tr class="row-odd"><td>retain_event</td>
<td>Increment refcount of event</td>
</tr>
<tr class="row-even"><td>release_event</td>
<td>Decrement refcount of event</td>
</tr>
<tr class="row-odd"><td>create_user_event</td>
<td>Create user event</td>
</tr>
<tr class="row-even"><td>is_valid_event</td>
<td>Check if event is valid</td>
</tr>
<tr class="row-odd"><td>set_user_event_status</td>
<td>Signal user event</td>
</tr>
<tr class="row-even"><td>capture_event_profiling_info</td>
<td>Schedule capture of profiling info</td>
</tr>
<tr class="row-odd"><td>get_default_queue</td>
<td>Get default queue</td>
</tr>
<tr class="row-even"><td>ndrange_1D</td>
<td>Create 1D NDRange</td>
</tr>
<tr class="row-odd"><td>ndrange_2D</td>
<td>Create 2D NDRange</td>
</tr>
<tr class="row-even"><td>ndrange_3D</td>
<td>Create 3D NDRange</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="deprecated-built-ins-barrier">
<h3>Deprecated built-ins barrier<a class="headerlink" href="#deprecated-built-ins-barrier" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><div class="line-block">
<div class="line">mem_fence</div>
<div class="line">read_mem_fence</div>
<div class="line">write_mem_fence</div>
<div class="line">atomic_add</div>
<div class="line">atomic_sub</div>
<div class="line">atomic_xchg</div>
<div class="line">atomic_inc</div>
<div class="line">atomic_dec</div>
<div class="line">atomic_cmpxchg</div>
<div class="line">atomic_min</div>
<div class="line">atomic_max</div>
<div class="line">atomic_and</div>
<div class="line">atomic_or</div>
<div class="line">atomic_xor</div>
</div>
</div></blockquote>
</div>
<div class="section" id="new-runtime-apis-in-opencl-2-0">
<h3>New runtime APIs in OpenCL 2.0<a class="headerlink" href="#new-runtime-apis-in-opencl-2-0" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="new-types">
<h3>New Types<a class="headerlink" href="#new-types" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="42%" />
<col width="58%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>cl_device_svm_capabilities</td>
<td>Returned by clGetDeviceInfo(…CL_DEVICE_SVM_CAPABILITIES…)</td>
</tr>
<tr class="row-even"><td>cl_queue_properties</td>
<td>See clCreateCommandQueueWithProperties</td>
</tr>
<tr class="row-odd"><td>cl_svm_mem_flags</td>
<td>See clSVMAlloc</td>
</tr>
<tr class="row-even"><td>cl_pipe_properties</td>
<td>See clCreatePipe</td>
</tr>
<tr class="row-odd"><td>cl_pipe_info</td>
<td>See clGetPipeInfo</td>
</tr>
<tr class="row-even"><td>cl_sampler_properties</td>
<td>See clCreateSamplerWithProperties</td>
</tr>
<tr class="row-odd"><td>cl_kernel_exec_info</td>
<td>See clSetKernelExecInfo</td>
</tr>
<tr class="row-even"><td>cl_image_desc</td>
<td>A field name changed from buffer to mem_object</td>
</tr>
<tr class="row-odd"><td>cl_kernel_sub_group_info</td>
<td>See clGetKernelSubGroupInfoKHR</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="new-macros">
<h3>New Macros<a class="headerlink" href="#new-macros" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><div class="line-block">
<div class="line">CL_INVALID_PIPE_SIZE</div>
<div class="line">CL_INVALID_DEVICE_QUEUE</div>
<div class="line">CL_VERSION_2_0</div>
<div class="line">CL_DEVICE_QUEUE_ON_HOST_PROPERTIES</div>
<div class="line">CL_DEVICE_MAX_READ_WRITE_IMAGE_ARGS</div>
<div class="line">CL_DEVICE_MAX_GLOBAL_VARIABLE_SIZE</div>
<div class="line">CL_DEVICE_QUEUE_ON_DEVICE_PROPERTIES</div>
<div class="line">CL_DEVICE_QUEUE_ON_DEVICE_PREFERRED_SIZE</div>
<div class="line">CL_DEVICE_QUEUE_ON_DEVICE_MAX_SIZE</div>
<div class="line">CL_DEVICE_MAX_ON_DEVICE_QUEUES</div>
<div class="line">CL_DEVICE_MAX_ON_DEVICE_EVENTS</div>
<div class="line">CL_DEVICE_SVM_CAPABILITIES</div>
<div class="line">CL_DEVICE_GLOBAL_VARIABLE_PREFERRED_TOTAL_SIZE</div>
<div class="line">CL_DEVICE_MAX_PIPE_ARGS</div>
<div class="line">CL_DEVICE_PIPE_MAX_ACTIVE_RESERVATIONS</div>
<div class="line">CL_DEVICE_PIPE_MAX_PACKET_SIZE</div>
<div class="line">CL_DEVICE_PREFERRED_PLATFORM_ATOMIC_ALIGNMENT</div>
<div class="line">CL_DEVICE_PREFERRED_GLOBAL_ATOMIC_ALIGNMENT</div>
<div class="line">CL_DEVICE_PREFERRED_LOCAL_ATOMIC_ALIGNMENT</div>
<div class="line">CL_QUEUE_ON_DEVICE</div>
<div class="line">CL_QUEUE_ON_DEVICE_DEFAULT</div>
<div class="line">CL_DEVICE_SVM_COARSE_GRAIN_BUFFER</div>
<div class="line">CL_DEVICE_SVM_FINE_GRAIN_BUFFER</div>
<div class="line">CL_DEVICE_SVM_FINE_GRAIN_SYSTEM</div>
<div class="line">CL_DEVICE_SVM_ATOMICS</div>
<div class="line">CL_QUEUE_SIZE</div>
<div class="line">CL_MEM_SVM_FINE_GRAIN_BUFFER</div>
<div class="line">CL_MEM_SVM_ATOMICS</div>
<div class="line">CL_sRGB</div>
<div class="line">CL_sRGBx</div>
<div class="line">CL_sRGBA</div>
<div class="line">CL_sBGRA</div>
<div class="line">CL_ABGR</div>
<div class="line">CL_MEM_OBJECT_PIPE</div>
<div class="line">CL_MEM_USES_SVM_POINTER</div>
<div class="line">CL_PIPE_PACKET_SIZE</div>
<div class="line">CL_PIPE_MAX_PACKETS</div>
<div class="line">CL_SAMPLER_MIP_FILTER_MODE</div>
<div class="line">CL_SAMPLER_LOD_MIN</div>
<div class="line">CL_SAMPLER_LOD_MAX</div>
<div class="line">CL_PROGRAM_BUILD_GLOBAL_VARIABLE_TOTAL_SIZE</div>
<div class="line">CL_KERNEL_ARG_TYPE_PIPE</div>
<div class="line">CL_KERNEL_EXEC_INFO_SVM_PTRS</div>
<div class="line">CL_KERNEL_EXEC_INFO_SVM_FINE_GRAIN_SYSTEM</div>
<div class="line">CL_COMMAND_SVM_FREE</div>
<div class="line">CL_COMMAND_SVM_MEMCPY</div>
<div class="line">CL_COMMAND_SVM_MEMFILL</div>
<div class="line">CL_COMMAND_SVM_MAP</div>
<div class="line">CL_COMMAND_SVM_UNMAP</div>
<div class="line">CL_PROFILING_COMMAND_COMPLETE</div>
</div>
</div></blockquote>
</div>
<div class="section" id="new-api-calls">
<h3>New API calls<a class="headerlink" href="#new-api-calls" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><div class="line-block">
<div class="line">clCreateCommandQueueWithProperties See section 5.1</div>
<div class="line">clCreatePipe See section 5.4.1</div>
<div class="line">clGetPipeInfo See section 5.4.2</div>
<div class="line">clSVMAlloc See section 5.6.1</div>
<div class="line">clSVMFree See section 5.6.1</div>
<div class="line">clEnqueueSVMFree See section 5.6.1</div>
<div class="line">clEnqueueSVMMemcpy See section 5.6.1</div>
<div class="line">clEnqueueSVMMemFill See section 5.6.1</div>
<div class="line">clEnqueueSVMMap See section 5.6.1</div>
<div class="line">clEnqueueSVMUnmap See section 5.6.1</div>
<div class="line">clCreateSamplerWithProperties See section 5.7.1</div>
<div class="line">clSetKernelArgSVMPointer See section 5.9.2</div>
<div class="line">clSetKernelExecInfo See section 5.9.2</div>
<div class="line">clGetKernelSubGroupInfoKHR See section 9.17.2.1</div>
</div>
</div></blockquote>
</div>
<div class="section" id="deprecated-runtimes">
<h3>Deprecated runtimes<a class="headerlink" href="#deprecated-runtimes" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><div class="line-block">
<div class="line">clCreateCommandQueue</div>
<div class="line">clCreateSampler</div>
<div class="line">clEnqueueTask</div>
</div>
</div></blockquote>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2014, Thomas Edvalson.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'1.0.0',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>